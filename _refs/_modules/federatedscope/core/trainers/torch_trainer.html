<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>federatedscope.core.trainers.torch_trainer &mdash; federatedscope 0.3.0 documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../../index.html" class="icon icon-home"> federatedscope
          </a>
              <div class="version">
                0.3.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../core.html">Core Module References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cv.html">Federated Computer Vision  Module References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nlp.html">Federated Natural Language Processing  Module References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../gfl.html">Federated Graph Learning  Module References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autotune.html">Auto-tuning Module References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../attack.html">Attack Module References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mf.html">Federated Matrix Factorization Module References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">federatedscope</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">federatedscope.core.trainers.torch_trainer</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for federatedscope.core.trainers.torch_trainer</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">torch</span>
    <span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">Dataset</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="n">torch</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">DataLoader</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">Dataset</span> <span class="o">=</span> <span class="kc">None</span>

<span class="kn">from</span> <span class="nn">federatedscope.core.trainers.enums</span> <span class="kn">import</span> <span class="n">MODE</span><span class="p">,</span> <span class="n">LIFECYCLE</span>
<span class="kn">from</span> <span class="nn">federatedscope.core.trainers.trainer</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">federatedscope.core.trainers.context</span> <span class="kn">import</span> <span class="n">CtxVar</span>
<span class="kn">from</span> <span class="nn">federatedscope.core.auxiliaries.optimizer_builder</span> <span class="kn">import</span> <span class="n">get_optimizer</span>
<span class="kn">from</span> <span class="nn">federatedscope.core.auxiliaries.scheduler_builder</span> <span class="kn">import</span> <span class="n">get_scheduler</span>
<span class="kn">from</span> <span class="nn">federatedscope.core.data</span> <span class="kn">import</span> <span class="n">ClientData</span>
<span class="kn">from</span> <span class="nn">federatedscope.core.data.wrap_dataset</span> <span class="kn">import</span> <span class="n">WrapDataset</span>
<span class="kn">from</span> <span class="nn">federatedscope.core.auxiliaries.dataloader_builder</span> <span class="kn">import</span> <span class="n">get_dataloader</span>
<span class="kn">from</span> <span class="nn">federatedscope.core.auxiliaries.ReIterator</span> <span class="kn">import</span> <span class="n">ReIterator</span>
<span class="kn">from</span> <span class="nn">federatedscope.core.auxiliaries.utils</span> <span class="kn">import</span> <span class="n">param2tensor</span><span class="p">,</span> \
    <span class="n">merge_param_dict</span>
<span class="kn">from</span> <span class="nn">federatedscope.core.monitors.monitor</span> <span class="kn">import</span> <span class="n">Monitor</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="GeneralTorchTrainer"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer">[docs]</a><span class="k">class</span> <span class="nc">GeneralTorchTrainer</span><span class="p">(</span><span class="n">Trainer</span><span class="p">):</span>
<div class="viewcode-block" id="GeneralTorchTrainer.get_model_para"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer.get_model_para">[docs]</a>    <span class="k">def</span> <span class="nf">get_model_para</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">federate</span><span class="o">.</span><span class="n">process_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_filter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ctx</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_param_filter</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ctx</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">federate</span><span class="o">.</span>
                <span class="n">share_local_model</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctx</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span></div>

<div class="viewcode-block" id="GeneralTorchTrainer.setup_data"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer.setup_data">[docs]</a>    <span class="k">def</span> <span class="nf">setup_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialization data by ``cfg``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">ClientData</span><span class="p">):</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">cfg</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;The data type should be `ClientData` to &#39;</span>
                           <span class="sa">f</span><span class="s1">&#39;enable new `config`, but got &#39;</span>
                           <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="si">}</span><span class="s1"> instead.&#39;</span><span class="p">)</span></div>

<div class="viewcode-block" id="GeneralTorchTrainer.parse_data"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer.parse_data">[docs]</a>    <span class="k">def</span> <span class="nf">parse_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Populate &quot;${split}_data&quot;, &quot;${split}_loader&quot; and &quot;num_${</span>
<span class="sd">        split}_data&quot; for different data splits</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">init_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">split</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;val&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">]:</span>
                    <span class="k">continue</span>
                <span class="n">init_dict</span><span class="p">[</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_data&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">split</span><span class="p">)]</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">init_dict</span><span class="p">[</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_loader&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">split</span><span class="p">)]</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">init_dict</span><span class="p">[</span><span class="s2">&quot;num_</span><span class="si">{}</span><span class="s2">_data&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">split</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">if</span> <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">split</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">split</span><span class="p">),</span> <span class="n">Dataset</span><span class="p">):</span>
                        <span class="n">init_dict</span><span class="p">[</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_data&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">split</span><span class="p">)]</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">split</span><span class="p">)</span>
                        <span class="n">init_dict</span><span class="p">[</span><span class="s2">&quot;num_</span><span class="si">{}</span><span class="s2">_data&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">split</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
                            <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">split</span><span class="p">))</span>
                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">split</span><span class="p">),</span> <span class="n">DataLoader</span><span class="p">):</span>
                        <span class="n">init_dict</span><span class="p">[</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_loader&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">split</span><span class="p">)]</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">split</span><span class="p">)</span>
                        <span class="n">init_dict</span><span class="p">[</span><span class="s2">&quot;num_</span><span class="si">{}</span><span class="s2">_data&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">split</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
                            <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">split</span><span class="p">)</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">split</span><span class="p">),</span> <span class="nb">dict</span><span class="p">):</span>
                        <span class="n">init_dict</span><span class="p">[</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_data&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">split</span><span class="p">)]</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">split</span><span class="p">)</span>
                        <span class="n">init_dict</span><span class="p">[</span><span class="s2">&quot;num_</span><span class="si">{}</span><span class="s2">_data&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">split</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span>
                            <span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">split</span><span class="p">)[</span><span class="s1">&#39;y&#39;</span><span class="p">])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type </span><span class="si">{}</span><span class="s2"> is not supported.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">split</span><span class="p">))))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Type of data should be dict.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">init_dict</span></div>

<div class="viewcode-block" id="GeneralTorchTrainer.update"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer.update">[docs]</a>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_parameters</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Called by the FL client to update the model parameters</span>
<span class="sd">        Arguments:</span>
<span class="sd">            model_parameters (dict): PyTorch Module object&#39;s state_dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">model_parameters</span><span class="p">:</span>
            <span class="n">model_parameters</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">param2tensor</span><span class="p">(</span><span class="n">model_parameters</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
        <span class="c1"># Due to lazy load, we merge two state dict</span>
        <span class="n">merged_param</span> <span class="o">=</span> <span class="n">merge_param_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ctx</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
                                        <span class="bp">self</span><span class="o">.</span><span class="n">_param_filter</span><span class="p">(</span><span class="n">model_parameters</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ctx</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">merged_param</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target_data_split_name</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="nb">super</span><span class="p">(</span><span class="n">GeneralTorchTrainer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">target_data_split_name</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctx</span><span class="o">.</span><span class="n">eval_metrics</span>

    <span class="k">def</span> <span class="nf">register_default_hooks_train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_train</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_fit_start_init</span><span class="p">,</span>
                                    <span class="s2">&quot;on_fit_start&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_train</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_fit_start_calculate_model_size</span><span class="p">,</span> <span class="s2">&quot;on_fit_start&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_train</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_epoch_start</span><span class="p">,</span>
                                    <span class="s2">&quot;on_epoch_start&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_train</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_start_init</span><span class="p">,</span>
                                    <span class="s2">&quot;on_batch_start&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_train</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_forward</span><span class="p">,</span>
                                    <span class="s2">&quot;on_batch_forward&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_train</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_forward_regularizer</span><span class="p">,</span>
                                    <span class="s2">&quot;on_batch_forward&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_train</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_forward_flop_count</span><span class="p">,</span>
                                    <span class="s2">&quot;on_batch_forward&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_train</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_backward</span><span class="p">,</span>
                                    <span class="s2">&quot;on_batch_backward&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_train</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_end</span><span class="p">,</span> <span class="s2">&quot;on_batch_end&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_train</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_fit_end</span><span class="p">,</span> <span class="s2">&quot;on_fit_end&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">register_default_hooks_ft</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_ft</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_fit_start_init</span><span class="p">,</span> <span class="s2">&quot;on_fit_start&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_ft</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_fit_start_calculate_model_size</span><span class="p">,</span>
                                 <span class="s2">&quot;on_fit_start&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_ft</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_epoch_start</span><span class="p">,</span> <span class="s2">&quot;on_epoch_start&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_ft</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_start_init</span><span class="p">,</span>
                                 <span class="s2">&quot;on_batch_start&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_ft</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_forward</span><span class="p">,</span>
                                 <span class="s2">&quot;on_batch_forward&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_ft</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_forward_regularizer</span><span class="p">,</span>
                                 <span class="s2">&quot;on_batch_forward&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_ft</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_forward_flop_count</span><span class="p">,</span>
                                 <span class="s2">&quot;on_batch_forward&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_ft</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_backward</span><span class="p">,</span>
                                 <span class="s2">&quot;on_batch_backward&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_ft</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_end</span><span class="p">,</span> <span class="s2">&quot;on_batch_end&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_ft</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_fit_end</span><span class="p">,</span> <span class="s2">&quot;on_fit_end&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">register_default_hooks_eval</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># test/val</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_eval</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_fit_start_init</span><span class="p">,</span>
                                   <span class="s2">&quot;on_fit_start&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_eval</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_epoch_start</span><span class="p">,</span> <span class="s2">&quot;on_epoch_start&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_eval</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_start_init</span><span class="p">,</span>
                                   <span class="s2">&quot;on_batch_start&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_eval</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_forward</span><span class="p">,</span>
                                   <span class="s2">&quot;on_batch_forward&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_eval</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_batch_end</span><span class="p">,</span> <span class="s2">&quot;on_batch_end&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook_in_eval</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hook_on_fit_end</span><span class="p">,</span> <span class="s2">&quot;on_fit_end&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="GeneralTorchTrainer._hook_on_fit_start_init"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer._hook_on_fit_start_init">[docs]</a>    <span class="k">def</span> <span class="nf">_hook_on_fit_start_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Note:</span>
<span class="sd">          The modified attributes and according operations are shown below:</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            Attribute                           Operation</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            ``ctx.model``                       Move to ``ctx.device``</span>
<span class="sd">            ``ctx.optimizer``                   Initialize by ``ctx.cfg``</span>
<span class="sd">            ``ctx.scheduler``                   Initialize by ``ctx.cfg``</span>
<span class="sd">            ``ctx.loss_batch_total``            Initialize to 0</span>
<span class="sd">            ``ctx.loss_regular_total``          Initialize to 0</span>
<span class="sd">            ``ctx.num_samples``                 Initialize to 0</span>
<span class="sd">            ``ctx.ys_true``                     Initialize to ``[]``</span>
<span class="sd">            ``ctx.ys_prob``                     Initialize to ``[]``</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># prepare model and optimizer</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">cur_mode</span> <span class="ow">in</span> <span class="p">[</span><span class="n">MODE</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span> <span class="n">MODE</span><span class="o">.</span><span class="n">FINETUNE</span><span class="p">]:</span>
            <span class="c1"># Initialize optimizer here to avoid the reuse of optimizers</span>
            <span class="c1"># across different routines</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">get_optimizer</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
                                          <span class="o">**</span><span class="n">ctx</span><span class="o">.</span><span class="n">cfg</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">cur_mode</span><span class="p">]</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_scheduler</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span>
                                          <span class="o">**</span><span class="n">ctx</span><span class="o">.</span><span class="n">cfg</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">cur_mode</span><span class="p">]</span><span class="o">.</span><span class="n">scheduler</span><span class="p">)</span>

        <span class="c1"># TODO: the number of batch and epoch is decided by the current mode</span>
        <span class="c1">#  and data split, so the number of batch and epoch should be</span>
        <span class="c1">#  initialized at the beginning of the routine</span>

        <span class="c1"># prepare statistics</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">loss_batch_total</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">ROUTINE</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">loss_regular_total</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">ROUTINE</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">ROUTINE</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">ys_true</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">([],</span> <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">ROUTINE</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">ys_prob</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">([],</span> <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">ROUTINE</span><span class="p">)</span></div>

<div class="viewcode-block" id="GeneralTorchTrainer._hook_on_fit_start_calculate_model_size"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer._hook_on_fit_start_calculate_model_size">[docs]</a>    <span class="k">def</span> <span class="nf">_hook_on_fit_start_calculate_model_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Note:</span>
<span class="sd">          The modified attributes and according operations are shown below:</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            Attribute                           Operation</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            ``ctx.monitor``                     Track model size</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">monitor</span><span class="p">,</span> <span class="n">Monitor</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The trainer </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2"> does contain a valid monitor, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;this may be caused by initializing trainer subclasses &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;without passing a valid monitor instance.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Plz check whether this is you want.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">total_model_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">track_model_size</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">models</span><span class="p">)</span></div>

<div class="viewcode-block" id="GeneralTorchTrainer._hook_on_epoch_start"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer._hook_on_epoch_start">[docs]</a>    <span class="k">def</span> <span class="nf">_hook_on_epoch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Note:</span>
<span class="sd">          The modified attributes and according operations are shown below:</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            Attribute                           Operation</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            ``ctx.{ctx.cur_split}_loader``      Initialize DataLoader</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># prepare dataloader</span>
        <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_loader&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">cur_split</span><span class="p">))</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">loader</span> <span class="o">=</span> <span class="n">get_dataloader</span><span class="p">(</span>
                <span class="n">WrapDataset</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_data&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">cur_split</span><span class="p">))),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">cur_split</span><span class="p">)</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_loader&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">cur_split</span><span class="p">),</span> <span class="n">ReIterator</span><span class="p">(</span><span class="n">loader</span><span class="p">))</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_loader&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">cur_split</span><span class="p">)),</span>
                            <span class="n">ReIterator</span><span class="p">):</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_loader&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">cur_split</span><span class="p">),</span>
                    <span class="n">ReIterator</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_loader&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">cur_split</span><span class="p">))))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_loader&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">cur_split</span><span class="p">))</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span></div>

<div class="viewcode-block" id="GeneralTorchTrainer._hook_on_batch_start_init"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer._hook_on_batch_start_init">[docs]</a>    <span class="k">def</span> <span class="nf">_hook_on_batch_start_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Note:</span>
<span class="sd">          The modified attributes and according operations are shown below:</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            Attribute                           Operation</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            ``ctx.data_batch``                  Initialize batch data</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># prepare data batch</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">data_batch</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">(</span>
                <span class="nb">next</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_loader&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">cur_split</span><span class="p">))),</span>
                <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">StopIteration</span></div>

<div class="viewcode-block" id="GeneralTorchTrainer._hook_on_batch_forward"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer._hook_on_batch_forward">[docs]</a>    <span class="k">def</span> <span class="nf">_hook_on_batch_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Note:</span>
<span class="sd">          The modified attributes and according operations are shown below:</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            Attribute                           Operation</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            ``ctx.y_true``                      Move to `ctx.device`</span>
<span class="sd">            ``ctx.y_prob``                      Forward propagation get y_prob</span>
<span class="sd">            ``ctx.loss_batch``                  Calculate the loss</span>
<span class="sd">            ``ctx.batch_size``                  Get the batch_size</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="p">[</span><span class="n">_</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">data_batch</span><span class="p">]</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">ctx</span><span class="o">.</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">y_prob</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">loss_batch</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">label</span><span class="p">),</span> <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">),</span> <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)</span></div>

<div class="viewcode-block" id="GeneralTorchTrainer._hook_on_batch_forward_flop_count"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer._hook_on_batch_forward_flop_count">[docs]</a>    <span class="k">def</span> <span class="nf">_hook_on_batch_forward_flop_count</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The monitoring hook to calculate the flops during the fl course</span>

<span class="sd">        Note:</span>
<span class="sd">          For customized cases that the forward process is not only \</span>
<span class="sd">          based on ctx.model, please override this function (inheritance \</span>
<span class="sd">          case) or replace this hook (plug-in case)</span>

<span class="sd">          The modified attributes and according operations are shown below:</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            Attribute                           Operation</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            ``ctx.monitor``                     Track average flops</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">monitor</span><span class="p">,</span> <span class="n">Monitor</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The trainer </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2"> does contain a valid monitor, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;this may be caused by initializing trainer subclasses &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;without passing a valid monitor instance.&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Please check whether this is you want.&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">eval</span><span class="o">.</span><span class="n">count_flops</span> <span class="ow">and</span> <span class="n">ctx</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">flops_per_sample</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># calculate the flops_per_sample</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">_</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">ctx</span><span class="o">.</span><span class="n">data_batch</span><span class="p">]</span>
                <span class="kn">from</span> <span class="nn">fvcore.nn</span> <span class="kn">import</span> <span class="n">FlopCountAnalysis</span>
                <span class="n">flops_one_batch</span> <span class="o">=</span> <span class="n">FlopCountAnalysis</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">total</span><span class="p">()</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_nums</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">ctx</span><span class="o">.</span><span class="n">mirrored_models</span><span class="p">:</span>
                    <span class="n">flops_one_batch</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_nums</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;the flops_per_batch is multiplied &quot;</span>
                        <span class="s2">&quot;by internal model nums as self.mirrored_models=True.&quot;</span>
                        <span class="s2">&quot;if this is not the case you want, &quot;</span>
                        <span class="s2">&quot;please customize the count hook&quot;</span><span class="p">)</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">track_avg_flops</span><span class="p">(</span><span class="n">flops_one_batch</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="c1"># Raise warning at the first failure</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;current flop count implementation is for general &quot;</span>
                    <span class="s2">&quot;trainer case: &quot;</span>
                    <span class="s2">&quot;1) ctx.data_batch = [x, y]; and&quot;</span>
                    <span class="s2">&quot;2) the ctx.model takes only x as input.&quot;</span>
                    <span class="s2">&quot;Please check the forward format or implement your own &quot;</span>
                    <span class="s2">&quot;flop_count function&quot;</span><span class="p">)</span>
                <span class="n">ctx</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">flops_per_sample</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="c1"># by default, we assume the data has the same input shape,</span>
        <span class="c1"># thus simply multiply the flops to avoid redundant forward</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">total_flops</span> <span class="o">+=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">flops_per_sample</span> <span class="o">*</span> \
            <span class="n">ctx</span><span class="o">.</span><span class="n">batch_size</span></div>

<div class="viewcode-block" id="GeneralTorchTrainer._hook_on_batch_forward_regularizer"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer._hook_on_batch_forward_regularizer">[docs]</a>    <span class="k">def</span> <span class="nf">_hook_on_batch_forward_regularizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Note:</span>
<span class="sd">          The modified attributes and according operations are shown below:</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            Attribute                           Operation</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            ``ctx.loss_regular``                Calculate the regular loss</span>
<span class="sd">            ``ctx.loss_task``                   Sum the ``ctx.loss_regular`` \</span>
<span class="sd">            and ``ctx.loss``</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">loss_regular</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">regularizer</span><span class="o">.</span><span class="n">mu</span> <span class="o">*</span> <span class="n">ctx</span><span class="o">.</span><span class="n">regularizer</span><span class="p">(</span><span class="n">ctx</span><span class="p">),</span> <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">loss_task</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">loss_batch</span> <span class="o">+</span> <span class="n">ctx</span><span class="o">.</span><span class="n">loss_regular</span><span class="p">,</span>
                               <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">BATCH</span><span class="p">)</span></div>

<div class="viewcode-block" id="GeneralTorchTrainer._hook_on_batch_backward"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer._hook_on_batch_backward">[docs]</a>    <span class="k">def</span> <span class="nf">_hook_on_batch_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Note:</span>
<span class="sd">          The modified attributes and according operations are shown below:</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            Attribute                           Operation</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            ``ctx.optimizer``                   Update by gradient</span>
<span class="sd">            ``ctx.loss_task``                   Backward propagation</span>
<span class="sd">            ``ctx.scheduler``                   Update by gradient</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">loss_task</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">grad_clip</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                                           <span class="n">ctx</span><span class="o">.</span><span class="n">grad_clip</span><span class="p">)</span>

        <span class="n">ctx</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span></div>

<div class="viewcode-block" id="GeneralTorchTrainer._hook_on_batch_end"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer._hook_on_batch_end">[docs]</a>    <span class="k">def</span> <span class="nf">_hook_on_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Note:</span>
<span class="sd">          The modified attributes and according operations are shown below:</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            Attribute                           Operation</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            ``ctx.num_samples``                 Add ``ctx.batch_size``</span>
<span class="sd">            ``ctx.loss_batch_total``            Add batch loss</span>
<span class="sd">            ``ctx.loss_regular_total``          Add batch regular loss</span>
<span class="sd">            ``ctx.ys_true``                     Append ``ctx.y_true``</span>
<span class="sd">            ``ctx.ys_prob``                     Append ``ctx.ys_prob``</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># update statistics</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">num_samples</span> <span class="o">+=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">loss_batch_total</span> <span class="o">+=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">loss_batch</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">ctx</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">loss_regular_total</span> <span class="o">+=</span> <span class="nb">float</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;loss_regular&quot;</span><span class="p">,</span> <span class="mf">0.</span><span class="p">))</span>
        <span class="c1"># cache label for evaluate</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">ys_true</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">y_true</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">ys_prob</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">y_prob</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span></div>

<div class="viewcode-block" id="GeneralTorchTrainer._hook_on_fit_end"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer._hook_on_fit_end">[docs]</a>    <span class="k">def</span> <span class="nf">_hook_on_fit_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Evaluate metrics.</span>

<span class="sd">        Note:</span>
<span class="sd">          The modified attributes and according operations are shown below:</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            Attribute                           Operation</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">            ``ctx.ys_true``                     Convert to ``numpy.array``</span>
<span class="sd">            ``ctx.ys_prob``                     Convert to ``numpy.array``</span>
<span class="sd">            ``ctx.monitor``                     Evaluate the results</span>
<span class="sd">            ``ctx.eval_metrics``                Get evaluated results from \</span>
<span class="sd">            ``ctx.monitor``</span>
<span class="sd">            ==================================  ===========================</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">ys_true</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">ys_true</span><span class="p">),</span> <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">ROUTINE</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">ys_prob</span> <span class="o">=</span> <span class="n">CtxVar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">ys_prob</span><span class="p">),</span> <span class="n">LIFECYCLE</span><span class="o">.</span><span class="n">ROUTINE</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">monitor</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>
        <span class="nb">setattr</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="s1">&#39;eval_metrics&#39;</span><span class="p">,</span> <span class="n">results</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">save_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">cur_round</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctx</span><span class="o">.</span><span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="n">ckpt</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;cur_round&#39;</span><span class="p">:</span> <span class="n">cur_round</span><span class="p">,</span> <span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctx</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()}</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">ckpt</span><span class="p">,</span> <span class="n">path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">load_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">ctx</span><span class="o">.</span><span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
            <span class="n">ckpt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ctx</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ctx</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">ckpt</span><span class="p">[</span><span class="s1">&#39;model&#39;</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">ckpt</span><span class="p">[</span><span class="s1">&#39;cur_round&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The file </span><span class="si">{}</span><span class="s2"> does NOT exist&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">path</span><span class="p">))</span>

<div class="viewcode-block" id="GeneralTorchTrainer.discharge_model"><a class="viewcode-back" href="../../../../core.html#federatedscope.core.trainers.GeneralTorchTrainer.discharge_model">[docs]</a>    <span class="k">def</span> <span class="nf">discharge_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Discharge the model from GPU device</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Avoid memory leak</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">federate</span><span class="o">.</span><span class="n">share_local_model</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">torch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">pass</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ctx</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span></div></div>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, The DAIL Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>