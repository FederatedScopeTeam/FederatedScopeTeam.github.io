var store = [{
        "title": "Documentation",
        "excerpt":"Federated Learning (FL) [1,2,3] is a learning paradigm for collaboratively learning models from isolated data without directly sharing privacy information, which helps to satisfy the requirements of privacy protection of the public. FederatedScope (FS), a comprehensive federated learning platform with event-driven architecture, aims to provide easy-to-use and flexible support for users who want to get started and customize task-specific FL procedures quickly.   Tutorials           For beginner                       Installation                        Start With Examples                        Start Your Own Case                        DataZoo                        ModelZoo                        AlgoZoo                        Tuning Federated Learning                        For advanced users or developer                       FS Data module                        Event-driven Architecture                        New Types of Messages and Handlers                        Privacy Protection for Message                        Local Learning Abstraction: Trainer                        Personalized FL                        Cross-Device FL                        Cross-Silo FL                        Accelerating Federated HPO                        Simulation and Deployment                        Application with FS                       Recommendation                        Differential Privacy                        Privacy Attacks                        Graph                        NLP and Speech                        LLM                        Get involved                       Join Our Community                        Contributing to FederatedScope                   API References      Core Module References   Federated Computer Vision Module References   Federated Natural Language Processing Module References   Federated Graph Learning Module References   Auto-tuning Module References   Attack Module References   Federated Matrix Factorization Module References   References   [1] McMahan B, Moore E, Ramage D, et al. “Communication-efficient learning of deep networks from decentralized data”. Artificial intelligence and statistics. PMLR, 2017: 1273-1282.  [2] Konečný J, McMahan H B, Ramage D, et al. “Federated optimization: Distributed machine learning for on-device intelligence”. arXiv preprint arXiv:1610.02527, 2016.  [3] Yang Q, Liu Y, Cheng Y, et al. “Federated learning”. Synthesis Lectures on Artificial Intelligence and Machine Learning, 2019, 13(3): 1-207.  ","categories": [],
        "tags": [],
        "url": "/docs/documentation/",
        "teaser": null
      },{
        "title": "Installation",
        "excerpt":"First of all, users need to clone the source code and install the required packages (we suggest python version &gt;= 3.9). You can choose between the following two installation methods (via docker or conda) to install FederatedScope.   git clone https://github.com/alibaba/FederatedScope.git cd FederatedScope  Use Docker   You can build docker image and run with docker env (cuda 11 and torch 1.10):  docker build -f environment/docker_files/federatedscope-torch1.10.Dockerfile -t alibaba/federatedscope:base-env-torch1.10 . docker run --gpus device=all --rm -it --name \"fedscope\" -w $(pwd) alibaba/federatedscope:base-env-torch1.10 /bin/bash  If you need to run with down-stream tasks such as graph FL, change the requirement/docker file name into another one when executing the above commands:  # environment/requirements-torch1.10.txt -&gt;  environment/requirements-torch1.10-application.txt  # environment/docker_files/federatedscope-torch1.10.Dockerfile -&gt; environment/docker_files/federatedscope-torch1.10-application.Dockerfile  Note: You can choose to use cuda 10 and torch 1.8 via changing torch1.10 to torch1.8. The docker images are based on the nvidia-docker. Please pre-install the NVIDIA drivers and nvidia-docker2 in the host machine. See more details here.   Use Conda   We recommend using a new virtual environment to install FederatedScope:   conda create -n fs python=3.9 conda activate fs   If your backend is torch, please install torch in advance (torch-get-started). For example, if your cuda version is 11.3 please execute the following command:   conda install -y pytorch=1.10.1 torchvision=0.11.2 torchaudio=0.10.1 cudatoolkit=11.3 -c pytorch -c conda-forge   For users with Apple M1 chips:  conda install pytorch torchvision torchaudio -c pytorch # Downgrade torchvision to avoid segmentation fault python -m pip install torchvision==0.11.3   Finally, after the backend is installed, you can install FederatedScope from source:   From source   pip install .  # Or (for dev mode) pip install -e .[dev] pre-commit install   Now, you have successfully installed the minimal version of FederatedScope. (Optinal) For application version including graph, nlp and speech, run:   bash environment/extra_dependencies_torch1.10-application.sh  ","categories": [],
        "tags": [],
        "url": "/docs/installation/",
        "teaser": null
      },{
        "title": "Start With Examples",
        "excerpt":"Prepare datasets &amp; models   To run an FL course, firstly you should prepare datasets for FL. The DataZoo provided in FederatedScope can help to automatically download and preprocess widely-used public datasets from various FL applications, including computer vision, natural language processing, graph learning, recommendation, etc. Users can conveniently conduct experiments on the provided dataset via specifying cfg.data.type = DATASET_NAMEin the configuration.  We also support users to adopt customized datasets, please refer to DataZoo for more details about the provided datasets, and refer to Customized Datasets for introducing customized datasets in FederatedScope.   Secondly, you should specify the model architecture that will be federally trained, such as ConvNet or LSTM. FederatedScope provides the ModelZoo that contains the implementation of widely-used model architectures for various FL applications. Users can set up cfg.model.type = MODEL_NAME to apply a specific model architecture in FL tasks. We allow users to use customized models via registering without caring about the federated process. You can refer to ModelZoo for more details about how to customize models.   For a vanilla FL course, all participants share the same model architecture and training configuration. And FederatedScope also supports adopting client-specific models and training configurations (known as personalized FL) to handle the non-IID issue in practical  FL applications, please refer to Personalized FL for more details.   Run an FL course with configurations   Note that FederatedScope provides a unified view for both standalone simulation and distributed deployment, therefore users can easily run an FL course with standalone mode or distributed mode via configuring.   Standalone mode   The standalone mode in FederatedScope means to simulate multiple participants (servers and clients) in a single device, while participants’ data are isolated from each other and their models might be shared via message passing.   Here we demonstrate how to run a vanilla FL course with FederatedScope, with setting cfg.data.type = 'FEMNIST'and cfg.model.type = 'ConvNet2' to run vanilla FedAvg [1] for an image classification task. Users can include more training configurations, such as cfg.federated.total_round_num, cfg.data.batch_size, and cfg.train.optimizer.lr, in the configuration (a .yaml file), and run a vanilla FL course as:   # Run with default configurations python federatedscope/main.py --cfg scripts/example_configs/femnist.yaml # Or with custom configurations python federatedscope/main.py --cfg scripts/example_configs/femnist.yaml federated.total_round_num 50 data.batch_size 128   Then users can observe some monitored metrics during the training process as:   INFO: Server has been set up ... INFO: Model meta-info: &lt;class 'federatedscope.cv.model.cnn.ConvNet2'&gt;. ... ... INFO: Client has been set up ... INFO: Model meta-info: &lt;class 'federatedscope.cv.model.cnn.ConvNet2'&gt;. ... ... INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {'train_loss': 207.6341676712036, 'train_acc': 0.02, 'train_total': 50, 'train_loss_regular': 0.0, 'train_avg_loss': 4.152683353424072}} INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {'train_loss': 209.0940284729004, 'train_acc': 0.02, 'train_total': 50, 'train_loss_regular': 0.0, 'train_avg_loss': 4.1818805694580075}} INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {'train_loss': 202.24929332733154, 'train_acc': 0.04, 'train_total': 50, 'train_loss_regular': 0.0, 'train_avg_loss': 4.0449858665466305}} INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {'train_loss': 209.43883895874023, 'train_acc': 0.06, 'train_total': 50, 'train_loss_regular': 0.0, 'train_avg_loss': 4.1887767791748045}} INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {'train_loss': 208.83140087127686, 'train_acc': 0.0, 'train_total': 50, 'train_loss_regular': 0.0, 'train_avg_loss': 4.1766280174255375}} INFO: ----------- Starting a new training round (Round #1) ------------- ... ... INFO: Server: Training is finished! Starting evaluation. INFO: Client #1: (Evaluation (test set) at Round #20) test_loss is 163.029045 ... ... INFO: Server: Final evaluation is finished! Starting merging results. ... ...   Distributed mode  The distributed mode in FederatedScope denotes running multiple procedures to build up an FL course, where each procedure plays as a participant (server or client) that instantiates its model and loads its data. The communication between participants is already provided by the communication module of FederatedScope.   To run with distributed mode, you only need to:      Prepare isolated data file and set up cfg.distribute.data_file = PATH/TO/DATA for each participant;   Change cfg.federate.model = 'distributed', and specify the role of each participant  by cfg.distributed.role = 'server'/'client'.   Set up a valid address by cfg.distribute.server_host/client_host = x.x.x.x and cfg.distribute.server_port/client_port = xxxx. (Note that for a server, you need to set up server_host and server_port for listening messages, while for a client, you need to set up client_host and client_port for listening as well as server_host and server_port for joining in an FL course)   We prepare a synthetic example for running with distributed mode:   # For server python federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_server.yaml distribute.data_file 'PATH/TO/DATA' distribute.server_host x.x.x.x distribute.server_port xxxx  # For clients python federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_client_1.yaml distribute.data_file 'PATH/TO/DATA' distribute.server_host x.x.x.x distribute.server_port xxxx distribute.client_host x.x.x.x distribute.client_port xxxx python federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_client_2.yaml distribute.data_file 'PATH/TO/DATA' distribute.server_host x.x.x.x distribute.server_port xxxx distribute.client_host x.x.x.x distribute.client_port xxxx python federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_client_3.yaml distribute.data_file 'PATH/TO/DATA' distribute.server_host x.x.x.x distribute.server_port xxxx distribute.client_host x.x.x.x distribute.client_port xxxx   An executable example with generated toy data can be run with (a script can be found in scripts/distributed_scripts/run_distributed_lr.sh):  # Generate the toy data python scripts/gen_data.py  # Firstly start the server that is waiting for clients to join in python federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_server.yaml distribute.data_file toy_data/server_data distribute.server_host 127.0.0.1 distribute.server_port 50051  # Start the client #1 (with another process) python federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_client_1.yaml distribute.data_file toy_data/client_1_data distribute.server_host 127.0.0.1 distribute.server_port 50051 distribute.client_host 127.0.0.1 distribute.client_port 50052 # Start the client #2 (with another process) python federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_client_2.yaml distribute.data_file toy_data/client_2_data distribute.server_host 127.0.0.1 distribute.server_port 50051 distribute.client_host 127.0.0.1 distribute.client_port 50053 # Start the client #3 (with another process) python federatedscope/main.py --cfg scripts/distributed_scripts/distributed_configs/distributed_client_3.yaml distribute.data_file toy_data/client_3_data distribute.server_host 127.0.0.1 distribute.server_port 50051 distribute.client_host 127.0.0.1 distribute.client_port 50054   And you can observe the results as (the IP addresses are anonymized with ‘x.x.x.x’):   INFO: Server: Listen to x.x.x.x:xxxx... INFO: Server has been set up ... Model meta-info: &lt;class 'federatedscope.core.lr.LogisticRegression'&gt;. ... ... INFO: Client: Listen to x.x.x.x:xxxx... INFO: Client (address x.x.x.x:xxxx) has been set up ... Client (address x.x.x.x:xxxx) is assigned with #1. INFO: Model meta-info: &lt;class 'federatedscope.core.lr.LogisticRegression'&gt;. ... ... {'Role': 'Client #2', 'Round': 0, 'Results_raw': {'train_avg_loss': 5.215108394622803, 'train_loss': 333.7669372558594, 'train_total': 64}} {'Role': 'Client #1', 'Round': 0, 'Results_raw': {'train_total': 64, 'train_loss': 290.9668884277344, 'train_avg_loss': 4.54635763168335}} ----------- Starting a new training round (Round #1) ------------- ... ... INFO: Server: Training is finished! Starting evaluation. INFO: Client #1: (Evaluation (test set) at Round #20) test_loss is 30.387419 ... ... INFO: Server: Final evaluation is finished! Starting merging results. ... ...   Run FS with configured scripts   We provide some scripts for reproducing existing algorithms with FederatedScope, which are constantly being updated in the scripts folder. You learn how to configure FS and reproduce the results with them.           Distribute Mode            Asynchronous Training Strategy            Graph Federated Learning            Attacks in Federated Learning            Federated Optimization Algorithm            Personalized Federated Learning            Differential Privacy in Federated Learning            Matrix Factorization in Federated Learning       References   [1] McMahan B, Moore E, Ramage D, et al. “Communication-efficient learning of deep networks from decentralized data”. Artificial intelligence and statistics. PMLR, 2017: 1273-1282.  [2] Konečný J, McMahan H B, Ramage D, et al. “Federated optimization: Distributed machine learning for on-device intelligence”. arXiv preprint arXiv:1610.02527, 2016.  [3] Yang Q, Liu Y, Cheng Y, et al. “Federated learning”. Synthesis Lectures on Artificial Intelligence and Machine Learning, 2019, 13(3): 1-207.  ","categories": [],
        "tags": [],
        "url": "/docs/examples/",
        "teaser": null
      },{
        "title": "Start Your Own Case",
        "excerpt":"In addition to the rich collcetion of datasets, models and evaluation metrics, FederatedScope also allows to create your own or introduce more to our package.   We provide register function to help build your own federated learning workflow.  This introduction will help you to start with your own case:      Load a dataset   Build a model   Create a trainer   Introduce more evaluation metrics   Specify your own configuration   Load a dataset   We provide a function federatedscope.register.register_data to make your dataset available with three steps:           Step1: set up your data with Data Translator (standalone), which  translates your torch.utils.data.Dataset to FS data format (for more details about FS data module, see FS Data):         def load_my_data(config, client_cfgs=None):     from federatedscope.core.data import BaseDataTranslator     # Load a dataset, whose class is `torch.utils.data.Dataset`     dataset = ... \t\t# Instantiate a translator according to config     translator = BaseDataTranslator(config, client_cfgs)     # Translate torch dataset to FS data     fs_data = translator(dataset)     return fs_data, config\t                We take torchvision.datasets.MNIST, which is split and assigned to two clients, as an example:         def load_my_data(config, client_cfgs=None):       transform = transforms.Compose([             transforms.ToTensor(),             transforms.Normalize(mean=[0.1307], std=[0.3081])         ])         data_train = MNIST(root='data', train=True, transform=transform, download=True)       data_test = MNIST(root='data', train=False, transform=transform, download=True)       translator = BaseDataTranslator(config, client_cfgs)       fs_data = translator([data_train, [], data_test])       return fs_data, config                Step2: register your data with a keyword, such as \"mydata\".        from federatedscope.register import register_data       def call_my_data(config):      if config.data.type == \"mydata\":          data, modified_config = load_my_data(config)          return data, modified_config       register_data(\"mydata\", call_my_data)                Step3: put this .py file in the federatedscope/contrib/data/ folder, and set cfg.data.type = \"mydata\" to use it.       Also,  you can modify the source code to make the FederatedScope support your dataset. Please see federatedscope.core.auxiliaries.data_builder , and you can add an elif to skip Step2 and Step3 above.   Build a model   We provide a function federatedscope.register.register_model to make your model available with three steps: (we take ConvNet2 as an example)           Step1: build your model with Pytorch or Tensorflow and instantiate your model class with config and data.        import torch            class MyNet(torch.nn.Module):      def __init__(self,                   in_channels,                   h=32,                   w=32,                   hidden=2048,                   class_num=10,                   use_bn=True):          super(MyNet, self).__init__()          self.conv1 = torch.nn.Conv2d(in_channels, 32, 5, padding=2)          self.conv2 = torch.nn.Conv2d(32, 64, 5, padding=2)          self.fc1 = torch.nn.Linear((h // 2 // 2) * (w // 2 // 2) * 64, hidden)          self.fc2 = torch.nn.Linear(hidden, class_num)          self.relu = torch.nn.ReLU(inplace=True)          self.maxpool = torch.nn.MaxPool2d(2)           def forward(self, x):          x = self.conv1(x)          x = self.maxpool(self.relu(x))          x = self.conv2(x)          x = self.maxpool(self.relu(x))          x = torch.nn.Flatten()(x)          x = self.relu(self.fc1(x))          x = self.fc2(x)          return x            def load_my_net(model_config, local_data):      # You can also build models without local_data      data = next(iter(local_data['train']))      model = MyNet(in_channels=data[0].shape[1],                    h=data[0].shape[2],                    w=data[0].shape[3],                    hidden=model_config.hidden,                    class_num=model_config.out_channels)      return model                Step2: register your model with a keyword, such as \"mynet\".        from federatedscope.register import register_model       def call_my_net(model_config, local_data):      if model_config.type == \"mynet\":          model = load_my_net(model_config, local_data)          return model       register_model(\"mynet\", call_my_net)                Step3: put this .py file in the federatedscope/contrib/model/ folder, and set cfg.model.type = \"mynet\" to use it.       Also,  you can modify the source code to make the FederatedScope support your model. Please see federatedscope.core.auxiliaries.model_builder , and you can add an elif to skip Step2 and Step3 above.   Create a trainer   FederatedScope decouples the local learning process and details of FL communication and schedule, allowing users to freely customize the local learning algorithms via the Trainer. We recommend user build trainer by inheriting federatedscope.core.trainers.trainer.GeneralTrainer, for more details, please see Trainer. Similarly, we provide federatedscope.register.register_trainer to make your customized trainer available:           Step1: build your trainer by inheriting GeneralTrainer. Our GeneralTrainer already supports many different usages, for the advanced user, please see federatedscope.core.trainers.trainer.GeneralTrainer for more details.        from federatedscope.core.trainers.trainer import GeneralTrainer       class MyTrainer(GeneralTrainer):      pass                Step2: register your trainer with a keyword, such as \"mytrainer\".        from federatedscope.register import register_trainer       def call_my_trainer(trainer_type):      if trainer_type == 'mytrainer':          trainer_builder = MyTrainer          return trainer_builder       register_trainer('mytrainer', call_my_trainer)                Step3: put this .py file in the federatedscope/contrib/trainer/ folder, and set cfg.trainer.type = \"mytrainer\" to use it.       Also,  you can modify the source code to make the FederatedScope support your model. Please see federatedscope/core/auxiliaries/trainer_builder.py , and you can add an elif to skip Step2 and Step3 above.   Introduce more evaluation metrics   We provide a number of metrics to monitor the entire federal learning process. You just need to list the name of the metric you want in cfg.eval.metrics. We currently support metrics such as loss, accuracy, etc. (See federatedscope.core.monitors.metric_calculator for more details).   We also provide a function federatedscope.register.register_metric to make your evaluation metrics available with three steps:           Step1: build your metric (see federatedscope.core.trainers.context for more about ctx)        def cal_my_metric(ctx, **kwargs):      ...      return MY_METRIC_VALUE                Step2: register your metric with a keyword, such as \"mymetric\".        from federatedscope.register import register_metric       def call_my_metric(types):      if \"mymetric\" in types:          metric_builder = cal_my_metric          return \"mymetric\", metric_builder       register_metric(\"mymetric\", call_my_metric)                Step3: put this .py file in the federatedscope/contrib/metircs/ folder, and add \"mymetric\" to cfg.eval.metric activate it.       Specify your own configuration   Basic usage  FederatedScope provides an extended configuration system based on yacs. We leverage a two-level tree structure that consists of several internal dict-like containers to allow simple key-value access and management. For example,   cfg.backend = 'torch'  # level-1 configuration  cfg.federate = CN()  # level-2 configuration cfg.federate.client_num = 0   The frequently-used APIs include     merge_from_file, merge_from_other_cfg and merge_from_list that load configs from a yaml file, another cfg instance or a list stores the keys and values.   Besides, we can use freeze to make the configs immutable and save the configs in a yaml file under the specified cfg.outdir.   Both these functions will trigger the configuration validness checking.   To modify a config node after calling freeze, we can call defrost.   As a start, our package will initialize a global_cfg instance by default, i.e.,   global_cfg = CN() init_global_cfg(global_cfg)  see more details in the file federatedscope/core/configs/config.py.  Users can clone and use their own configuration object as follows:   from federatedscope.core.configs.config import global_cfg  def main():      init_cfg = global_cfg.clone()     args = parse_args()     init_cfg.merge_from_file(args.cfg_file)     init_cfg.merge_from_list(args.opts)      setup_logger(init_cfg)     setup_seed(init_cfg.seed)      # federated dataset might change the number of clients     # thus, we allow the creation procedure of dataset to modify the global cfg object     data, modified_cfg = get_data(config=init_cfg.clone())     init_cfg.merge_from_other_cfg(modified_cfg)      init_cfg.freeze()          # do sth. further   Built-in configurations  We divide the configuration could be used in the FL process into several sub files such as cfg_fl_setting, cfg_fl_algo, cfg_model, cfg_training, cfg_evaluation, see more details in federatedscope/core/configs directory.   Customized configuration  To add new configuration, you need          implement your own extend function extend_my_cfg(cfg):, e.g.,       def extend_training_cfg(cfg):     # ------------------------------------------------------------------------ #     # Trainer related options     # ------------------------------------------------------------------------ #     cfg.trainer = CN()         cfg.trainer.type = 'general'     cfg.trainer.finetune = CN()     cfg.trainer.finetune.steps = 0     cfg.trainer.finetune.only_psn = True     cfg.trainer.finetune.stepsize = 0.01              # --------------- register corresponding check function ----------     cfg.register_cfg_check_fun(assert_training_cfg)                and implement your own config validation check function assert_my_cfg, e.g.,       def assert_training_cfg(cfg):     if cfg.backend not in ['torch', 'tensorflow']:         raise ValueError(              \"Value of 'cfg.backend' must be chosen from ['torch', 'tensorflow'].\"         )                finally, register your own extended function, e.g.,       from federatedscope.register import register_config register_config(\"fl_training\", extend_training_cfg)           We recommend users put the new customized configuration in federatedscope/contrib/configs directory   ","categories": [],
        "tags": [],
        "url": "/docs/own-case/",
        "teaser": null
      },{
        "title": "DataZoo",
        "excerpt":"FederatedScope provides a rich collection of datasets for researchers, including images, texts, graphs, recommendation systems and speeches. Our DataZoo contains real federation datasets as well as simulated federation datasets with a different splitter. And more datasets are coming soon!   To use our DataZoo, set cfg.data.type = DATASET_NAME. Downloading and pre-processing of our datasets is automatic, and you do not need to perform additional operations. Please feel free to use it! For more dataset-related settings, please refer to each dataset.     Dataset     Computer Vision      FMNIST FEMNIST is a federation image dataset from LEAF [1], and the task is image classification. The images are split by writers into clients. Moreover, you can set the sampling rate to sample from the clients via cfg.data.subsample. Statistics: 62 classes, 805263 images, about 3500 clients.   Celeba Celeba is a federation image dataset from LEAF [1], and the task is image classification.  The images are split by humans into clients. Moreover, you can set the sampling rate to sample from the clients via cfg.data.subsample. Statistics: 2 classes (smiling or not), 200288 images, about 9300 clients.     Natural Language Processing      Shakespeare Shakespeare is a federation text dataset of Shakespeare Dialogues from LEAF [1], and the task is next-character prediction. Moreover, you can set the sampling rate to sample from the clients via cfg.data.subsample. Statistics: 422615 sentences, about 1100 clients.   SubReddit SubReddit is a federation text dataset and subsampled of Reddit from LEAF [1], and the task is next-word prediction. Moreover, you can set the sampling rate to sample from the clients via cfg.data.subsample. Statistics: 216858 sentences, about 800 clients.   Sentiment140 Sentiment140 is a federation text dataset of Twitter from LEAF [1], and the task is Sentiment Analysis. Moreover, you can set the sampling rate to sample from the clients via cfg.data.subsample. Statistics: 1600498 sentences, about 660000  clients.     Graph   For details of statistics, see FedGraph.     Node-level dataset      FedDBLP (including dblp_conf and dblp_org) FedDBLP is a federation citation network from the latest DBLP [2] dump, where each node corresponds to a published paper, and each edge corresponds to a citation. We use the bag-of-words of each paper’s abstract as its node attributes and regard the theme of paper as its label. To simulate the scenario that a venue or an organizer forbids others to cite its papers, we allow users to split this dataset by each node’s venue or the organizer of that venue.   FedcSBM FedcSBM from cSBM [3] can produce the synthetic graph dataset. For more details, see FedGraph.   FedCora, FedCiteSeer, FedPubMed FedCora, FedCiteSeer and FedPubMed are simulated federation datasets split from Cora [4], CiteSeer [5], PubMed [6] by community_splitter or random_splitter. For more details, see FedGraph.     Link-level dataset      Ciao Ciao [7] is a federation recommendation dataset from FedGraphNN, and its task is link classification. For more details, see FedGraph.   FedWN18, FedFB15K-237 FedWN18 and FedFB15K-237 are simulated federation datasets split from WN18 and FB15K-237 [8] by label_space_splitter. For more details, see FedGraph.     Graph-level dataset      Multi-task dataset Multi_task dataset is a federation dataset contains several Sub-datasets named graph_multi_domain_mol, graph_multi_domain_small, graph_multi_domain_mix,  graph_multi_domain_biochem, and graph_multi_domain_molv1. In these datasets, each client holds some graphs from different domains from TUDataset [9], and their task is different from each other.            graph_multi_domain_mol: ‘MUTAG’, ‘BZR’, ‘COX2’, ‘DHFR’, ‘PTC_MR’, ‘AIDS’, ‘NCI1’       graph_multi_domain_small: ‘MUTAG’, ‘BZR’, ‘COX2’, ‘DHFR’, ‘PTC_MR’, ‘ENZYMES’, ‘DD’, ‘PROTEINS’       graph_multi_domain_mix: ‘MUTAG’, ‘BZR’, ‘COX2’, ‘DHFR’, ‘PTC_MR’, ‘AIDS’, ‘NCI1’, ‘ENZYMES’, ‘DD’, ‘PROTEINS’, ‘COLLAB’, ‘IMDB-BINARY’, ‘IMDB-MULTI’       graph_multi_domain_biochem: ‘MUTAG’, ‘BZR’, ‘COX2’, ‘DHFR’, ‘PTC_MR’, ‘AIDS’, ‘NCI1’, ‘ENZYMES’, ‘DD’, ‘PROTEINS’       graph_multi_domain_molv1: ‘MUTAG’, ‘BZR’, ‘COX2’, ‘DHFR’, ‘PTC_MR’, ‘AIDS’, ‘NCI1’, ‘Mutagenicity’, ‘NCI109’, ‘PTC_MM’, ‘PTC_FR’           FedHIV, FedProteins, FedIMDB FedHIV,  FedProteins and FedIMDB are simulated federation datasets split from HIV [10], Proteins [9], IMDB [9] by instance_space_splitter. For more details, see FedGraph.     Recommendation System      MovieLens MovieLens [11] is a series of movie recommendation datasets collected from the website MovieLens. To support different federated settings (VFL/HFL) and various datasets, all the MovieLens datasets inherit two parent classes VMFDataset and HMFDataset, which specific the splitting of MF datasets (VFL or HFL).     Audio and Speech   Coming soon…     Synthetic      Synthetic Mixture Synthetic_Mixture is a synthetic federated dataset from FedEM [12], and its task is binary classification. The data distribution of each client is the mixture of (M) underlying distributions.     Tools     Splitter   To generate simulated federation datasets, we provide splitter who are responsible for dispersing a given standalone dataset into multiple clients, with configurable statistical heterogeneity among them.   For euclidean data:      random_splitter The data is randomly split into 𝑁 subsets with or without intersections.   label_space_splitter It is designed to provide label distribution skew via latent Dirichlet allocation (LDA).   For graph data:      Node-level task            community_splitter: Community detection algorithms such as Louvain are at first applied to partition a graph into several clusters.       random_splitter: The node-set of the original graph is randomly split into 𝑁 subsets with or without intersections.           Link-level task            label_space_splitter: It is designed to provide label distribution skew via latent Dirichlet allocation (LDA).           Graph-level task            instance_space_splitter: It is responsible for creating feature distribution skew (i.e., covariate shift).       multi_task_splitter: Different clients have different tasks.             How to use   For the built-in datasets, you can configure them via a .yaml file.   # Dataset related options data: \t# Root directory where the data stored \troot: 'data' \t# Dataset name \ttype: 'femnist'   # Batch_size for DataLoader \tbatch_size: 64   # Drop last batch of DataLoader \tdrop_last: False   # Shuffle the train DataLoader \tshuffle: True   # Transforms of data \ttransforms: ''   # Subsample of total client \tsubsample: 1.0   # Train, valid, test splits \tsplits: [0.6, 0.2, 0.2] \t# Splitter, if not simulated dataset, disabled.   splitter: 'random'     References   [1] Caldas, Sebastian, et al. “Leaf: A benchmark for federated settings.” arXiv 2018.   [2] Tang, Jie, et al. “Arnetminer: extraction and mining of academic social networks.” SIGKDD 2008.   [3] Deshpande, Yash, et al. “Contextual stochastic block models.” NeurIPS 2018.   [4] McCallum, Andrew Kachites, et al. “Automating the construction of internet portals with machine learning.” Information Retrieval 2000   [5] Giles, C. Lee, Kurt D. Bollacker, and Steve Lawrence. “CiteSeer: An automatic citation indexing system.” Proceedings of the third ACM conference on Digital libraries. 1998.   [6] Sen, Prithviraj, et al. “Collective classification in network data.” AI magazine 2008.   [7] Tang, Jiliang, Huiji Gao, and Huan Liu. “mTrust: Discerning multi-faceted trust in a connected world.” WSDM 2012.   [8] Bordes, Antoine, et al. “Translating embeddings for modeling multi-relational data.” NeurIPS 2013.   [9] Ivanov, Sergei, Sergei Sviridov, and Evgeny Burnaev. “Understanding isomorphism bias in graph data sets.” arXiv 2019.   [10] Wu, Zhenqin, et al. “MoleculeNet: a benchmark for molecular machine learning.” Chemical science 2018.   [11] Harper, F. Maxwell, and Joseph A. Konstan. “The movielens datasets: History and context.” Acm transactions on interactive intelligent systems 2015.   [12] Marfoq, Othmane, et al. “Federated multi-task learning under a mixture of distributions.” NeurIPS 2021.  ","categories": [],
        "tags": [],
        "url": "/docs/datazoo/",
        "teaser": null
      },{
        "title": "ModelZoo",
        "excerpt":"FederatedScope provides many built-in models in different deep learning fields, including Computer Vision, Natural Language Processing, Graph, Recommendation Systems, and Speech. Furthermore, more models are on the way!   To use our ModelZoo, set cfg.model.type = Model_NAME. And you can configure the model-related hyperparameters via ayamlfile.   # Some methods may leverage more than one model in each trainer cfg.model.model_num_per_trainer = 1  # Model name cfg.model.type = 'lr' cfg.model.use_bias = True # For graph model cfg.model.task = 'node' # Hidden dim cfg.model.hidden = 256 # Drop out ratio cfg.model.dropout = 0.5 # in_channels dim. If 0, model will be built by data.shape cfg.model.in_channels = 0 # out_channels dim. If 0, model will be built by label.shape cfg.model.out_channels = 1 # In GPR-GNN, K = gnn_layer cfg.model.gnn_layer = 2 cfg.model.graph_pooling = 'mean' cfg.model.embed_size = 8 cfg.model.num_item = 0 cfg.model.num_user = 0   For more model-related settings, please refer to each model.     Computer Vision      ConvNet2 ConvNet2 (from federatedscope/cv/model) is a two-layer CNN for image classification. (cfg.model.type = 'convnet2')     class ConvNet2(Module):  def __init__(self, in_channels, h=32, w=32, hidden=2048, class_num=10, use_bn=True):      ...           ConvNet5 ConvNet5 (from federatedscope/cv/model) is a five-layer CNN for image classification. (cfg.model.type = 'convnet5')     class ConvNet5(Module):  def __init__(self, in_channels, h=32, w=32, hidden=2048, class_num=10):      ...           VGG11 VGG11 [1] (from federatedscope/cv/model) is an 11 layer CNN with very small (3x3) convolution filters for image classification. It is from Very Deep Convolutional Networks for Large-Scale Image Recognition. (cfg.model.type = 'vgg11')     class VGG11(Module):  def __init__(self, in_channels, h=32, w=32, hidden=128, class_num=10):      ...             Natural Language Processing      LSTM LSTM [2] (from federatedscope/nlp/model) is a type of RNN that solves the vanishing gradient problem through additional cells, input and output gates. (cfg.model.type = 'lstm')     class LSTM(nn.Module):  def __init__(self, in_channels, hidden, out_channels, n_layers=2, embed_size=8):      ...             Graph      GCN GCN [3] (from federatedscope/gfl/model) is a kind of Graph Neural Networks from Semi-supervised Classification with Graph Convolutional Networks, which is adapted for node-level, link-level and graph-level tasks. (cfg.model.type = 'gcn', cfg.model.task = 'node')     class GCN_Net(torch.nn.Module):  def __init__(self, in_channels, out_channels, hidden=64, max_depth=2, dropout=.0):      ...           GAT GAT [4] (from federatedscope/gfl/model) is a kind of Graph Neural Networks from Graph Attention Networks. GAT employ attention mechanisms to node neighbors to learn attention coefficients, which is adapted for node-level, link-level and graph-level tasks.  (cfg.model.type = 'gat', cfg.model.task = 'node' # node, link or graph)     class GAT_Net(torch.nn.Module):  def __init__(self, in_channels, out_channels, hidden=64, max_depth=2, dropout=.0):      ...           GraphSAGE GraphSAGE [5] (from federatedscope/gfl/model) is a general inductive GNN framework, from Inductive Representation Learning on Large Graphs. GraphSAGE learns a function that generates embeddings by sampling and aggregating from the local neighborhood of each node, which is adapted for node-level and link-level tasks.  (cfg.model.type = 'sage', cfg.model.task = 'node' # node, link or graph)     class SAGE_Net(torch.nn.Module):  def __init__(self, in_channels, out_channels, hidden=64, max_depth=2, dropout=.0):      ...           GPR-GNN GPR-GNN [6] (from federatedscope/gfl/model) adaptively learns the Generalized PageRank weights so as to jointly optimize node feature and topological information extraction from Adaptive Universal Generalized PageRank Graph Neural Network, which is adapted for node-level and link-level tasks.  (cfg.model.type = 'gpr', cfg.model.task = 'node' # node or link)     class GPR_Net(torch.nn.Module):  def __init__(self, in_channels, out_channels, hidden=64, K=10, dropout=.0, ppnp='GPR_prop', alpha=0.1, Init='PPR'):      ...           GIN GIN [7] (from federatedscope/gfl/model) generalizes the Weisfeiler-Lehman test and achieves maximum discriminative power among GNNs from How Powerful are Graph Neural Networks? which is adapted for graph-level tasks.  (cfg.model.type = 'gin', cfg.model.task = 'graph')     class GIN_Net(torch.nn.Module):  def __init__(self, in_channels, out_channels, hidden=64, max_depth=2, dropout=.0):      ...             Recommendation System      MF models MF model [8] (from federatedscope/mf/model) has two trainable parameters: user embedding and item embedding. Based on the given federated setting, they share different embedding with the other participators. FederatedScope achieves VMFNetand HMFNetto support federated MF, and both of them inherit the basic MF model class BasicMFNet.   class VMFNet(BasicMFNet):     name_reserve = \"embed_item\"   class HMFNet(BasicMFNet):     name_reserve = \"embed_user\"      Speech   Coming Soon!     References   [1] Simonyan, Karen, and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” arXiv preprint arXiv 2014.   [2] Hochreiter, Sepp, and Jürgen Schmidhuber. “Long short-term memory.” Neural computation 1997.   [3] Kipf, Thomas N., and Max Welling. “Semi-supervised classification with graph convolutional networks.” arXiv 2016.   [4] Veličković, Petar, et al. “Graph attention networks.” ICLR 2018.   [5] Hamilton, Will, Zhitao Ying, and Jure Leskovec. “Inductive representation learning on large graphs.” NeurIPS 2017.   [6] Chien, Eli, et al. “Adaptive universal generalized pagerank graph neural network.” ICLR 2021.   [7] Xu, Keyulu, et al. “How powerful are graph neural networks?.”  ICLR 2019.   [8] Yongjie, Du, et al. “Federated matrix factorization for privacy-preserving recommender systems” VLDB 2022.  ","categories": [],
        "tags": [],
        "url": "/docs/modelzoo/",
        "teaser": null
      },{
        "title": "AlgoZoo",
        "excerpt":"FederatedScope has built in various advanced federated learning algorithms. All of them are implemented as plug-ins, which are detachable and combinable.      Detachable: only the activated code will participate in the operation,   Combinable: different algorithms can be combined.   In this tutorial, you will learn the buildin algorithms, and how to implement a new federated algorithm in FederatedScope.   Buildin Methods   Distributed Optimization Methods  To tackle the challenge of statistical heterogeneity, we implement the following distributed optimization methods: FedAvg (Default), FedProx and FedOpt. Set the parameter cfg.{METHOD_NAME}.useas True to call them.   FedAvg  FedAvg [1] is a basic distributed optimization method in federated learning. During federated training, it broadcasts the initialized model to all clients, and aggregates the updated weights collected from several clients. FederatedScope implements it with a fedavg aggregator. More details can be found in federatedscope/core/aggregator.py.   We provide some evaluation results for fedavg on different tasks as follows.                  Task       Data       Accuracy(%)                       Logistic regression       Synthetic       68.36                 Image classification       FEMNIST       84.93                 Next-character Prediction       Shakespeare       43.80           To reproduce the results, the running scripts are listed as follows.  # logistic regression python federatedscope/main.py --cfg federatedscope/nlp/baseline/fedavg_lr_on_synthetic.yaml  # image classification on femnist python federatedscope/main.py --cfg federatedscope/cv/baseline/fedavg_convnet2_on_femnist.yaml  # next-character prediction on Shackespeare python federatedscope/main.py --cfg federatedscope/nlp/baseline/fedavg_lstm_on_shakespeare.yaml   FedOpt  FedOpt [2] is an advanced distributed optimization method in federated learning. Compare with FedAvg, it permits the server to update weights rather than simply averaging the collected weights. More details can be found in federatedscope/core/aggregator.py.   Similar with FedAvg, we perform some evaluation of FedAvg on different tasks.                  Task       Data       Learning rate (Server)       Accuracy(%)                       Logistic regression       Synthetic       0.5       68.32                 Image classification       FEMNIST       1.0       84.92                 Next-character Prediction       Shakespeare       0.5       47.39           FedProx  FedProx [3] is designed to solve the problem of heterogeneity, which updates model with a proximal regularizer. FederatedScope provides build-in FedProx implementation and it can easily be combined with other algorithms. More details can be found in federatedscope/core/trainer/flpackage/core/trainers/trainer_fedprox.py.   The evaluation results are presented as follows.                  Task       Data       $\\mu$       Accuracy(%)                       Logistic regression       Synthetic       0.1       68.36                 Image classification       FEMNIST       0.01       84.77                 Next-character Prediction       Shakespeare       0.01       47.85           Personalization Methods   FedBN   FedBN [4] is a simple yet effective approach to address feature shift non-iid challenge, in which the client BN parameters are trained locally, without communication and aggregation via server. FederatedScope provides simple configuration to implement FedBN and other variants that need to keep parameters of some model sub-modules local.   We provide some evaluation results for FedBN on different tasks as follows, in which the models contain batch normalization. Complete results, config files and running scripts can be found in scripts/personalization_exp_scripts/fedbn.                  Task       Data       Accuracy (%)                       Image classification       FEMNIST       85.48                 Graph classification       multi-task-molecule       72.90           pFedMe   pFedMe [5]  is an effective pFL approach to address data heterogeneity, in which the personalized model and global model are decoupled with Moreau envelops. FederatedScope implements pFedMe in federatedscope/core/trainers/trainer_pFedMe.py and ServerClientsInterpolateAggregator in federatedscope/core/aggregator.py.   We provide some evaluation results for pFedMe on different tasks as follows. Complete results, config files and running scripts can be found in scripts/personalization_exp_scripts/pfedme.                  Task       Data       Accuracy (%)                       Logistic regression       Synthetic       68.73                 Image classification       FEMNIST       87.65                 Next-character Prediction       Shakespeare       37.40           Ditto   Ditto [6] is a SOTA pFL approach that improves fairness and robustness of FL via training local personalized model and global model simultaneously, in which the local model update is based on regularization to global model parameters. FederatedScope provides built-in Ditto implementation and users can easily extend to other pFL methods by re-using the model-para regularization. More details can be found in federatedscope/core/trainers/trainer_Ditto.py.   We provide some evaluation results for Ditto on different tasks as follows. Complete results, config files and running scripts can be found in scripts/personalization_exp_scripts/ditto.                  Task       Data       Accuracy (%)                       Logistic regression       Synthetic       69.67                 Image classification       FEMNIST       86.61                 Next-character Prediction       Shakespeare       45.14           FedEM  FedEM [7] is a SOTA pFL approach that assumes local data distribution is a mixture of unknown underlying distributions, and correspondingly learn a mixture of multiple internal models with Expectation-Maximization learning. FederatedScope provides built-in FedEM implementation and users can easily extends to other multi-model pFL methods based on this example. More details can be found in federatedscope/core/trainers/trainer_FedEM.py.   We provide some evaluation results for FedBN on different tasks as follows. Complete results, config files and running scripts can be found in scripts/personalization_exp_scripts/fedem.                  Task       Data       Accuracy (%)                       Logistic regression       Synthetic       68.80                 Image classification       FEMNIST       84.79                 Next-character Prediction       Shakespeare       48.06           Implementation  Preliminary  Before implementing a new federated algorithm, you need to realize the structure of Trainer  and Context. If you already have the knowledge about them, in this part we’ll learn how to add an algorithm in FederatedScope.   In FederatedScope, there are three steps to implement a new federated algorithm:      Prepare parameters: figure out the parameters required by your algorithm, and fill them into the Context   Prepare hook functions: split your algorithm into several functions according to their insert positions within Trainer/Server,   Assemble algorithm: create a warp function to assemble your algorithm before create the trainer object.   Example (Fedprox)  Let’s take FedProx as an example to show how to implement a new federated algorithm.   Prepare parameters  First, FedProx requires to set proximal regularizer and its factor ctx.regularizer.mu.   # ------------------------------------------------------------------------ # # Init variables for FedProx algorithm # ------------------------------------------------------------------------ # def init_fedprox_ctx(base_trainer):     ctx = base_trainer.ctx     cfg = base_trainer.cfg      cfg.regularizer.type = 'proximal_regularizer'     cfg.regularizer.mu = cfg.fedprox.mu      from federatedscope.core.auxiliaries.regularizer_builder import get_regularizer     ctx.regularizer = get_regularizer(cfg.regularizer.type)   Prepare Hook Functions  During training,FredProxrequires to record the initalized weights before local updating. Therefore, we create two hook functions to maintain the initialized weights.      record_initialization: record initialized weights, and   del_initialization: delete initialized weights to avoid memory leakage   # ------------------------------------------------------------------------ # # Additional functions for FedProx algorithm # ------------------------------------------------------------------------ # def record_initialization(ctx):     ctx.weight_init = deepcopy(         [_.data.detach() for _ in ctx.model.parameters()])   def del_initialization(ctx):     ctx.weight_init = None   Assemble algorithm  After preparing parameters and hook functions, we assemble FedProx within the function wrap_fedprox_trainer in two steps:      initialize parameters (call functioninit_fedprox_ctx)   register hook functions for the given trainer   def wrap_fedprox_trainer(         base_trainer: Type[GeneralTrainer]) -&gt; Type[GeneralTrainer]:     \"\"\"Implementation of fedprox refer to `Federated Optimization in Heterogeneous Networks` [Tian Li, et al., 2020]         (https://proceedings.mlsys.org/paper/2020/file/38af86134b65d0f10fe33d30dd76442e-Paper.pdf)      \"\"\"      # ---------------- attribute-level plug-in -----------------------     init_fedprox_ctx(base_trainer)      # ---------------- action-level plug-in -----------------------     base_trainer.register_hook_in_train(new_hook=record_initialization,                                         trigger='on_fit_start',                                         insert_pos=-1)      base_trainer.register_hook_in_eval(new_hook=record_initialization,                                        trigger='on_fit_start',                                        insert_pos=-1)      base_trainer.register_hook_in_train(new_hook=del_initialization,                                         trigger='on_fit_end',                                         insert_pos=-1)      base_trainer.register_hook_in_eval(new_hook=del_initialization,                                        trigger='on_fit_end',                                        insert_pos=-1)      return base_trainer   Finally, add FedProx into the function get_trainer(federatedscope/core/auxiliaries/trainer_builder.py).   def get_trainer(model=None,                 data=None,                 device=None,                 config=None,                 only_for_eval=False,                 is_attacker=False):          trainer = ...          # fed algorithm plug-in     if config.fedprox.use:         from federatedscope.core.trainers.trainer_fedprox import wrap_fedprox_trainer         trainer = wrap_fedprox_trainer(trainer)   Run an Example  Generally, build-in algorithms are called by setting the parameter $cfg.{METHOD_NAME}.use as True. For more infomation about their parameters, you can refer to federatedscope/config.py. Similarily, taking FedProx as an exmple, its parameters in federatedscope/core/config.py are   # ------------------------------------------------------------------------ # # fedprox related options # ------------------------------------------------------------------------ # cfg.fedprox = CN()  cfg.fedprox.use = True\t\t# Whether to use fedprox cfg.fedprox.mu = 0. \t\t# The regularizer factor within fedprox   You can call FedProx by the following command in the terminal   python federatedscope/main.py --cfg {YOUR_CONFIG_FILE} fedprox.use True fedprox.mu 0.1   More example scripts are refer toscripts/example_configs/.   Note  Most combinations of the buildin methods have been tested. When implementing your own methods, it is suggested to carefully check the code to avoid conflicts (e.g. duplication of variables).    References  [1] McMahan B, Moore E, Ramage D, et al. “Communication-efficient learning of deep networks from decentralized data”. International Conference on Artificial Intelligence and Statistics, 2017.   [2] Reddi S J, Charles Z, Zaheer M, et al. “Adaptive federated optimization”. Intertional Conference on Learning Representations, 2021.   [3] Li T, Sahu A K, Zaheer M, et al. “Federated optimization in heterogeneous networks”. Proceedings of Machine Learning and Systems, 2020.   [4] Li X, Jiang M, Zhang X, et al. “Fedbn: Federated learning on non-iid features via local batch normalization”. arXiv preprint arXiv:2102.07623 (2021).   [5] Dinh C T, Tran N H, and Nguyen T D. “Personalized federated learning with moreau envelopes”. Advances in Neural Information Processing Systems 33 (2020): 21394-21405.   [6] Li T, Hu S, Beirami A, et al. “Ditto: Fair and robust federated learning through personalization”. International Conference on Machine Learning. PMLR, 2021.   [7] Marfoq O, Neglia G, Bellet A, et al. “Federated multi-task learning under a mixture of distributions”. Advances in Neural Information Processing Systems 34 (2021).   ","categories": [],
        "tags": [],
        "url": "/docs/algozoo/",
        "teaser": null
      },{
        "title": "Tuning Federated Learning",
        "excerpt":"It is well-known that deep neural networks are often sensitive to their hyperparameters, which need to be tuned carefully. When it comes to federated learning (FL), there are additional hyperparameters concerning the FL behaviors, which include the number of steps to make local update (federate.local_update_steps), the ratio of clients sampled at each round (federate.sample_client_rate), the coefficient of proximal regularization in FedProx (fedprox.mu), etc. Any poor choices for these hyperparameters would lead to unsatisfactory performances or even divergence.   Therefore, FederatedScope has provided the functionality of hyperparameter optimization (HPO), which saves our users from the tedious trial-and-error procedure.   An hands-on exercise   We encourage our users to try the provided toy example of HPO by:   python demo/bbo.py   This toy example shows how to use Gaussian Process (GP) algorithm to seek the appropriate learning rate (optimizer.lr) for a logistic regression model. There are more such scripts in this demo/ folder to show how to use popular HPO packages (e.g., SMAC3 and HpBandSter) to interact with FederatedScope. After reading these script, users would be able to conduct HPO for their own FL cases with FederatedScope.   Meanwhile, we have implemented several popular HPO methods in the autotune module of FederatedScope, including random search [1], Successive Halving Algorithm (SHA) [2], etc. Users could try the rando search algorithm by:   python federatedscope/hpo.py --cfg scripts/example_configs/toy_rs.yaml   How to declare the search space?  At first, any HPO procedure starts with declaring the search space, say that, which hyperparameters need to be determined and what are the candidate choices for them. FederatedScope allows users to specify the search space via the argument hpo.ss (in the above example, it is specified as ‘scripts/example_configs/toy_hpo_ss.yaml’). As you can see from this .yaml file, the search space of each hyperparameter is described as a dict:   train.optimizer.lr:   type: float   lower: 0.001   upper: 0.5   log: True train.optimizer.weight_decay:   type: cate   choices: [0.0, 0.0005, 0.005]   where the key type specifies whether the search space of this hyperparameter is continuous or categorical. Then other keys and values correspond to the argument names and values for instantiating UniformFloatHyperparameter or CategoricalHyperparameter objects (see ConfigureSpace package).   How the scheduler interact with the FL runner?  Up to now, we have declared the search spaces. As random search algorithm has been adopted (hpo.init_strategy == 'random'  and default BruteForce scheduler is adopted), FederatedScope will randomly sample a specified number of candidate configurations from the declared search spaces. Specifically, to sample each configuration, the considered hyperparameters are enumerated, and choice for each hyperparameter is sampled from its search space uniformly:     optimizer.weight_decay ~ Uniform({0.0, 0.0005, 0.005}),    It is worth mentioning that users are allowed to apply the uniform distribution to any continuous search space with log scale (by setting hpo.log_scale to be True), which is a convention in machine learning, e.g., considering learning rates 0.001, 0.01, and 0.1. After acquiring the specified number of candidate configurations, the scheduler, as an HPO agent, is going to interact with the FL runner, where each configuration is attempted (i.e., evaluated). Such interactions are repeated again and again till the agent has determined which configuration is the optimal, as the following figure illustrats.      Results of HPO  Executing this example eventually produces some outputs like the following:      where, as you can see, the scheduler has evaluated 16 configurations each of which takes different choices of the learning rate and the weight decay coefficient. Since we have (by default) chosen the test loss as the evaluation metric (hpo.metric == 'client_summarized_weighted_avg.test_loss), the optimal configuration is the one that has the smallest value in the ‘performance’ column (by default, hpo.larger_better == False). Obvioiusly, taking optimizer.lr=0.228037 and optimizer.weight_decay=.0 is the best.   More HPO algorithms   In addition to the simple search strategies including random search and grid search, FederatedScope has provided more sophisticated HPO algorithms, e.g., Hyperband (variant of Successive Halving Algorithm (SHA)) [2] and Population-Based Training (PBT) [3]. Users can specify which HPO algorithm to be used by setting hpo.scheduler.   Taking SHA as an example, we can try it by:   python federatedscope/hpo.py --cfg federatedscope/example_configs/toy_sha.yaml   As the default values for SHA are hpo.sha.elim_round_num=3 and hpo.sha.elim_rate=3, the scheduler begins with $3 \\times 3=27$ randomly sampled configurations. Then the search procedure continues iteratively. At the first round, all these 27 configurations are evaluated, and then only the top 1/3 candidates are reserved for the next round. In the next round, the reserved 9 configurations are evaluated, where each FL course is restored from the checkpoint resulted from the last round. The scheduler repeat such iterations untill there is only one configuration remaining. We just show the outputs of the final round:      Obviously, the configuration to be reserved as the winner consists of the choices optimizer.lr=0.223291 and optimizer.weight_decay=0.0005.   Try it yourself   In this post, we have introduced the HPO functionalities of FederatedScope via two examples, i.e., random search and SHA. We encourage users to try these HPO algorithms on your own dataset (simply modify the data and model related fields in the .yaml file). For more details about the HPO functionalities of FederatedScope, please look up the API references.   References   [1] Bergstra, James, and Yoshua Bengio. “Random search for hyper-parameter optimization.” Journal of machine learning research 13.2 (2012).   [2] Li, Lisha, et al. “Hyperband: A novel bandit-based approach to hyperparameter optimization.” The Journal of Machine Learning Research 18.1 (2017): 6765-6816.   [3] Jaderberg, Max, et al. “Population based training of neural networks.” arXiv preprint arXiv:1711.09846 (2017).  ","categories": [],
        "tags": [],
        "url": "/docs/use-hpo/",
        "teaser": null
      },{
        "title": "Event-driven Architecture",
        "excerpt":"Before introducing more details about how to develop customized FL procedures for various real-world applications with FederatedScope, it is necessary to present the event-driven architecture for you. On the one hand, our design philosophy can provide users with another perspective for federated learning, which is different from the widely-adopted sequential process and might bring benefits for customizing. On the other hand, the introduction of the event-driven architecture can help users to know how to implement using provided rich functional components or extending fancy features with FederatedScope conveniently and effortlessly.   Infrastructure  We first present the infrastructure of FederatedScope, showing how an FL training course can be framed and expressed in the event-handler pairs, and why the desgin of modules in FederatedScope can makes it easy and flexible for users to program an FL course.   Event-Handler  FederatedScope is implemented with an event-driven architecture, which is widely used in distributed systems [1, 2] and recently applied to federated learning [3, 4]. Based on the event-driven architecture, an FL course can be framed into event-handler pairs: the participants wait for certain events (e.g., model’s parameters are broadcast to the clients) to trigger corresponding handlers (e.g., training models based on the local data).  Hence, users can express the behaviors of servers and clients independently from respective perspectives rather than sequentially from a global perspective, i.e., the case in a procedural programming paradigm. As a result, it becomes easier for users to implement FL algorithms, and the implementations are finer modularized.   Specifically, the events in FederatedScope can be categorized into two classes. One is related to message passing (e.g., receiving information or request from others), and the other comes from the certain conditions for training (e.g., timeout and staleness toleration in asynchronous machine learning algorithms). These two types of events with corresponding handlers provide FederatedScope sufficient expressive power to describe and execute asynchronous federated training.   Modules  FederatedScope consists of two basic modules for implementing an FL procedure, i.e., Worker Module and Communication Module.   Worker Module   The worker module is used to describe the behaviors of servers and clients in an FL course, which is designed towards expressing rich behaviors conveniently.   We categorize the behaviors of servers and clients into federated behaviors and training behaviors. The federated behaviors include various message exchanges among participants, for example, a client sends a join-in application to the server, and a server broadcasts the model parameter to clients for starting a new training round. The training behaviors mainly denote the operations of updating models, including a client performing local training based on its data, a server aggregating the feedbacks (e.g., the updated models) from clients to generate the global model, and so on.   Considering both flexibility and convenience, we define the following basic member attributes of a worker (a server or a client) to decouple the federated behaviors and training behaviors:      ID: The worker’s ID is unique in an FL course for distinguishing. By default, we assign the server with “0” and number the clients according to the order they joined the FL course.   Data &amp; Model: Each worker holds its data and/or model locally. According to the settings of FL, the data are stored in the worker’s private space and won’t be directly shared directly because of privacy concerns. A client usually owns a training dataset and might have a local test dataset,  while a server might hold a dataset for global evaluation.   Besides, each worker maintains a model locally. In a vanilla FL course, all clients share the same model architecture, and the model parameters are synchronized through the server by aggregating. Since the local data are isolated and cannot be shared directly, the knowledge of these data is encoded into the models via local training and shared across participants in a privacy-preserving manner. Recent studies on personalized FL [5, 6] propose to control the extent of learning from the global models during an FL course, which might cause differences among the local models.      Communicator: Each worker holds a communicator object for message exchange. The communicator hides the low-level details of the communication backends and only exposes high-level interfaces for workers, such as send and receive.   Trainer/Aggregator: A client/server holds a trainer/aggregator to encapsulate its training behaviors, which has the authority to access/update data and models. The trainer/aggregator manages the training details, such as loss function, optimizer, aggregation algorithm. The client/server only needs to call the high-level interfaces (e.g., train, eval, aggregate) without caring about the training details. In this way, we decouple the training behaviors and the federated behaviors of workers.   Based on the aforementioned attributes, users can conveniently express the rich behaviors of servers and clients for various FL applications. For example, users can customize the aggregation algorithms in the Aggregator without caring about federated behaviors and model architectures.   Communication Module   An FL course might need to exchange various types of information among participants, such as the model parameters, gradients, and also join-in applications, signal of finish, etc. In FederatedScope, we abstract all the exchanged information among servers and clients as “Message”.   The message is one of the key objects to describe and drive an FL course in FederatedScope. To this end, the message is designed to contain type, sender, receiver, and payload. Different FL applications need different types of messages. For a vanilla FL course, servers and clients exchange model parameters during the training process, while for Graph Federated Learning tasks [7, 8], node embeddings, adjacency tensors might be needed to share among participants.   Users can define various types of messages according to the unique requirements of customized FL courses, and, at the same time, describe the handling functions of servers and clients to handle these received messages. Thus, a customized FL course can be implemented by adding new types of messages and the corresponding handling functions.   The detailed implementation of adding new types of exchanged messages and behaviors of servers and clients can be found in New Types of Messages and Handlers.   Besides, a communicator is used for participants to exchange messages with each other, which can be regarded as a black box viewed by servers and clients, since only high-level interfaces (such as send and receive) are exposed to them. The communicator hides the low-level implementation of backends so that it can provide a unified view for both standalone simulation and distributed deployment.   In FederatedScope, we implement a simulated communicator for standalone mode, and a gRPC communicator for distributed mode. Users can implement more communicators based on various protocols according to the adopted environments.   Implementation   In FederatedScope, an FL course is framed to multiple rounds of message passing. In general, users need to abstract the types of exchanged messages in the FL course, and then transform the behaviors of servers and clients into handling functions as subroutines to handle different types of received messages.   Next we will introduce two examples to better demonstrate how to construct an FL course and how to customize.   Construct an FL Course    A vanilla FL course viewed from the perspective of message passing is shown in the figure. Using the procedural programming paradigm, developers need to carefully coordinate the participants. For example, firstly the clients send the join-in application to the server, and then the server receives these applications, and broadcasts the models to them, after that … It can become too complicated for developers to implement an FL course.   On the other hand, programming with FederatedScope, to perform the vanilla FedAvg, users first need to abstract the types of exchanged messages and the corresponding handlers: The server needs to handle two types of message, i.e., handle join for admitting a new client to join in the FL course, and handle the updated models to perform aggregation. As for the clients, they train the model on the local data and return the updated model when receiving model from the server.  Finally, the FL course can be triggered by the instantiated clients sending join to the server.   The event-driven architecture allows users to focus on the behaviors as subroutines in an FL course and saves users’ effort in coordinating the participants. And it also brings flexibility to support various FL applications that require heterogeneous message exchanging and rich behaviors, as shown in another example below.   Customization    We demonstrate another example in the figure to show how to customize a real-world FL application where heterogeneous messages are exchanged and handled. Compared to the vanilla FL course, here clients need to exchange intermediate results during each training round. To achieve this, developers who use the procedural programming paradigm are required to describe the complete FL course sequentially, add the new behaviors into the procedures after carefully positioning.   For comparison, to express heterogeneous information change and rich behaviors,  developers who use FederatedScope only need to define the new types of messages and the corresponding handling function of servers and clients, eliminating the efforts for coordinating participants.   For example, developers only need to add a new type intermediate results for clients, and specify that clients continue training locally when receiving the intermediate results, without bothering by when the new behavior (i.e., exchanging intermediate results) happens or how many times it happens in an FL course.   In a nutshell, based on the event-driven architecture, FederatedScope is well-modularized toward flexibility and extensibility for promoting the various FL applications. Also, it provides a unified view for both standalone mode and distributed mode, which helps users change from simulation to deployment effortlessly.   References   [1] Kreps, Jay, Neha Narkhede, and Jun Rao. “Kafka: A distributed messaging system for log processing.” Proceedings of the NetDB. Vol. 11. 2011.  [2] Michelson, Brenda M. “Event-driven architecture overview.” Patricia Seybold Group 2.12 (2006): 10-1571.  [3] He, Chaoyang, et al. “Fedml: A research library and benchmark for federated machine learning.” arXiv preprint arXiv:2007.13518 (2020).  [4] Chadha, Mohak, Anshul Jindal, and Michael Gerndt. “Towards federated learning using faas fabric.” Proceedings of the 2020 Sixth International Workshop on Serverless Computing. 2020.  [5] Tan A Z, Yu H, Cui L, et al. “Towards personalized federated learning”. arXiv preprint arXiv:2103.00710, 2021.  [6] Fallah A, Mokhtari A, Ozdaglar A. “Personalized federated learning: A meta-learning approach”. arXiv preprint arXiv:2002.07948, 2020.  [7] Wu C, Wu F, Cao Y, et al. “Fedgnn: Federated graph neural network for privacy-preserving recommendation”. arXiv preprint arXiv:2102.04925, 2021.  [8] Zhang K, Yang C, Li X, et al. “Subgraph federated learning with missing neighbor generation”. Advances in Neural Information Processing Systems, 2021, 34.  ","categories": [],
        "tags": [],
        "url": "/docs/event-driven-architecture/",
        "teaser": null
      },{
        "title": "New Types of Messages and Handlers",
        "excerpt":"with the help of the Event-driven Architecture, FederatedScope allows developers to customize FL applications via introducing new types of  messages and the corresponding handling functions. Here we provide the implementation details.   Define new message type   Firstly developers should define a new type of message that is exchanged in the customized FL course. The new message should include sender, receiver, msg_type, and payload.   For example, we define Message(sender=server, receiver=client, msg_type='gradients', payload=MODEL_GRADIENTS) to denote a new message containing gradients that is passed from the server to the client.   Add handling function   After that, users should implement the handling function for the receiver (here is the client) to handle the newly defined message. The operations in the handling function might include parsing the payload, updating models, aggregating, triggering some events, returning feedback,  and so on. For example:   class Client(object):      ... ...          # A handling function of client for 'gradients'     def callback_for_messgae_gradients(self, message):         # parse the payload         sender, model_gradients = message.sender, message.content         assert sender == self.server_ID              # update model via trainer         self.trainer.update_by_gradients(model_gradients)              # trigger some events         if\tself.trainer.get_delta_of_model() &gt; self.threshold:             # local training             updated_model = self.trainer.local_train()         else:             updated_model = self.model              # return the feedback via communivator         self.comm_manager.send(             Message(sender=self.ID,                      receiver=sender,                      msg_type='updated_model',                      content=updated_model))   Note that in some cases, the newly added handling function includes returning a message, such as updated_model in the example. Users might need to define a new handling function for the returned message if it is also a new type, or accordingly modify the implemented handling functions if necessary.   Register the handling function   FederatedScope allows users to add the new handling functions for servers or clients by registering:   self.register_handlers(     message_type='gradients',      callback_func=callback_for_messgae_gradients)   Thus, a new type of message can be exchanged and handled in a customized FL task.  ","categories": [],
        "tags": [],
        "url": "/docs/new-type/",
        "teaser": null
      },{
        "title": "Privacy Protection for Message",
        "excerpt":"In order to satisfy different requirements of message privacy protection,  several technologies, including Differential Privacy, Encryption, Multi-Party Computation, etc., are applied in an FL course to enhance the strength of protection.   Here we give a brief introduction of how to use these technologies in FederatedScope.   Differential Privacy   Differential privacy (DP) is a powerful theoretical criterion for privacy. A mechanism satisfying $(\\epsilon,\\delta)-$differential privacy promises the indistinguishability of similar information with the possibility $1-\\delta$.   In FL courses, participants usually exchanges messages at high frequency. Differential privacy provides a powerful theoretical metric to evaluate the balance between training performance and privacy protection.   To protect the privacy contained in the message, FederatedScope provides flexible supports for differential privacy:      Abundant benchmark datasets and tasks are preset in FederatedScope, e.g., CV, graph learning, NLP and recommendation tasks. It’s quite convenient and simple to conduct DP evaluations under different settings;   Build-in DP algorithms are user-friendly for the beginners;   Flexible DP APIs are preset for users to develop and implement their own DP algorithms;   Rich attackers are implemented in FederatedScope. It’s convenient to test your DP algorithm.   Encryption   One of the techniques to protect the privacy of messages is encryption. Before sending messages to others, participants can apply encryption algorithms to transfer the plaintext to ciphertext, which prevents privacy leakage during the communication.   When receiving the ciphertext, the receiver usually needs to recover the plaintext via decryption. However, if homomorphic encryption algorithms, such as Paillier [1], are adopted, the receiver is allowed to perform (limited) operations on the ciphertext, which ensures the correctness of operations as well as prevents the receiver from being aware of the plaintext.  Readers can refer to Cross-silo FL for an example of using homomorphic encryption algorithms in an FL course. Note that users can extend more encryption algorithms in FederatedScope without caring about other modules since they have been decoupled with each other.   The biggest weakness of applying encryption in an FL course is bringing additional costs for both computation and communication. It might be intolerable in an FL course, especially for cross-device scenarios that involve IoT devices or smartphones that have limited computation resources and communication width.   Secure Multi-Party Computation   Secure Multi-Party Computation (SMPC) aims to jointly compute a function by multiple participants while keeping the original inputs private. Formally, given$n$participants and each participant$i$owns the private data$x_i$. SMPC proposes to learn$y=f(x_1, x_2, …, x_n)$without exposing the values of $x_1, x_2, …, x_n$to other participants. We show an example of additive secret sharing in the figure with instantiating$f$as MEAN operation, which can be used in vanilla FedAvg [2].   To protect the privacy of input$x_i$, the client splits it into several frames and makes sure that $x_{i,1} + x_{i,2}+ … + x_{i,n}=x_i$( where$n-1$frames are randomly chosen from$[p]$ and $p$is a large number). After that, the clients exchange the frames and sum the received frames up, and send the results to the server. In this way, what the server receives is a mixture of frames. Meanwhile, when the server performs the MEAN operation on these received mixture frames, it happens to be the same result as performing the MEAN operation on the original inputs.   In FederatedScope, we provide the fundamental functionalities for the additive secret sharing.  Uers only need to utilize the high-level interface such as secret_split after specifying the number of the shared parties (i.e., the number of frames).  Note that the configuration cfg.federate.use_ss controls whether to apply additive secret sharing in the FL course. When it is set to be True, the client splits each element of messages into several frames and sends one frame to every client. At the same time, the client receives$n-1$frames from others. The results of applying additively secret sharing can be demonstrated as (we use a small value of $p=2^{15}+1=32769$in this example for better understanding, and preserve two decimals):   ### Before using secret_split: tensor([43.74])  ### After using secret_split: [[21712.]   [ 1075.]   [14356.]]   ### Recover: (21712. + 1075. + 14356.) mod p / 1e2 = 43.74   Furthermore, more SMPC algorithms can be implemented in FederatedScope to support various FL applications. We aim to provide more support for SMPC in the future.   References   [1] Paillier P. “Public-key cryptosystems based on composite degree residuosity classes”. International conference on the theory and applications of cryptographic techniques. 1999: 223-238.  [2] McMahan B, Moore E, Ramage D, et al. “Communication-efficient learning of deep networks from decentralized data”. Artificial intelligence and statistics. 2017: 1273-1282.  ","categories": [],
        "tags": [],
        "url": "/docs/protected-msg/",
        "teaser": null
      },{
        "title": "Local Learning Abstraction: Trainer",
        "excerpt":"FederatedScope decouples the local learning process and details of FL communication and schedule, allowing users to freely customize local learning algorithm via the trainer. Each worker holds a trainer object to manage the details of local learning, such as the loss function, optimizer, training step, evaluation, etc.   In this tutorial, you will learn:      The structure of Trainer used in FederatedScope;   How the Trainer maintains attributes and how to extend new attributes?   How the Trainer maintains learning behaviors and how to extend new behaviors?   How to extend  Trainer to learn with more than one internal model?   Trainer Structure  A typical machine learning process consists of the following procedures:      Preparing datasets and pre-extracting data mini-batches   Iterations over training datasets to update the model parameters   Evaluation the quality of learned model on validation/evaluation datasets   Saving, loading, and monitoring the model and intermediate results      As the figure shows, in FederatedScope Trainer,  these above procedures are provided with high-level routines abstraction, which are made up of Context class and several pluggable Hooks.      The Context class is used to holds learning-related attributes, including data, model, optimizer and etc. We will introduce more details in next Section.     self.ctx = Context(model,                  self.cfg,                  data,                  device,                  init_dict=self.parse_data(data))           The Hooks represent fine-grained learning behaviors at different point-in-times, which provides a simple yet powerful way to customize learning behaviors with a few modifications and easy re-use of fruitful default hooks. More details about the behavior customization are in following Section. ```python HOOK_TRIGGER = [       “on_fit_start”, “on_epoch_start”, “on_batch_start”, “on_batch_forward”,       “on_batch_backward”, “on_batch_end”, “on_epoch_end”, “on_fit_end”   ] self.hooks_in_train = collections.defaultdict(list)     By default, use the same trigger keys      self.hooks_in_eval = copy.deepcopy(self.hooks_in_train) self.hooks_in_ft = copy.deepcopy(self.hooks_in_train)       register necessary hooks into self.hooks_in_train and  self.hooks_in_eval  if not only_for_eval:     self.register_default_hooks_train() if self.cfg.finetune.before_eval:     self.register_default_hooks_ft() self.register_default_hooks_eval()   ## Trainer Behaviors  ### Routines  - Besides the common I/O procedures `save_model` and `load_model`, FederatedScope trainer uses the `update` function to load the model from FL clients.  - For the train/eval/validate procedures, FederatedScope implements them via calling a general `_run_routine` with different datasets, hooks_set and running mode.   def _run_routine(self, mode, hooks_set, dataset_name=None)     - We decouple the learning process with several fine-grained point-in-time and calling all registered hooks at specific point-in-times as follows      ```python     @lifecycle(LIFECYCLE.ROUTINE)     def _run_routine(self, mode, hooks_set, dataset_name=None):         \"\"\"Run the hooks_set and maintain the mode         Arguments:             mode: running mode of client, chosen from train/val/test         Note:             Considering evaluation could be in ```hooks_set[\"on_epoch_end\"]```, there could be two data loaders in         self.ctx, we must tell the running hooks which data_loader to call and which num_samples to count         \"\"\"         for hook in hooks_set[\"on_fit_start\"]:             hook(self.ctx)          self._run_epoch(hooks_set)          for hook in hooks_set[\"on_fit_end\"]:             hook(self.ctx)          return self.ctx.num_samples      @lifecycle(LIFECYCLE.EPOCH)     def _run_epoch(self, hooks_set):         for epoch_i in range(self.ctx.get(f\"num_{self.ctx.cur_split}_epoch\")):             self.ctx.cur_epoch_i = CtxVar(epoch_i, \"epoch\")              for hook in hooks_set[\"on_epoch_start\"]:                 hook(self.ctx)              self._run_batch(hooks_set)              for hook in hooks_set[\"on_epoch_end\"]:                 hook(self.ctx)      @lifecycle(LIFECYCLE.BATCH)     def _run_batch(self, hooks_set):         for batch_i in range(self.ctx.get(f\"num_{self.ctx.cur_split}_batch\")):             self.ctx.cur_batch_i = CtxVar(batch_i, LIFECYCLE.BATCH)              for hook in hooks_set[\"on_batch_start\"]:                 hook(self.ctx)              for hook in hooks_set[\"on_batch_forward\"]:                 hook(self.ctx)              for hook in hooks_set[\"on_batch_backward\"]:                 hook(self.ctx)              for hook in hooks_set[\"on_batch_end\"]:                 hook(self.ctx)              # Break in the final epoch             if self.ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE] and self.ctx.cur_epoch_i == self.ctx.num_train_epoch - 1:                 if batch_i &gt;= self.ctx.num_train_batch_last_epoch - 1:                     break     ```  ### Hooks    - We implement fruitful default hooks to support various training/evaluation processes, such as [personalized FL behaviors](/docs/pfl/#demonstration), [graph-task related behaviors](/docs/graph/#develop-federated-gnn-algorithms), [privacy-preserving behaviors](/docs/privacy-attacks/#2-usage-of-attack-module).     - Each hook takes the learning `context` as input and performs the learning actions such as         - prepare model and statistics            ```python     def _hook_on_fit_start_init(ctx):         # prepare model and optimizer         ctx.model.to(ctx.device)          if ctx.cur_mode in [MODE.TRAIN, MODE.FINETUNE]:             # Initialize optimizer here to avoid the reuse of optimizers             # across different routines             ctx.optimizer = get_optimizer(ctx.model,                                           **ctx.cfg[ctx.cur_mode].optimizer)             ctx.scheduler = get_scheduler(ctx.optimizer,                                           **ctx.cfg[ctx.cur_mode].scheduler)          # prepare statistics         ctx.loss_batch_total = CtxVar(0., LIFECYCLE.ROUTINE)         ctx.loss_regular_total = CtxVar(0., LIFECYCLE.ROUTINE)         ctx.num_samples = CtxVar(0, LIFECYCLE.ROUTINE)         ctx.ys_true = CtxVar([], LIFECYCLE.ROUTINE)         ctx.ys_prob = CtxVar([], LIFECYCLE.ROUTINE)          ```      - calculate loss in forward stage          ```python     def _hook_on_batch_forward(ctx):         x, label = [_.to(ctx.device) for _ in ctx.data_batch]         pred = ctx.model(x)         if len(label.size()) == 0:             label = label.unsqueeze(0)          ctx.y_true = CtxVar(label, LIFECYCLE.BATCH)         ctx.y_prob = CtxVar(pred, LIFECYCLE.BATCH)         ctx.loss_batch = CtxVar(ctx.criterion(pred, label), LIFECYCLE.BATCH)         ctx.batch_size = CtxVar(len(label), LIFECYCLE.BATCH)     ```           - update model parameters in backward stage          ```python     def _hook_on_batch_backward(ctx):         ctx.optimizer.zero_grad()         ctx.loss_task.backward()         if ctx.grad_clip &gt; 0:             torch.nn.utils.clip_grad_norm_(ctx.model.parameters(),                                            ctx.grad_clip)         ctx.optimizer.step()         if ctx.scheduler is not None:             ctx.scheduler.step()     ```      - To customize more trainer behaviors, users can reset and replace existing hooks, or register new hooks    - Users can freely check the current hook set via print `trainer.hooks_in_train` and `trainer.hooks_in_eval`.    - For delete case, users can either 1) reset all the hooks at a target point-in-time trigger; or 2) a specific hook by passing the target function name `hook_name `  in  train/eval hook set.      ```python      def reset_hook_in_train(self, target_trigger, target_hook_name=None)     def reset_hook_in_eval(self, target_trigger, target_hook_name=None)    - For create case,  we allows registering a new hook at a target point-in-time trigger, and support 1) specifying a specific  positions (i.e., the order a hook called within the trigger set); or 2) inserting before or after a base hook    ```python   def register_hook_in_train(self,                              new_hook,                              trigger,                              insert_pos=None,                              base_hook=None,                              insert_mode=\"before\")           For update case, we provide functions to replace existing hook (by name) with a new_hook (function)       def replace_hook_in_train(self, new_hook, target_trigger, target_hook_name)           Customized Data Preparation     We provide the data pre-processing operations in parse_data function, which parses the dataset and initializes the variables {}_data, {}_loader,  and the counter num_{MODE}_data according to the types of datasets within data as follows.       def parse_data(self, data):         \"\"\"Populate \"{}_data\", \"{}_loader\" and \"num_{}_data\" for different modes          \"\"\"         # TODO: more robust for different data         init_dict = dict()         if isinstance(data, dict):             for mode in [\"train\", \"val\", \"test\"]:                 init_dict[\"{}_data\".format(mode)] = None                 init_dict[\"{}_loader\".format(mode)] = None                 init_dict[\"num_{}_data\".format(mode)] = 0                 if data.get(mode, None) is not None:                     if isinstance(data.get(mode), Dataset):                         init_dict[\"{}_data\".format(mode)] = data.get(mode)                         init_dict[\"num_{}_data\".format(mode)] = len(                             data.get(mode))                     elif isinstance(data.get(mode), DataLoader):                         init_dict[\"{}_loader\".format(mode)] = data.get(mode)                         init_dict[\"num_{}_data\".format(mode)] = len(                             data.get(mode).dataset)                     elif isinstance(data.get(mode), dict):                         init_dict[\"{}_data\".format(mode)] = data.get(mode)                         init_dict[\"num_{}_data\".format(mode)] = len(                             data.get(mode)['y'])                     else:                         raise TypeError(\"Type {} is not supported.\".format(                             type(data.get(mode))))         else:             raise TypeError(\"Type of data should be dict.\")         return init_dict     To support customized dataset, please implement the function parse_data in the new trainer and initialize the following variables.            {train/test/val}_data: the data object,       {train/test/val}_loader: the data loader object,       num_{train/test/val}_data: the number of samples within the dataset.           Trainer Context   Context class is an implementation of messager within the trainer. All variables within it can be called by ctx.{VARIABLE_NAME}.   As stated above, both the training and evaluation processes are consisted of independent hook functions, which only receive an instance of Context as the sole parameter. Therefore, the parameter ctx should     maintain the references of objects (e.g. model, data, optimizer),   provide running parameters (e.g. number of training epochs),   indicate the current operating status (e.g. train/test/validate) and the current selected data split (e.g. the train/test/validate split), and   maintain and manage statistical variables (e.g. loss, output, accuracy).   Maintain the References of Objects  During federated training and evaluation, Context needs to maintain some necessary objects, such as     model: The FL training/evaluation model,   data: The dataset used in FL training/evaluation,   device: The specific device, and   criterion: The specific loss function.   Note the above references of objects are all shared across different routines.   Provide running parameters  Some parameters are calculated within the routine, such as     the number of training/test/validate epochs,   the total number of training/test/valiate batches,   the number of the batches within the last training epoch,   For now, the above running parameters are calculated by the setup_vars function in Context.  def setup_vars(self):     if self.cfg.backend == 'torch':         self.trainable_para_names = get_trainable_para_names(self.model)         self.criterion = get_criterion(self.cfg.criterion.type,                                        self.device)         self.regularizer = get_regularizer(self.cfg.regularizer.type)         self.grad_clip = self.cfg.grad.grad_clip     elif self.cfg.backend == 'tensorflow':         self.trainable_para_names = self.model.trainable_variables()         self.criterion = None         self.regularizer = None         self.optimizer = None         self.grad_clip = None      # Process training data     if self.get('train_data', None) is not None or self.get(             'train_loader', None) is not None:         # Calculate the number of update steps during training given the         # local_update_steps         self.num_train_batch, self.num_train_batch_last_epoch, self.num_train_epoch, self.num_total_train_batch = calculate_batch_epoch_num(             self.cfg.train.local_update_steps,             self.cfg.train.batch_or_epoch, self.num_train_data,             self.cfg.data.batch_size, self.cfg.data.drop_last)      # Process evaluation data     for mode in [\"val\", \"test\"]:         setattr(self, \"num_{}_epoch\".format(mode), 1)         if self.get(\"{}_data\".format(mode)) is not None or self.get(                 \"{}_loader\".format(mode)) is not None:             setattr(                 self, \"num_{}_batch\".format(mode),                 getattr(self, \"num_{}_data\".format(mode)) //                 self.cfg.data.batch_size +                 int(not self.cfg.data.drop_last and bool(                     getattr(self, \"num_{}_data\".format(mode)) %                     self.cfg.data.batch_size)))   Indicate the Current Operating Status and the Selected Dataset  The Context class uses two attributes to indicate the current operating status (cur_mode) and selected dataset (cur_split).   cur_mode  The value of cur_mode is selected among MODE.TRAIN, MODE.FINETUNE, MODE.TEST and MODE.VAL as follows.  You can find the enum class in federatedscope/core/auxiliaries/enums.py.  class MODE:     \"\"\"      Note:         Currently StrEnum cannot be imported with the environment         `sys.version_info &lt; (3, 11)`, so we simply create a MODE class here.     \"\"\"     TRAIN = 'train'     TEST = 'test'     VAL = 'val'     FINETUNE = 'finetune'  At the beginning of one routine, we will check cur_mode to     change the status of the models            execute model.train() if cur_mode equals MODE.TRAIN or MODE.FINETUNE, and       execute model.eval() if cur_mode equals MODE.TEST or MODE.VAL           cur_split  The attribute cur_split indicates which part of dataset that the routine will use, and the printed metrics will be named with a cur_split prefix. In general setting, the dataset is divided into train, test and validate splits.  class MetricCalculator(object):     ...     def eval(self, ctx):         results = {}         y_true, y_pred, y_prob = self._check_and_parse(ctx)         for metric, func in self.eval_metric.items():             results[\"{}_{}\".format(ctx.cur_split,                                    metric)] = func(ctx=ctx,                                                    y_true=y_true,                                                    y_pred=y_pred,                                                    y_prob=y_prob,                                                    metric=metric)   By default, the training routine will execute on the train split, and the evaluation routine will execute on the test and validate splits.  However, you can also specify the split by the argument target_data_split_name.  def train(self, target_data_split_name=\"train\", hooks_set=None):     ...  def evaluate(self, target_data_split_name=\"test\", hooks_set=None):     ...  def finetune(self, target_data_split_name=\"train\", hooks_set=None):     ...   Maintain and Manage Statistical Variables  The statistical variables include average/total training/test loss, number of training/test samples and so on.  Theoretically, the lifecycle of all the statistical variables should be within the routine.  FederatedScope achieves automatic lifecycle management by a wrapper class CtxVar and a decorator @lifecycle.   The class CtxVar takes two arguments, where obj is the value of the statistical variable, and lifecycle is chosen from the enum class federatedscope.core.auxiliaries.enums.LIFECYCLE.  class CtxVar(object):     \"\"\"Basic variable class     Arguments:         lifecycle: specific lifecycle of the attribute     \"\"\"      LIEFTCYCLES = [\"batch\", \"epoch\", \"routine\", None]      def __init__(self, obj, lifecycle=None):         assert lifecycle in CtxVar.LIEFTCYCLES         self.obj = obj         self.lifecycle = lifecycle  Taking the average loss as an example, you can initialize a statistical variable loss_total as follows,  def _hook_on_fit_start(ctx):     ctx.loss_total = CtxVar(0., LIFECYCLE.ROUTINE)  LIFECYCLE.ROUTINE indicates the variable loss_total will be deleted automatically at the end of the routine.   Note     The wrapper class CtxVar is only used to record the lifecycle and won’t influence the usage of the variable.  e.g. in the above example type(ctx.loss_total) still equals float.   While the decorator @lifecycle(lifecycle) decides which variables will be deleted after running the decorated function.  In the following example the variables created with CtxVar(xxx, LIFECYCLE.EPOCH) will be deleted after executing _run_epoch.  @lifecycle(LIFECYCLE.EPOCH) def _run_epoch(self, hooks_set):     for epoch_i in range(self.ctx.get(f\"num_{self.ctx.cur_split}_epoch\")):         self.ctx.cur_epoch_i = CtxVar(epoch_i, \"epoch\")          for hook in hooks_set[\"on_epoch_start\"]:             hook(self.ctx)          self._run_batch(hooks_set)          for hook in hooks_set[\"on_epoch_end\"]:             hook(self.ctx)   NOTE     The users can also manage the variables all by themselves. However, you must check the lifecycle of the record varibales carefully, and release them once they are not used. An unreleased variable may cause memory leakage during federated learning.  Feel free to implement your own algorithm in FederatedScope!   Multi-model Trainer   Several learning methods may leverage multiple models in each client such as clustering based method [1] and multi-task learning based method [2], FederatedScope implements the MultiModelTrainer class to meet this requirement.           We instantiate multiple models, optimizer objects &amp; hook_sets as lists for MultiModelTrainer. Different internal models can have different hook_sets and optimizers to support diverse multi-model based methods       self.init_multiple_models()    # -&gt; self.ctx.models = [...]   # -&gt; self.ctx.optimizers = [...] self.init_multiple_model_hooks()   # -&gt; self.hooks_in_train_multiple_models = [...] # -&gt; self.hooks_in_eval_multiple_models = [...]                To enable easy extension, we support copy initialization from a single-model trainer.       # By default, the internal models &amp; optimizers are the same type additional_models = [     copy.deepcopy(self.ctx.model) for _ in range(self.model_nums - 1) ] self.ctx.models = [self.ctx.model] + additional_models                We can customized hooks and optimizers for multi-model interaction. Specifically,  two types of internal model interaction mode are built in MultiModelTrainer .                       # assert models_interact_mode in [\"sequential\", \"parallel\"] self.models_interact_mode = models_interact_mode                                The sequential interaction mode indicates the interaction are conducted at run_routine level           [one model runs its whole routine, then do sth. for interaction, then next model runs its whole routine] ... -&gt; run_routine_model_i \t\t-&gt; _switch_model_ctx     -&gt; (on_fit_end, _interact_to_other_models)        -&gt; run_routine_model_i+1     -&gt; ...                                The parallel  interaction mode indicates the interaction are conducted at point-in-time level           [At a specific point-in-time, one model call hooks (including interaction), then next model call hooks] ... -&gt;  (on_xxx_point, hook_xxx_model_i)     -&gt;  (on_xxx_point, _interact_to_other_models)     -&gt;  (on_xxx_point, _switch_model_ctx)     -&gt;  (on_xxx_point, hook_xxx_model_i+1)     -&gt; ...                                Note that these two modes call _switch_model_ctx at different positions. By default, we will switch cur_model, and optimizer, and users can override this function to support customized switch logic           def _switch_model_ctx(self, next_model_idx=None):     if next_model_idx is None:         next_model_idx = (self.ctx.cur_model_idx + 1) % len(             self.ctx.models)     self.ctx.cur_model_idx = next_model_idx     self.ctx.model = self.ctx.models[next_model_idx]     self.ctx.optimizer = self.ctx.optimizers[next_model_idx]                           Reference  [1] Felix Sattler, Klaus-Robert Müller, and Wojciech Samek. “Clustered Federated Learning: Model-Agnostic Distributed Multitask Optimization Under Privacy Constraints”. In: IEEE Transactions on Neural Networks and Learning Systems (2020).   [2] Marfoq, Othmane, et al. “Federated multi-task learning under a mixture of distributions.” Advances in Neural Information Processing Systems 34 (2021).  ","categories": [],
        "tags": [],
        "url": "/docs/trainer/",
        "teaser": null
      },{
        "title": "Personalized FL",
        "excerpt":"FederatedScope  is a flexible FL framework, which enables users to implement complex FL algorithms simply and intuitively. In this tutorial, we will show how to implement diverse personalized FL algorithms.   Background   In an FL course, multiple clients aim to cooperatively learn models without directly sharing their private data. As a result, these clients can be arbitrarily different in terms of their underlying data distribution and system resources such as computational power and communication width.      On one hand, the data quantity skew, feature distribution skew,  label distribution skew, and temporal skew are pervasive in real-world applications as different users generate the data with different usage manners. Simply applying the shared global model for all participants might lead to sub-optimal performance.   On the other hand, the participation degrees of different FL participants can be diverse due to their different hardware capabilities and network conditions.   It is challenging to make full use of local data considering such systematical heterogeneity. As a natural and effective approach to address these challenges, personalization gains increasing attention in recent years. Personalized FL (pFL) raises strong demand for various customized FL implementation, e.g., the personalization may exist in      Model objects, optimizers and hyper-parameters   Model sub-modules   Client-end behaviors such as regularization and multi-model interaction   Server-end behaviors such as model interpolation   We will demonstrate several implementations for state-of-the-art (SOTA) pFL methods  to meet the above requirements and show how  powerful and flexible the FederatedScope framework to implement pFL extensions.   Demonstration   Personalized model sub-modules - FedBN   FedBN [1] is a simple yet effective approach to address feature shift non-iid, in which the client BN parameters are trained locally, without communication and aggregation via server. FederatedScope provides simple configuration to implement FedBN and other variants that need to keep parameters of some model sub-modules local.      By specifying the local parameter names as follows, the clients and server will filter out the sub-modules contains the given names in the model parameter update function.     cfg.personalization.local_param = []  # e.g., ['pre', 'post', 'bn']           We provide auxiliary logging function print_trainer_meta_info() to show the model type, local and filtered model parameter names in trainer instantiation   # trainer.print_trainer_meta_info()  Model meta-info: &lt;class 'federatedscope.cv.model.cnn.ConvNet2'&gt;. Num of original para names: 18. Num of original trainable para names: 12. Num of preserved para names in local update: 8. Preserved para names in local update: {'fc2.bias', 'conv1.weight', 'conv2.weight', 'fc1.weight', 'fc2.weight', 'conv1.bias', 'fc1.bias', 'conv2.bias'}. Num of filtered para names in local update: 10. Filtered para names in local update: {'bn2.weight', 'bn2.num_batches_tracked', 'bn1.num_batches_tracked', 'bn1.running_var', 'bn2.running_mean', 'bn1.weight', 'bn2.running_var', 'bn1.running_mean', 'bn1.bias', 'bn2.bias'}.   Personalized regularization - Ditto   Ditto [2] is a SOTA pFL approach that improves fairness and robustness of FL via training local personalized model and global model simultaneously, in which the local model update is based on regularization to global model parameters. FederatedScope provides built-in Ditto implementation and users can easily extends to other pFL methods by re-using the model-para regularization. More details can be found in federatedscope/core/trainers/trainer_Ditto.py.           To preserve distinct local models in trainer, we can simply use another model object in trainer’s context       ctx.local_model = copy.deepcopy(ctx.model)  # the personalized model ctx.global_model = ctx.model                To train local models with global-model regularization, we implement a new hook on  run_routine fit start and register the global model parameters into the new optimizer.       def hook_on_fit_start_set_regularized_para(ctx):     # set the compared model data for local personalized model     ctx.global_model.to(ctx.device)     ctx.local_model.to(ctx.device)     ctx.global_model.train()     ctx.local_model.train()     compared_global_model_para = [{         \"params\": list(ctx.global_model.parameters())     }]     ctx.optimizer_for_local_model.set_compared_para_group(         compared_global_model_para)    def regularize_by_para_diff(self):     \"\"\"        before optim.step(), regularize the gradients based on para diff     \"\"\"     for group, compared_group in zip(self.param_groups, self.compared_para_groups):         for p, compared_weight in zip(group['params'], compared_group['params']):             if p.grad is not None:                if compared_weight.device != p.device:                     compared_weight = compared_weight.to(p.device)                     p.grad.data = p.grad.data + self.regular_weight * (p.data - compared_weight.data)                We implement Ditto with a pluggable manner, some Ditto specific attributes (contexts) and behaviors (hooks) can be added into an existing base_trainer as follows.       def wrap_DittoTrainer(         base_trainer: Type[GeneralTrainer]) -&gt; Type[GeneralTrainer]):          # ---------------- attribute-level plug-in -----------------------     init_Ditto_ctx(base_trainer)        # ---------------- action-level plug-in -----------------------     base_trainer.register_hook_in_train(         new_hook=hook_on_fit_start_set_regularized_para,         trigger=\"on_fit_start\",         insert_pos=0)           Personalized multi-model interaction - FedEM   FedEM [3] is a SOTA pFL approach that assumes local data distribution is a mixture of unknown underlying distributions, and correspondingly learn a mixture of multiple internal models with Expectation-Maximization learning. FederatedScope provides built-in FedEM implementation and users can easily extends to other multi-model pFL methods based on this example. More details can be found in federatedscope/core/trainers/trainer_FedEM.py.           The FedEMTrainer is derived from GeneralMultiModelTrainer. We can easily add FedEM-specific attributes and behaviors via context and hooks register functions       # ---------------- attribute-level modifications ----------------------- # used to mixture the internal models self.weights_internal_models = (torch.ones(self.model_nums) /                                 self.model_nums).to(device) self.weights_data_sample = (     torch.ones(self.model_nums, self.ctx.num_train_batch) /     self.model_nums).to(device)    self.ctx.all_losses_model_batch = torch.zeros(     self.model_nums, self.ctx.num_train_batch).to(device) self.ctx.cur_batch_idx = -1    # ---------------- action-level modifications ----------------------- # see customized register_multiple_model_hooks(), which is called in the __init__ of `GeneralMultiModelTrainer`                We can simply extend  GeneralMultiModelTrainer with the default sequential interaction mode, and add some training behaviors such as mixture_weights_update, weighted_loss_adjustment and track_batch_idx       # hooks example, for only train def hook_on_batch_forward_weighted_loss(self, ctx):     ctx.loss_batch *= self.weights_internal_models[ctx.cur_model_idx]    def register_multiple_model_hooks(self):     # First register hooks for model 0     # ---------------- train hooks -----------------------     self.register_hook_in_train(         new_hook=self.hook_on_fit_start_mixture_weights_update,         trigger=\"on_fit_start\",         insert_pos=0)  # insert at the front     self.register_hook_in_train(         new_hook=self.hook_on_batch_forward_weighted_loss,         trigger=\"on_batch_forward\",         insert_pos=-1)     self.register_hook_in_train(         new_hook=self.hook_on_batch_start_track_batch_idx,         trigger=\"on_batch_start\",         insert_pos=0)  # insert at the front                We also need to add some evaluation behavior modifications such as model_ensemble and loss_gather           # ---------------- eval hooks -----------------------     self.register_hook_in_eval(         new_hook=self.hook_on_batch_end_gather_loss,         trigger=\"on_batch_end\",         insert_pos=0     )  # insert at the front, (we need gather the loss before clean it)     self.register_hook_in_eval(         new_hook=self.hook_on_batch_start_track_batch_idx,         trigger=\"on_batch_start\",         insert_pos=0)  # insert at the front     # replace the original evaluation into the ensemble one     self.replace_hook_in_eval(         new_hook=self._hook_on_fit_end_ensemble_eval,         target_trigger=\"on_fit_end\",         target_hook_name=\"_hook_on_fit_end\")        # hooks example, for only eval def hook_on_batch_end_gather_loss(self, ctx):     # before clean the loss_batch; we record it for further weights_data_sample update     ctx.all_losses_model_batch[ctx.cur_model_idx][             ctx.cur_batch_idx] = ctx.loss_batch.item()                Note that the GeneralMultiModelTrainer will switch the model states automatically, we can differentiate different internal models in the new hooks with ctx.cur_model_idx and ` self.model_nums` attributes.       FedEM can be generalized to many clustering ** based methods &amp;  **multi-task modeling based methods (see details inSection 2.3 in [3]) and we can extend FedEMTrainer to more multi-model based pFL methods.   Evaluation Results  To facilitate rapid and reproducible pFL research, we provide the experimental results and corresponding scripts to benchmark  pFL performance for several SOTA pFL methods via FederatedScope.  We will continue to add more algorithm implementations and experimental results in different scenarios.   FedBN   We provide some evaluation results for FedBN on different tasks as follows, in which the models contain batch normalization. Complete results, config files and running scripts can be found in scripts/personalization_exp_scripts/fedbn.                  Task       Data       Accuracy (%)                       Image classification       FEMNIST       85.48                 Graph classification       multi-task-molecule       72.90           pFedMe   pFedMe [4] is an effective pFL approach to address data heterogeneity, in which the personalized model and global model are decoupled with Moreau envelops. FederatedScope implements pFedMe in federatedscope/core/trainers/trainer_pFedMe.py and ServerClientsInterpolateAggregator in federatedscope/core/aggregator.py.   We provide some evaluation results for pFedMe on different tasks as follows. Complete results, config files and running scripts can be found in scripts/personalization_exp_scripts/pfedme.                  Task       Data       Accuracy (%)                       Logistic regression       Synthetic       68.73                 Image classification       FEMNIST       87.65                 Next-character Prediction       Shakespeare       37.40           Ditto   We provide some evaluation results for Ditto on different tasks as follows. Complete results, config files and running scripts can be found in scripts/personalization_exp_scripts/ditto.   | Task                      | Data        | Accuracy (%) | | ————————- | ———– | ———— | | Logistic regression       | Synthetic   | 69.67        | | Image classification      | FEMNIST     | 86.61        | | Next-character Prediction | Shakespeare |   45.14   |  FedEM   FedEM is a SOTA pFL approach that assumes local data distribution is a mixture of unknown underlying distributions, and correspondingly learn a mixture of multiple internal models with Expectation-Maximization learning. FederatedScope provides built-in FedEM implementation and users can easily extends to other multi-model pFL methods based on this example. More details can be found in federatedscope/core/trainers/trainer_FedEM.py.   We provide some evaluation results for FedBN on different tasks as follows. Complete results, config files and running scripts can be found in scripts/personalization_exp_scripts/fedem.                  Task       Data       Accuracy (%)                       Logistic regression       Synthetic       68.80                 Image classification       FEMNIST       84.79                 Next-character Prediction       Shakespeare       48.06           Reference  [1] Li, Xiaoxiao, et al. “Fedbn: Federated learning on non-iid features via local batch normalization.” arXiv preprint arXiv:2102.07623 (2021).   [2] Li, Tian, et al. “Ditto: Fair and robust federated learning through personalization.” International Conference on Machine Learning. PMLR, 2021.   [3] Marfoq, Othmane, et al. “Federated multi-task learning under a mixture of distributions.” Advances in Neural Information Processing Systems 34 (2021).   [4] T Dinh, Canh, Nguyen Tran, and Josh Nguyen. “Personalized federated learning with moreau envelopes.” Advances in Neural Information Processing Systems 33 (2020): 21394-21405.  ","categories": [],
        "tags": [],
        "url": "/docs/pfl/",
        "teaser": null
      },{
        "title": "Cross-Device FL",
        "excerpt":"  Background   A typical FL scenario is cross-device FL, which is pioneered by Google [1], and several efforts have been devoted to consumer applications such as mobile keyboard assistants [1,2,3,4] and audio keyword spotting [5,6,7].   The cross-device FL setting usually contains the following charateristics:      massively parallel processing:  there can be 10^4  ~ 10^10 mobile or IoT devices in an FL course;   un-balanced and diverse local data:  clients (devices) usually have heterogeneous data in terms of data quantity and data distributions;   limited client-end resources: the client devices usually have limited hardware and communication resources that are much weaker than cloud servers. Low latency and low costs in storage, computation, and communication are much-needed in cross-device FL applications.   As the following figure shows, a typical cross-device FL process adopts a centralized network topology and involves the following repeated steps:       Server broadcasts the intermediate exchange information (usually the global model weights) and (optional) the clients-end FL program to selected clients.   The selected clients download the information from the server, and execute local learning based on the private local data, the FL programs, and the messages from the server.   The selected clients upload the local update information such as model gradients to the server.   The server collects and aggregates the update information from clients, and applies the updates into the intermediate exchange information (the shared global) for next-round federation.   Next we show how to run a cross-device FL simulation for next-character/word prediction task with our framework.     Example: LSTM on Shakespeare dataset   Next-character/word prediction is a classic NLP task as it can be applied in many consumer applications and appropriately be modeled by statistical language models, we show how to achieve the cross-device FL simulation for this task.      Here we implement a simple LSTM model for next-character prediction: taking an English-character sequence as input, the model learns to predict the next possible character. After registering the modela we can use it by specifying cfg.model.type=lstm and  hyper-parameters such as  cfg.model.in_channels=80, cfg.model.out_channels=80, cfg.model.emd_size=8.  Complete codes are in federatedscope/nlp/model/rnn.py and federatedscope/nlp/model/model_builder.py.   class LSTM(nn.Module):     def __init__(self,                  in_channels,                  hidden,                  out_channels,                  n_layers=2,                  embed_size=8):         super(LSTM, self).__init__()         self.in_channels = in_channels         self.hidden = hidden         self.embed_size = embed_size         self.out_channels = out_channels         self.n_layers = n_layers          self.encoder = nn.Embedding(in_channels, embed_size)          self.rnn =\\             nn.LSTM(                 input_size=embed_size,                 hidden_size=hidden,                 num_layers=n_layers,                 batch_first=True             )          self.decoder = nn.Linear(hidden, out_channels)      def forward(self, input_):         encoded = self.encoder(input_)         output, _ = self.rnn(encoded)         output = self.decoder(output)         output = output.permute(0, 2, 1)  # change dimension to (B, C, T)         final_word = output[:, :, -1]         return final_word      For the dataset, we use the Shakespeare dataset from LEAF, which is built from The Complete Works of William Shakespeare,  and partitioned to ~1100 clients (speaking roles) from 422615.  We can specify the cfg.dataset.type=shakespeare and adjust the fraction of data subsample (cfg.data.sub_sample=0.2), and train/val/test ratio (cfg.data.splits=[0.6,0.2,0.2). Complete NLP data codes are in federatedscope/nlp/dataset.   class LEAF_NLP(LEAF):     \"\"\"     LEAF NLP dataset from          leaf.cmu.edu          self:         root (str): root path.         name (str): name of dataset, ‘shakespeare’ or ‘xxx’.         s_frac (float): fraction of the dataset to be used; default=0.3.         tr_frac (float): train set proportion for each task; default=0.8.         val_frac (float): valid set proportion for each task; default=0.0.     \"\"\"     def __init__(             self,             root,             name,             s_frac=0.3,              tr_frac=0.8,             val_frac=0.0,             seed=123,             transform=None,             target_transform=None):      To enable large-scale clients simulation, we provide online aggregator in standalone mode to save the memory, which  maintains only three model objects for the FL server aggregation. We can use this feature by specifying cfg.federate.online_aggr = True and federate.share_local_model=True , more details about this feature can be found in the post “Simulation and Deployment”.   To handle the non-i.i.d. challenge, FederatedScope supports several SOTA personalization algorithms and easy extension.   To enable partial clients participation in each FL round, we provide clients sampling feature with various configuration manners: 1) cfg.federate.sample_client_rate, which is in the range (0, 1] and indicates selecting partial clients using random sampling with replacement; 2) cfg.federate.sample_client_num , which is an integer to indicate sample client number at each round.   With these specification, we can run the experiment with python main.py --cfg federatedscope/nlp/baseline/fedavg_lstm_on_shakespeare.yaml . Other NLP related scripts to run the next-character prediction experiments can be found in federatedscope/nlp/baseline.     References   [1] McMahan, Brendan, et al. “Communication-efficient learning of deep networks from decentralized data.” AISTATS 2017.   [2] McMahan, H. Brendan, et al. “Learning differentially private recurrent language models.” ICLR 2018.   [3] Hard, Andrew, et al. “Federated learning for mobile keyboard prediction.” arXiv 2018.   [4] Chen, Mingqing, et al. “Federated learning of n-gram language models.” ACL 2019.   [5] Dimitriadis, Dimitrios, et al. “A federated approach in training acoustic models.” INTERSPEECH 2020.   [6] Cui, Xiaodong, Songtao Lu, and Brian Kingsbury. “Federated Acoustic Modeling for Automatic Speech Recognition.” ICASSP 2021.   [7] Apple. Designing for privacy (video and slide deck). Apple WWDC, https://developer.apple.com/ videos/play/wwdc2019/708, 2019.  ","categories": [],
        "tags": [],
        "url": "/docs/cross-device/",
        "teaser": null
      },{
        "title": "Cross-Silo FL",
        "excerpt":"In the cross-silo scenario where several departments or companies that own a large amount of data and computation resources want to jointly train a global model, vertical federated learning is a widespread learning paradigm. Vertical federated learning refers to the scenario where participants share the same sample ID scape but different feature spaces. For example, several companies want to federal learn global user profiles with their app data, which have a large amount of overlapped users but different user behaviors.   Settings    A typical process of Vertical FL is illustrated in the figure. Clients own different features within $(x1,x2,x3,x4,x5)$ and one of them owns the labels$y$. After aligning the user ID space via a private set intersection, participants aim to train a model based on the isolated data without directly sharing the features. The general process includes:      A server (also called a coordinator) hands out public keys (for encrypting the results) and computation tasks to clients;   Each client computes a part of the results based on the data, encrypts them, and exchanges the encrypted results with each other;   Then each client finishes the computation task and returns the final results to the server;   The server aggregates the results and updates the model.   Note that in the vertical FL, homomorphic encryption techniques might be used to make sure the correctness of the computation while protecting privacy.   Next we will introduce an example and show how to implement it with FederatedScope.   Example and implementation   We task the study from [1] as an example. To be simplified, we focus on the secure logistic regression (Algorithm 2 in the paper) while ignoring the entity matching process. Before introducing the algorithm, we need to present the additively homomorphic encryption and Taylor approximation for the objective function first.   Additively homomorphic encryption: Paillier   Paillier [2] is used to make sure the correctness of the computation while protecting privacy. We use the following notations for Paillier:      $[\\cdot]$ denotes the encryption   Due to the characteristics of additively homomorphic encryption, we have$[u] + [v] = [u+v]$and $v[u] =[vu]$   Note that Paillier can be extended to the inner product or element-wise product.   Taylor approximation for the objective function of LR   The objective function of logistic regression is not suitable to be calculated via additively homomorphic encryption, thus we need to perform the Taylor approximation according to [1]. Specifically,      Given the training set $S = {(x_i,y_i)_{i=1}^n}$, with logistic regression, we learn a linear model $\\theta\\in \\mathbb{R}^d$ that maps $x\\in \\mathbb{R}^d$ to binary labels $y\\in{-1,1}$.   The loss can be given as  $L_S(\\theta) = \\frac{1}{n}\\sum_{(x_i,y_i)\\in S} \\log(1+e^{-y_i\\theta^{\\top}x_i})$   With the Taylor series expansion of $\\log(1+e^{-z})$ around $z=0$,  the second-order approximation of the loss is$L_S(\\theta) \\approx \\frac{1}{n}\\sum_{(x_i,y_i)\\in S}\\left( \\log 2 - \\frac{1}{2}y_i {\\theta^{\\top}x_i}+\\frac{1}{8}(\\theta^\\top x_i)^2 \\right)$   The gradient can be given as $\\nabla L_{S}(\\theta) \\approx \\frac{1}{n}\\sum_{(x_i,y_i)\\in S}\\left( \\frac{1}{4}\\theta^\\top x_i - \\frac{1}{2}y_i \\right)x_i$   Algorithm                  Then we can describe the algorithm in [1] via message passing. Suppose that there exist two participants, A and B, which own the same number of instances$n$. And only A has the labels$Y$. We denote the features as$X = [X_A       X_B]$, $X\\in \\mathbb{R}^{n\\times d}$. There also exists a Coordinator C (i.e., the server).           We can decompose $\\theta^\\top x =\\theta_A^\\top x_A+\\theta_B^\\top x_B$. The algorithm via message passing can be summarized as:      Coordinator C:            Send$\\theta$to A, B;       Send public keys to A, B;           participant A:            Sample a batch of data with indexes$S’$ ;       Compute $u_A = \\frac{1}{4}X_{A,S’}\\theta_A - \\frac{1}{2}Y_{S’}$ and encrypt (via Paillier): $[u_A]$;       Send$S’$ and $[u_A]$ to B;           participant B:            Compute $u_B = \\frac{1}{4} X_{B,S’}\\theta_B$  and encrypt  (via Paillier): $[u_B]$ ;       Compute $[u] = [u_A] + [u_B]$ ;       Compute $[v_B]=X_{B,S’}[u]$ ;       Send $[u]$ and $[v_B]$ to A ;           participant A:            Compute $[v_A]=X_{A,S’}[u]$ ;       Send $[v_A]$ and $[v_B]$ to C ;           Coordinator C:            Obtain $[v]$ by concatenating $[v_A]$ and $[v_B]$ ;       Decrypt to obtain the gradients $v$ ;           To implement the algorithm in FederatedScope, developers need to abstract the types of exchanged messages and transform the behaviors into handling functions. For example:   # For Coordinator C Received messages:    [v_a] &amp; [v_b] Handling functions:   when receiving [v_a] &amp; [v_b] -&gt; concatenates [v_a] &amp; [v_b], and decrypt;   # For Participant A Received messages:    theta &amp; public_key, [u] &amp; [v_b] Handling functions:   when receiving theta &amp; public_key -&gt; sample a batch of data, compute [u_a], send sampled indexes and results to participant B;   when receiving [u] &amp; [v_b] -&gt; compute [v_a], send [v_a] &amp; [v_b] to Coordinator C;  # For Paritcipant B Received messages:   theta &amp; public_key, S' &amp; [u_a] Handling functions:   when receiving theta &amp; public_key -&gt; wait for intermediate results from participant A;   when receiving S' &amp; [u_a] -&gt; compute [u] and [v_b], send [u] &amp; [v_b] to participant A;   Users can refer to New types of messages and handlers for more details about how to add new types of messages and handlers to customize an FL task.   References   [1] Hardy S, Henecka W, Ivey-Law H, et al. “Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption”. arXiv preprint, 2017.  [2] Paillier P. “Public-key cryptosystems based on composite degree residuosity classes.” International conference on the theory and applications of cryptographic techniques. 1999: 223-238.  ","categories": [],
        "tags": [],
        "url": "/docs/cross-silo/",
        "teaser": null
      },{
        "title": "Accelerating Federated HPO",
        "excerpt":"In essence, hyperparameter optimization (HPO) is a trial-and-error procedure, where each trial often means an entire training course and then evaluation. Under the federated learning (FL) setting, each FL training course consists of hundereds of communication rounds, which makes it unaffordable to conduct such trials again and again.   Hence, FederatedScope has provided functionalities to conduct low-fidelity HPO, which allows users to trade evaluation precision for efficiency.   Achieving low-fidelity HPO under the federated learning setting   We encourage users to try low-fidelity HPO with our provided toy example:   python federatedscope/hpo.py --cfg federatedscope/example_configs/toy_hpo.yaml hpo.scheduler sha hpo.sha.budgets [1,3,9]   where Successive Halving Algorithm (SHA) [2] is employed as our HPO scheduler. At first, the number of initial candidates is determined by:      #initial candidates = elimination rate ** #elimination rounds,    where, in this example, hpo.sha.elim_round_num=3 and hpo.sha.elim_rate=3. Thus, the scheduler begins with 3**3=27 initial candidate configurations randomly sampled from the declared search space. As we have introduced in the primary HPO tutorial, SHA iteratively filters the maintained candidates round-by-round untill only one configuration remaining. According to the settings for elimination rate, the number of elimination rounds, and the budgets (i.e., hpo.sha.budgets), the scheduler proceeds as follow:           1st iteration: Each of the 27 candidates will be trained for 1 round.            2nd iteration: Each of the 9 candidates outstanding from last iteration will be trained for 3 rounds            3rd iteration: Each of the 3 candidates outstanding from last iteration will be trained for 9 rounds       As your can see, by controlling the resource allocation (here the assigned number of training rounds), the winning configuratioin enjoys the most resource—$1+3+9=13$ training rounds, while we didn’t waste much resource on those poor configurations. The total resource consumed is $27 \\times 1 + 9 \\times 3 + 3 \\times 9 = 81$ training rounds. In contrast, the SHA example presented in our primary HPO tutorial has not specified the budget and consumes $(27 + 9 + 3) \\times 20 = 780$ training rounds. Although insufficient training rounds may lead to configuration rankings that are less correlated with the ground-truth rankings, training round provides us an aspect to control the fidelity to trade-off between accuracy and efficiency.   The eventual results of the above example are as follow:      where the test loss cannot be minimized as small as that of the SHA example presented in our primary HPO tutorial. However, the goal of HPO is to determine the hyperparameters. In this sense, this SHA example with low-fidelity has attained the same decisions as that example, while consuming much less resource.   Another aspect that enables to control the fidelity is the ratio of clients sampled in each training round. FederatedScope allows users to specify either the sample rate (via federate.sample_client_rate) or the number of sampled client (via federate.sample_client_num), with the former prioritized. If none of them has been set, all clients would be involved in each training round.   Empirical study   We evaluate the effectiveness of SHA with low-fidelity on a case where graph convolutional network (GCN) is to be trained on the citation network Cora. In this experiment, we use the same setting as the above example, and thus there are 81 training rounds in total can be consumed. For the purpose of comparison, we adopt random search (RS) algorithm [4] as our baselines, where sample size of 81, 27, and 9 are considered, with training rounds per trial 1, 3, and 9, respectively.   We show the corresponding performances of their searched hyperparameters in the following table:                  Scheduler       Test accuracy (%)                       SHA       88.83                 RS (81-1)       88.51                 RS (27-3)       88.67                 RS (9-9)       88.67           where SHA with the given allocation outperforms all RS settings. As we sequentially simulate each trial, the HPO procedure can be visualized by plotting the best test accuracy achieved up to the latest trial:      Users can easily reproduce this HPO experiment by executing:   python federatedscope/hpo.py --cfg federatedscope/example_configs/hpo_for_gnn.yaml   Furthermore, we present an empirical comparison for different fidelities considered in optimizing the hyperparameters of GCN and GPR-GNN, respectively. Again, we employ SHA as our scheduler and controll the fidelity by considering:      Training rounds for each trial in {1, 2, 4, 8};   Client sampling rate in {20%, 40%, 80%, 100%}.   With different combinations of the them, we conduct HPO with different fideilities, which might result in different optimal hyperparameters. As we have construct the ground-truth rankings, the accuracy of each combination can be measured by the rank of its resulting optimal hyperparameters. We illustrate the results in the following two figures:                  GCN       GPR-GNN                                         where, as expected, higher fidelity leads to better configuration for both kinds of graph neural network models. At first, we want to remind our readers that the left-upper region in each grid table corresponds to extremely low-fidelity HPO. Although their performances cannot be comparable to those in the other regions, they have successfully eliminated a considerable fraction of poor configurations. Meanwhile, increasing fidelity through the two aspects, i.e., client sampling rate and the number of training rounds, reveal comparable efficiency in improving the quality of searched configurations. This property provides valuable flexibility for practitioners to keep a fixed fidelity while trade-off between these two aspects according to their system status (e.g., network latency and how the dragger behaves) [3].   Weight-sharing and personalized HPO   In general, the hyperparameters of an FL algorithm can be classified into two categories:      Server-side: The hyperparameters impact the aggregation, e.g., the learning rate of server’s optimizer in FedOPT [5].   Client-side: The hyperparameters impact the local updates, e.g., the local update steps, the batch size, the learning rate, etc.   In traditional standalone machine learning, only one specific configuration can be evaluated in each trial. When we consider an FL training course, since there are often more than one clients sampled in each round, it is possible to let different sampled clients explore different client-side configurations. From the perspective of multi-arm bandit, the agent (i.e., the HPO scheduler) can interact with many bandits under the FL setting, as the following figure shows.      To utilize this idea, FexEx [1] makes an analogy to the weight-sharing trick widely adopted in one-shot neural architecture search (NAS). Roughly speaking, one-shot NAS regards all candidate operators as an super-graph, evaluates a sampled subgraph at each step, and updates the controller (i.e., sampler) according to the feedback. In analogy, we could design a controller to sample configuration and, at each round, independently sample the client-side configuration for each client.   FederatedScope has provided flexible interfaces to instantiate such an idea into Federated HPO algorithm (more details can be found at this post). For instance, implementing FedEx can be sketched as the following steps:      To inherit the base server class and integrate your implementation of the controller into the server;   To augment the parameter broadcast method, including the sampled client-side configuration in the message;   To inherit the base client class and extend the handler—initializing the local model with received parameters and reset hyperparameters by the received choices;   To extend the handler of server, that is, updating the controller w.r.t. received performances.   We will provide an implementation of FedEx later, and we encourage users to contribute more latest federated HPO algorithms to FederatedScope.   It is worth noting that the bandits sampled in each round are different due to the non-i.i.d.ness of client-wise data distributions. Thus, a promising future direction is to explore the personalization functionalities to achieve personalized HPO.   References   [1] Khodak, Mikhail, et al. “Federated hyperparameter tuning: Challenges, baselines, and connections to weight-sharing.” Advances in Neural Information Processing Systems 34 (2021).   [2] Li, Lisha, et al. “Hyperband: A novel bandit-based approach to hyperparameter optimization.” The Journal of Machine Learning Research 18.1 (2017): 6765-6816.   [3] Zhang, Huanle, et al. “Automatic Tuning of Federated Learning Hyper-Parameters from System Perspective.” arXiv preprint arXiv:2110.03061 (2021).   [4] Bergstra, James, and Yoshua Bengio. “Random search for hyper-parameter optimization.” Journal of machine learning research 13.2 (2012).   [5] Asad, Muhammad, Ahmed Moustafa, and Takayuki Ito. “FedOpt: Towards communication efficiency and privacy preservation in federated learning.” Applied Sciences 10.8 (2020): 2864.  ","categories": [],
        "tags": [],
        "url": "/docs/improve-hpo/",
        "teaser": null
      },{
        "title": "Simulation and Deployment",
        "excerpt":"Efficient and memory saving simulation   As we have discussed in the post “Cross-device”, there are usually tremendous clients, ranging from hundreds to tens of thousands. In simulating such a scenario, there are mainly two challenges:      How to handle such a large number of client-wise models?   How to cache such a large number of aggregated updates?   Taking the famous benchmark LEAF [1] as an example, and suppose we apply a two-layer convolutional neural network to the FEMNIST dataset, although this model only occupies ~200MB, there are around 300 clients, and thus trivially maintaining 300 such models would consume more than 50GB.   In general, the neural architecture is consistent among these clients (exceptions mainly result from personalization). Meanwhile, the local updates happened at each client is often simulated one-by-one. Hence, it is feasible to maintain only one model for all the clients, which reduces the space complexity by the order of number of clients. FederatedScope has provided this memory saving mode, and users can enjoy it by setting:   federate.share_local_model=True   Even though we only keep one model in GPU for simulating the local updates happened at each client, each client will produce its updated model, where caching all these models in GPU is impossible due to the very limited video memory. As the following figure shows, one compromise is to cache these updated models in the main RAM, which is often much larger than the video memory.      However, as a well-known fact, the swap in-and-out between video memory and the main RAM is costly, leading to inefficient simulation.   Therefore, FederatedScope has provided an online counterpart for the vanilla FedAvg [2] aggregator, which makes it unnecessary to cache all these updated models. The rationale behind this online couterpart is straightforward. First, according to FedAvg, the aggregation operation is defined as follow:   \\[x = w_1 \\times x_1 + w_2 \\times x_2 + \\ldots + w_n \\times x_n,\\]  where $x_i$ denotes the $i$-th client’s updated model, $w_i$ denotes the weight of $i$-th client, and $x$ denotes the aggregation. When the simulation is conducted one-by-one, the server sequentially receives $(w_1, x_1), (w_2, x_2), \\ldots, (w_n, x_n)$. Then we maintain the result update now as follow:   \\[m_0 = 0, c_0 = 0\\\\  m_i = \\frac{c_{i-1} \\times m_{i-1} + w_i \\times x_i}{ c_{i-1} + w_i }, c_i = c_{i-1} + w_i,\\]  where $m_n$ will be equal to $x$. Users can utilize this online aggregator by setting:   federate.online_aggr=True   With both shared local model and this online aggregator, the simulation can be entirely conducted on GPU, if three times of the model size have not exceeded the video memory size. The procedure can be described as follow:      Empirical study   We use FederatedScope to conduct simulation for federally training a two-layer convolutional neural network on FEMNIST. We compare FederatedScope (with the efficient and memory saving mode) to FedML [3], regarding both the memory and time consumption. The results are as follow:                          Memory consumption (GB)       Time per round (s)                       FedML       7.88       4.4                 FederatedScope       2.76       1.4           As you can see from this table, FederatedScope enables users to conduct more efficient and memory saving simulation. We encourage users to reproduce this experiment by:   python federatedscaope/main.py --cfg federatedscope/cv/baseline/fedavg_convnet2_on_femnist.yaml federate.share_local_model True federate.online_aggr True   We also encourage contributions about accelerating FL simulation, which must be very helpful for both research and application.   From simulation to deployment   FederatedScope provides a unified interface for both standalone simulation and distributed deployment. Therefore, to transfer from simulation to deployment, users only need to:      Modify the configurations, add data_path, role, and communication address;   Run multiple FL procedures, each denotes a participant, to build an FL course.   An example can be found in Distributed Mode.   We aim to provide more support for different data storage,  various software/hardware environments,  distributed systems scheduling in the future.   Cross ML backends   One of the biggest challenges when applying federated learning in practice is to be compatible with different ML backends. Considering the situation that some participants perform local training based on Tensorflow while others use Pytorch, and these participants want to build up an FL course.    The most straightforward solution is to force all the participants to use the same ML backend. It can be feasible for some cross-device scenarios such as Gboard [4], where exists a powerful manager to unify the types of software and hardware environments. However, for cross-silo scenarios where each participant (usually a department or company) has already built up a large and complete system for local training, it is not practical and economical to unify the backends.   Another solution is getting help from intermedia representation, such as ONNX [5] and TensorRT [6], which transfers the original program into a defined intermedia representation, and further interprets into the target language during the runtime.   In order to be compatible with different ML backends, FederatedScope decouples the federal behaviors and training behaviors. The participants hold a trainer/aggregator object to encapsulate their training behaviors that might be related to their ML backends. Thus they can only care about high-level behaviors such as train or eval. For example, a client uses trainer.train(model, data, configuration)to perform local training but ignores what is the backend used behind.   In summary, if developers want to build up an FL course that involves participants with different ML backends, developers might need to:      Customize backend-specific Trainers accordingly;   A transformation to match the computation graph described in different backends.   We aim to provide more Trainers to support more widely-used ML backends in the future.   References   [1] Caldas, Sebastian, et al. “Leaf: A benchmark for federated settings.” arXiv preprint arXiv:1812.01097 (2018).   [2] McMahan, Brendan, et al. “Communication-efficient learning of deep networks from decentralized data.” Artificial intelligence and statistics. PMLR, 2017.   [3] He, Chaoyang, et al. “Fedml: A research library and benchmark for federated machine learning.” arXiv preprint arXiv:2007.13518 (2020).   [4] Hard A, Rao K, Mathews R, et al. Federated learning for mobile keyboard prediction. arXiv preprint arXiv:1811.03604, 2018.   [5] Open standard for machine learning interoperability. https://github.com/onnx/onnx   [6] NVIDIA TensorRT. https://developer.nvidia.com/zh-cn/tensorrt  ","categories": [],
        "tags": [],
        "url": "/docs/simulation-and-deployment/",
        "teaser": null
      },{
        "title": "Contributing to FederatedScope",
        "excerpt":"As an open-sourced project, we categorize our users into two classes:      Running our code with different configurations, or re-organizing the training courses with their own entry scripts. These users may have no interest in understanding our implementation details.   Customizing some modules of FederatedScope by themselves, e.g., customized data loader, novel function approximators, and even novel federated learning algorithms. If you believe that other users could benefit from your ideas, we always welcome your pull-requests.   This post targets for the latter. We give an overview of our design and clarify the requirements any pull-request should obey.   Develop new modules   At first, we elaborate on how the various modules of our package FederatedScope are organized together. Our package is implemented by Python language, and thus the submodules are all located in the folder federatedscope/:   federatedscope/     |- attack/     |- autotune/     |- contrib/     |- core/     |- cv/     |- gfl/     |- methods/     |- mf/     |- nlp/     |- vertical_fl/     |- __init__.py     |- config.py     |- hpo.py     |- main.py     |- parse_exp_results.py     |- register.py   The .py files main.py and hpo.py are entry files to conduct a single FL course and an HPO procedure consisting a series of FL courses, respectively. The .py file config.py maintains all the involved hyperparameters. As for the folders, each corresponds to a specific submodule, where we discuss several of them as follow:      federatedscope.core: The infrastructure for conducting a FL course, including the definition of key concepts of runner, worker, and trainer. For more details about the design of FederatedScope, we refer our readers to our another post “Event-driven Architecture”.   federatedscope.gfl, federatedscope.cv, federatedscope.nlp, and federatedscope.mf: Data, models, and trainers dedicated to the corresponding application domain, which has led to many successful applications.   federatedscope.autotune: The AutoML-related functionalities, which has provided a rich collection of hyperparameter optimization methods (see the posts about HPO usage and HPO development for more details), yet stuff about automatic feature engineering and neural architecture search will come later.   federatedscope.attack: The privacy attack and defense related functionalities (more details), which enable our users to further validate the privacy-preserving property of their FL instances.   Although FederatedScope has provided these out-of-the-box FL capabilities, we highly encourage our users to express their novel ideas via FederatedScope and contribute the developed new modules into our package. For the ease of seeminglessly integrating contributed modules, we follow a popular design among open-sourced machine learning packages that allows any external class/function/variable to be registered as internal stuff of the package:      register.py: There are several dict objects, e.g., trainer_dict, metric_dict, criterion_dict, etc. Each such dict contains the objects designated for the corresponding purpose and can be further augmented with new objects.   contrib/ folder: Users are expected to put their new modules there. We have provided a new metric as an example.   We refer our readers to the post “Start your own case” for more examples about this register mechanism.   Ready to submit your pull-request   Please run scripts/format.sh and apply the changes. Otherwise, our linter won’t let your pull-request pass.   Please run scripts/ci_test.sh and ensure all test cases are passed. Otherwise, the test automatically triggered by your pull-requests would fail.  ","categories": [],
        "tags": [],
        "url": "/docs/contributor/",
        "teaser": null
      },{
        "title": "Recommendation",
        "excerpt":"Matrix Factorization  FederatedScope has built in the matrix factorization (MF) task for recommendation, which provides flexible supports for MF models, datasets and federated settings. In this tutorial, we will introduce      The matrix factorization task in FederatedScope   How to implement matrix factorization task with FederatedScope   The privacy preserving techniques used in FederatedScope   Background  Matrix factorization (MF) [1-3] is a fundamental building block in recommendation system. For a matrix, a row corresponds to a user, while a column corresponds to an item. The target of matrix factorization is to approximate unobserved ratings by constructing user embedding $U\\in{\\mathbb{R}^{n\\times{d}}}$ and item embedding $V\\in{\\mathbb{R}^{m\\times{d}}}$.       Supposing $X\\in{\\mathbb{R}^{n\\times{m}}}$ is the target rating matrix, the task aims at minimizing the loss function:   \\[\\frac{1}{| \\mathcal{D} |} \\sum_{(i,j) \\in \\mathcal{D}} \\mathcal{L}_{i,j} (X,U,V) = \\frac{1}{| \\mathcal{D} |} \\sum_{(i,j) \\in \\mathcal{D}}( X_{i,j} - &lt;u_i, v_j&gt;)^2\\]  where $u_i \\in{\\mathbb{R}^{n \\times{1}}}$ and $v_j \\in{\\mathbb{R}^{m \\times{1}}}$ are the user and item vectors of $U$ and $V$.   MF in Federated Learning  In federated learning, the dataset is distributed in different clients. The vanilla federated matrix factorization algorithm runs as follows      Step1: Server initializes shared parameters   Step2: Server broadcasts shared parameters to all participators   Step3: Each participator updates their parameters locally   Step4: Participators upload their shared parameters to the server   Step5: Server aggregates the received parameters and repeat Step2 until the training is finished   With different data partitions, matrix factorization has three FL settings: Vertical FL(VFL), Horizontal FL(HFL) and Local FL(LFL).   Vertical FL  In VFL, the set of users is the same across different databases, and each participators only has partial items. In this setting, the user embedding is shared across all participators and each client maintains its own item embedding.      Horizontal FL  In HFL, the set of items is the same across different participators, and they only share the item embedding with the coordination server.     Local FL  LFL is a special case of HFL, where each user owns her/his own ratings. It’s a common scenario on mobile devices.     Support of MF  To support federated MF, FederatedScope builds in MF models, datasets and trainer in different federated learning settings.   MF models  MF model has two trainable parameters: user embedding and item embedding. Based on the given federated setting, they share different embedding with the other participators. FederatedScope achieves VMFNetand HMFNetto support the settings of VFL and HFL.  class VMFNet(BasicMFNet):     name_reserve = \"embed_item\"   class HMFNet(BasicMFNet):     name_reserve = \"embed_user\"  The attribute name_reservespecifics the name of local embedding vector, and the parent classBasicMFNetdefines the common actions, including      load/fetch parameters, and   forward propagation.   Note the rating matrix is usually very sparse. To impove the efficiency, FederatedScope creates the predicted matrix and the target rating matrix as sparse tensors.  class BasicMFNet(Module):     ...     def forward(self, indices, ratings):         pred = torch.matmul(self.embed_user, self.embed_item.T)         label = torch.sparse_coo_tensor(indices,                                         ratings,                                         size=pred.shape,                                         device=pred.device,                                         dtype=torch.float32).to_dense()         mask = torch.sparse_coo_tensor(indices,                                        np.ones(len(ratings)),                                        size=pred.shape,                                        device=pred.device,                                        dtype=torch.float32).to_dense()          return mask * pred, label, float(np.prod(pred.size())) / len(ratings)     ...   MF Datasets  MovieLens is series of movie recommendation datasets collected from the website MovieLens.  To satisify the requirement of different FL settings, FederatedScope splits the dataset into VFLMoviesLensand HFLMovieLensas follows. For example, if your want to use the dataset MovieLens1M in VFL settings, just set cfg.data.type='VFLMovieLens1M'.  class VFLMovieLens1M(MovieLens1M, VMFDataset):     \"\"\"MovieLens1M dataset in VFL setting          \"\"\"     pass   class HFLMovieLens1M(MovieLens1M, HMFDataset):     \"\"\"MovieLens1M dataset in HFL setting      \"\"\"     pass   class VFLMovieLens10M(MovieLens10M, VMFDataset):     \"\"\"MovieLens10M dataset in VFL setting      \"\"\"     pass   class HFLMovieLens10M(MovieLens10M, HMFDataset):     \"\"\"MovieLens10M dataset in HFL setting      \"\"\"     pass   The parent classes of the above datasets define the data information and the FL setting respectively.   Data information  The first parent class MovieLens1M and MovieLens10Mprovide the details (e.g. url, md5, filename).  class MovieLens1M(MovieLensData):     \"\"\"MoviesLens 1M Dataset     (https://grouplens.org/datasets/movielens)      Format:         UserID::MovieID::Rating::Timestamp      Arguments:         root (str): Root directory of dataset where directory             ``MoviesLen1M`` exists or will be saved to if download is set to True.         config (callable): Parameters related to matrix factorization.         train_size (float, optional): The proportion of training data.         test_size (float, optional): The proportion of test data.         download  (bool, optional): If true, downloads the dataset from the internet and             puts it in root directory. If dataset is already downloaded, it is not             downloaded again.      \"\"\"     base_folder = 'MovieLens1M'     url = \"https://files.grouplens.org/datasets/movielens/ml-1m.zip\"     filename = \"ml-1m\"     zip_md5 = \"c4d9eecfca2ab87c1945afe126590906\"     raw_file = \"ratings.dat\"     raw_file_md5 = \"a89aa3591bc97d6d4e0c89459ff39362\"   class MovieLens10M(MovieLensData):     \"\"\"MoviesLens 10M Dataset     (https://grouplens.org/datasets/movielens)      Format:         UserID::MovieID::Rating::Timestamp      Arguments:         root (str): Root directory of dataset where directory             ``MoviesLen1M`` exists or will be saved to if download is set to True.         config (callable): Parameters related to matrix factorization.         train_size (float, optional): The proportion of training data.         test_size (float, optional): The proportion of test data.         download  (bool, optional): If true, downloads the dataset from the internet and             puts it in root directory. If dataset is already downloaded, it is not             downloaded again.      \"\"\"     base_folder = 'MovieLens10M'     url = \"https://files.grouplens.org/datasets/movielens/ml-10m.zip\"     filename = \"ml-10M100K\"      zip_md5 = \"ce571fd55effeba0271552578f2648bd\"     raw_file = \"ratings.dat\"     raw_file_md5 = \"3f317698625386f66177629fa5c6b2dc\"   FL Setting  VMFDatasetand HMFDatasetspecific the spliting of MF datasets (VFL or HFL).  class VMFDataset:     \"\"\"Dataset of matrix factorization task in vertical federated learning.      \"\"\"     def _split_n_clients_rating(self, ratings: csc_matrix, num_client: int,                                 test_portion: float):         id_item = np.arange(self.n_item)         shuffle(id_item)         items_per_client = np.array_split(id_item, num_client)         data = dict()         for clientId, items in enumerate(items_per_client):             client_ratings = ratings[:, items]             train_ratings, test_ratings = self._split_train_test_ratings(                 client_ratings, test_portion)             data[clientId + 1] = {\"train\": train_ratings, \"test\": test_ratings}         self.data = data   class HMFDataset:     \"\"\"Dataset of matrix factorization task in horizontal federated learning.      \"\"\"     def _split_n_clients_rating(self, ratings: csc_matrix, num_client: int,                                 test_portion: float):         id_user = np.arange(self.n_user)         shuffle(id_user)         users_per_client = np.array_split(id_user, num_client)         data = dict()         for cliendId, users in enumerate(users_per_client):             client_ratings = ratings[users, :]             train_ratings, test_ratings = self._split_train_test_ratings(                 client_ratings, test_portion)             data[cliendId + 1] = {\"train\": train_ratings, \"test\": test_ratings}         self.data = data   MF Trainer  Considering the target rating matrix is large and sparse, FederatedScope achievesMFTrainerto support MF tasks during federated training.  class MFTrainer(GeneralTrainer):     \"\"\"     model (torch.nn.module): MF model.     data (dict): input data     device (str): device.     \"\"\"      def _hook_on_fit_end(self, ctx):         results = {             \"{}_avg_loss\".format(ctx.cur_mode): ctx.get(\"loss_batch_total_{}\".format(ctx.cur_mode)) /             ctx.get(\"num_samples_{}\".format(ctx.cur_mode)),             \"{}_total\".format(ctx.cur_mode): ctx.get(\"num_samples_{}\".format(ctx.cur_mode))         }         setattr(ctx, 'eval_metrics', results)      def _hook_on_batch_end(self, ctx):         # update statistics         setattr(             ctx, \"loss_batch_total_{}\".format(ctx.cur_mode),             ctx.get(\"loss_batch_total_{}\".format(ctx.cur_mode)) +             ctx.loss_batch.item() * ctx.batch_size)          if ctx.get(\"loss_regular\", None) is None or ctx.loss_regular == 0:             loss_regular = 0.         else:             loss_regular = ctx.loss_regular.item()         setattr(             ctx, \"loss_regular_total_{}\".format(ctx.cur_mode),             ctx.get(\"loss_regular_total_{}\".format(ctx.cur_mode)) +             loss_regular)         setattr(             ctx, \"num_samples_{}\".format(ctx.cur_mode),             ctx.get(\"num_samples_{}\".format(ctx.cur_mode)) + ctx.batch_size)          # clean temp ctx         ctx.data_batch = None         ctx.batch_size = None         ctx.loss_task = None         ctx.loss_batch = None         ctx.loss_regular = None         ctx.y_true = None         ctx.y_prob = None      def _hook_on_batch_forward(self, ctx):         indices, ratings = ctx.data_batch         pred, label, ratio = ctx.model(indices, ratings)         ctx.loss_batch = ctx.criterion(pred, label) * ratio          ctx.batch_size = len(ratings)   Start an Example  Taking the combination of dataset MovieLen1Mand VFL setting as an example, the running command is as follows.  python main.py --cfg federatedscope/mf/baseline/fedavg_vfl_fedavg_standalone_on_movielens1m.yaml   More running scripts can be found in federatedscope/scripts. Partial experimental results are shown as follows.                  Federated setting       Dataset       Number of clients       Loss                       VFL       MovieLens1M       5       1.16                 HFL       MovieLens1M       5       1.13           Privacy Protection  To protect the user privacy, FederatedScope implements two differential privacy algorithms, VFL-SGDMF and HFL-SGDMF in [vldb22] as plug-ins.   VFL-SGDMF  VFL-SGDMF is a DP based algorithm for privacy preserving in VFL setting. It satisfies $(\\epsilon, \\delta)$privacy by injecting noise into the embedding matrix. More details please refer to [3]. The related parameters are shown as follows.  # ------------------------------------------------------------------------ # # VFL-SGDMF(dp) related options # ------------------------------------------------------------------------ # cfg.sgdmf = CN()  cfg.sgdmf.use = False    # if use sgdmf cfg.sgdmf.R = 5.         # The upper bound of rating cfg.sgdmf.epsilon = 4.   # \\epsilon in dp cfg.sgdmf.delta = 0.5    # \\delta in dp cfg.sgdmf.constant = 1.  # constant cfg.sgdmf.theta = -1     # -1 means per-rating privacy, otherwise per-user privacy   VFL-SGDMF is implemented as plug-in in federatedscope/mf/trainer/trainer_sgdmf.py. Similar with the other plug-in algorithms, it initializes and registers hook functions in the functionwrap_MFTrainer.  def wrap_MFTrainer(base_trainer: Type[MFTrainer]) -&gt; Type[MFTrainer]:     \"\"\"Build `SGDMFTrainer` with a plug-in manner, by registering new functions into specific `MFTrainer`      \"\"\"      # ---------------- attribute-level plug-in -----------------------     init_sgdmf_ctx(base_trainer)      # ---------------- action-level plug-in -----------------------     base_trainer.replace_hook_in_train(new_hook=hook_on_batch_backward,                                        target_trigger=\"on_batch_backward\",                                        target_hook_name=\"_hook_on_batch_backward\")      return base_trainer   The embedding clipping and noise injection is finished in the new hook function hook_on_batch_backward.  def hook_on_batch_backward(ctx):     \"\"\"Private local updates in SGDMF      \"\"\"     ctx.optimizer.zero_grad()     ctx.loss_task.backward()          # Inject noise     ctx.model.embed_user.grad.data += get_random(         \"Normal\",         sample_shape=ctx.model.embed_user.shape,         params={             \"loc\": 0,             \"scale\": ctx.scale         },         device=ctx.model.embed_user.device)     ctx.model.embed_item.grad.data += get_random(         \"Normal\",         sample_shape=ctx.model.embed_item.shape,         params={             \"loc\": 0,             \"scale\": ctx.scale         },         device=ctx.model.embed_item.device)     ctx.optimizer.step()      # Embedding clipping     with torch.no_grad():         embedding_clip(ctx.model.embed_user, ctx.sgdmf_R)         embedding_clip(ctx.model.embed_item, ctx.sgdmf_R)   Start an Example  Similarly, taking MovieLens1M as an example, the running script is shown as follows.  python federatedscope/main.py --cfg federatedscope/mf/baseline/vfl-sgdmf_fedavg_standalone_on_movielens1m.yaml   Evaluation  Take the dataset MovieLens1M as an example, the detailed settings are listed in federatedscope/mf/baseline/vfl_fedavg_standalone_on_movielens1m.yaml and federatedscope/mf/baseline/vfl-sgdmf_fedavg_standalone_on_movielens1m.yaml. VFL-SGDMF is evaluated as follows.                  Algo       $\\epsilon$       $\\delta$       Loss                       VFL       -       -       1.16                 VFL-SGDMF       4       0.75       1.47                 VFL-SGDMF       4       0.25       1.54                 VFL-SGDMF       2       0.75       1.55                 VFL-SGDMF       2       0.25       1.56                 VFL-SGDMF       0.5       0.75       1.68                 VFL-SGDMF       0.5       0.25       1.84           HFL-SGDMF  On the other side,  HFL-SGDMF protects privacy in HFL setting in the same way, and share the same parameters with VFL-SGDMF.  # ------------------------------------------------------------------------ # # VFL-SGDMF(dp) related options # ------------------------------------------------------------------------ # cfg.sgdmf = CN()  cfg.sgdmf.use = False    # if use sgdmf cfg.sgdmf.R = 5.         # The upper bound of rating cfg.sgdmf.epsilon = 4.   # \\epsilon in dp cfg.sgdmf.delta = 0.5    # \\delta in dp cfg.sgdmf.constant = 1.  # constant cfg.sgdmf.theta = -1     # -1 means per-rating privacy, otherwise per-user privacy   Start and Example  Run an example of HFL-SGDMF by the following command.  python federatedscope/main.py --cfg federatedscope/mf/baseline/hfl-sgdmf_fedavg_standalone_on_movielens1m.yaml   Evaluation  The evaluation results of HFL-SGDMF on the dataset MovieLens1M are shown as follows.                  Algo       $\\epsilon$       $\\delta$       Loss                       HFL       -       -       1.13                 HFL-SGDMF       4       0.75       1.56                 HFL-SGDMF       4       0.25       1.62                 HFL-SGDMF       2       0.75       1.60                 HFL-SGDMF       2       0.25       1.64                 HFL-SGDMF       0.5       0.75       1.66                 HFL-SGDMF       0.5       0.25       1.73            References  [1] Ma H, Yang H, Lyu M R, et al. “SoRec: social recommendation using probabilistic matrix factorization”. Proceedings of the ACM Conference on Information and Knowledge Management, 2008.   [2] Jamali M, Ester M. “A matrix factorization technique with trust propagation for recommendation in social networks”. Proceedings of the ACM Conference on Recommender Systems, 2010.   [3] Li Z, Ding B, Zhang C, et al. “Federated matrix factorization with privacy guarantee”. Proceedings of the VLDB Endowment, 2022.  ","categories": [],
        "tags": [],
        "url": "/docs/recommendation/",
        "teaser": null
      },{
        "title": "Differential Privacy",
        "excerpt":"Background  Differential privacy (DP) [1] is a powerful theoretical criterion metric for privacy preserving in database. Specifically, a randomized mechanism satisfying $(\\epsilon, \\delta)$-DP promises the privacy loss of all neighboring datasets is bounded by $\\epsilon$ with the probability at least $1-\\delta$[1].   Differential Privacy:   A randomized algorithm $\\mathcal{M}$ satisfies $(\\epsilon, \\delta)$-DP if for all  $S \\subseteq{Range(\\mathcal{M})}$ and all neighboring datasets  $(x, y)$:   $Pr( \\mathcal{M} (x) \\in{S}) \\leq{exp(\\epsilon) Pr(\\mathcal{M} (y) \\in{S})} + \\delta$.   Support of DP  In federated learning scenario, noise injection and gradient clipping are foundational tools for differential privcay. In FederatedScope, DP algorithms are supported by preseting APIs in the core library, including      noise injection in download channel (server)   noise injection in unload channel (client)   gradient clipping before upload   Noise Injection in Download  In server, a protected attribute _noise_injector is preset in the server class for noise injection with default value is None.  class Server(Worker):     def __init__(**kwargs):         ...         # inject noise before broadcast         self._noise_injector = None          ...  Developers can achieve noise injection by calling register_noise_injector. Then the function self._noise_injectorwill be called before the server broadcasts parameters.  class Server(Worker):     ...     def register_noise_injector(self, func):         self._noise_injector = func     ...   Noise Injection in Upload  For clients, the noise injection can be done by registering hook function at the end of training (before uploading parameters).  def noise_injection_function(ctx):     pass  trainer.register_hook_in_train(new_hook=noise_injection_function,                               trigger='on_fit_end',                               insert_pos=-1)    Gradient Clipping  Gradient clipping is preset in flapackage/core/trainers/trainer.py. When the function _hook_on_batch_backwardis called, the gradient will be clipped by the parameter cfg.optimizer.grad_clipin the config.      ...     def _hook_on_batch_backward(self, ctx):         ctx.optimizer.zero_grad()         ctx.loss_task.backward()         if ctx.grad_clip &gt; 0:             torch.nn.utils.clip_grad_norm_(ctx.model.parameters(),                                            ctx.grad_clip)         ctx.optimizer.step()     ...  Threshold of gradient clipping in federatedscope/config.py.  # ------------------------------------------------------------------------ # # Optimizer related options # ------------------------------------------------------------------------ # cfg.optimizer = CN()  cfg.optimizer.type = 'SGD' cfg.optimizer.lr = 0.1 cfg.optimizer.weight_decay = .0 cfg.optimizer.grad_clip = -1.0  # negative numbers indicate we do not clip grad   Implementation of DP  NbAFL [2] is a DP algorithm designed for federated learning, which protects both the upload and download channels with $(\\epsilon, \\delta)$-DP. Taking NbAFL as an example, we show how to implement DP algorithm in FederatedScope.   Prepare DP Parameters  Add parameters into federatedscope/config.py. Note FederatedScope supports at most two levels of config, e.g., cfg.data.type.  # ------------------------------------------------------------------------ # # nbafl(dp) related options # ------------------------------------------------------------------------ # cfg.nbafl = CN()  # Params cfg.nbafl.use = False cfg.nbafl.mu = 0. cfg.nbafl.epsilon = 100. cfg.nbafl.w_clip = 1. cfg.nbafl.constant = 30.   Prepare DP Functions  Then developers should design their own DP functions. For NbAFL, the following three hook functions are required for clientTrainer.  The functionsrecord_initializationand del_initializationmaintain the initialization received from the server, and inject_noise_in_uploadinjects noise into the model before upload.  def record_initialization(ctx):     ctx.weight_init = deepcopy(         [_.data.detach() for _ in ctx.model.parameters()])   def del_initialization(ctx):     ctx.weight_init = None   def inject_noise_in_upload(ctx):     \"\"\"Inject noise into weights before the client upload them to server      \"\"\"     scale_u = ctx.nbafl_w_clip * ctx.nbafl_total_round_num * 2 * ctx.nbafl_constant / ctx.num_train_data / ctx.nbafl_epsilon     # logging.info({\"Role\": \"Client\", \"Noise\": {\"mean\": 0, \"scale\": scale_u}})     for p in ctx.model.parameters():         noise = get_random(\"Normal\", p.shape, {             \"loc\": 0,             \"scale\": scale_u         }, p.device)         p.data += noise   For the server, the following function is created for noise injection.  def inject_noise_in_broadcast(cfg, sample_client_num, model):     \"\"\"Inject noise into weights before the server broadcasts them      \"\"\"     if len(sample_client_num) == 0:         return      # Clip weight     for p in model.parameters():         p.data = p.data / torch.max(torch.ones(size=p.shape),                                     torch.abs(p.data) / cfg.nbafl.w_clip)      if len(sample_client_num) &gt; 0:         # Inject noise         L = cfg.federate.sample_client_num if cfg.federate.sample_client_num &gt; 0 else cfg.federate.client_num         if cfg.federate.total_round_num &gt; np.sqrt(cfg.federate.client_num) * L:             scale_d = 2 * cfg.nbafl.w_clip * cfg.nbafl.constant * np.sqrt(                 np.power(cfg.federate.total_round_num, 2) -                 np.power(L, 2) * cfg.federate.client_num) / (                     min(sample_client_num.values()) * cfg.federate.client_num *                     cfg.nbafl.epsilon)             for p in model.parameters():                 p.data += get_random(\"Normal\", p.shape, {                     \"loc\": 0,                     \"scale\": scale_d                 }, p.device)     Register DP Functions  The wrap function wrap_nbafl_trainer initializes parameters related to NbAFL and registers the above hook functions.  def wrap_nbafl_trainer(         base_trainer: Type[GeneralTrainer]) -&gt; Type[GeneralTrainer]:     \"\"\"Implementation of NbAFL refer to `Federated Learning with Differential Privacy: Algorithms and Performance Analysis` [et al., 2020]         (https://ieeexplore.ieee.org/abstract/document/9069945/)          Arguments:             mu: the factor of the regularizer             epsilon: the distinguishable bound             w_clip: the threshold to clip weights      \"\"\"      # ---------------- attribute-level plug-in -----------------------     init_nbafl_ctx(base_trainer)      # ---------------- action-level plug-in -----------------------     base_trainer.register_hook_in_train(new_hook=record_initialization,                                         trigger='on_fit_start',                                         insert_pos=-1)      base_trainer.register_hook_in_eval(new_hook=record_initialization,                                        trigger='on_fit_start',                                        insert_pos=-1)      base_trainer.register_hook_in_train(new_hook=del_initialization,                                         trigger='on_fit_end',                                         insert_pos=-1)      base_trainer.register_hook_in_eval(new_hook=del_initialization,                                        trigger='on_fit_end',                                        insert_pos=-1)      base_trainer.register_hook_in_train(new_hook=inject_noise_in_upload,                                         trigger='on_fit_end',                                         insert_pos=-1)     return base_trainer   Finally, in federatedscope/core/auxiliaries/trainer_builder.py, the function get_trainerwraps the basic trainer with NbAFL variables and functions.  def get_trainer(model=None,                 data=None,                 device=None,                 config=None,                 only_for_eval=False,                 is_attacker=False):     ...     # differential privacy plug-in     if config.nbafl.use:         from federatedscope.core.trainers.trainer_nbafl import wrap_nbafl_trainer         trainer = wrap_nbafl_trainer(trainer)     ...   Run an Example  Run the  following command to call NbAFL on the dataset Femnist.  python federatedscope/main.py --cfg federatedscope/cv/baseline/fedavg_convnet2_on_femnist.yaml \\   nbafl.mu 0.01 \\   nbafl.constant 1 \\   nbafl.w_clip 0.1 \\   nbafl.epsilon 10   Evaluation  Take the dataset Femnist as an example, the accuracy with different $(\\epsilon, \\delta)$-DP is shown as follows.                  Task       $\\epsilon$       $\\delta$       Accuracy(%)                       FEMNIST       10       0.01       11.73                         10       0.17       24.82                         10       0.76       41.71                         50       0.01       54.85                         50       0.17       67.98                         50       0.76       80.58                         100       0.01       74.80                         100       0.17       80.39                         100       0.76       80.58            References  [1] Dwork C, Roth A. “The Algorithmic Foundations of Differential Privacy”. Foundations and Trends in Theoretical Computer Science, 2014, 9(3-4): 211-407.   [2] Wei K, Li J, Ding M, et al. “Federated Learning With Differential Privacy: Algorithms and Performance Analysis”. IEEE Transactions on Information Forensics and Security, 2020, 15: 3454-3469.  ","categories": [],
        "tags": [],
        "url": "/docs/dp/",
        "teaser": null
      },{
        "title": "Privacy Attacks",
        "excerpt":"We provide the implementation of privacy attacker in FederateScope for developers to conveniently demonstrate the privacy-preserving strength of the design systems and algorithms, since applying privacy attacks directly on the algorithm is an effective way to detect the vulnerability of FL.   In this part, we will introduce:     Types of Privacy attacks.   Usages of attack module in FederatedScope            Example of Optimization based Training Data/Label Inference Attack       Example of Class Representative Attack       Example of Membership Inference Attack       Example of Property Inference Attack           Develop your own attacker.   Contribute your attackers.   1. Background of Privacy Attack in FL  This part briefly introduces the types of privacy attacks in FL. For more detailed information about the privacy attacks in FL, please refer [7].   1.1 Different Attack Targets  According to the types of attacker’s target, typical attacks include membership inference attack, property inference attack, class representative attack, and training data/label inference attack.           Membership Inference Attack (MIA): In membership inference attack, an attacker can be a server or a client, and the objective is to infer whether the specific given data exists in another client’s private dataset.            Property Inference Attack (PIA): Property inference attack aims to infer the dataset property (may be the sensitive property) other than the class label. For example, in the facial image dataset, the original task is to infer whether wearing glass, an attacker may also be curious about the gender and age unrelated to the original task.            Class Representative Attack (CRA): Class representative attack aims to infer the representative sample of a specific class. This type of attack often exists in the case where a client only owns a part of the class label, and is curious about the information related to other classes.            Training Data/Label Inference Attack (TIA): Training data/label inference attack aims to reconstruct the privately owned training samples through the intermediate information transmitted during the FL.       1.2 Active Attack v.s. Passive Attack  According to the attacker’s actions, the privacy attacks can be divided into passive and active attacks.           Passive Attack: In a passive attack, the attacker follows the FL protocols, and only saves the intermediate results or the received information for local attack computation. Due to the characteristics of a passive attack, it is very hard to be detected by the FL monitor.            Active Attack: Different from the passive attack, in the active attack, the attacker often injects malicious information into FL to induce other clients to reveal more private information into the global model. The attacker performing active attacks are also named malicious. Compared with the passive attack, due to the malicious information injection, this type of attack is easier to be detected with additional information checking.       2. Usage of Attack Module  2.1 Preliminary  Before calling the attack module, the user should make sure that the FL has been set up. For more details of setting up FL, please refer quick start and start your own case.   The attack module provides several attack methods for directly using. The users only need to set the corresponding hyper-parameters in the configuration to call the corresponding method and add the prior knowledge to the attacker.   In order to make it easier for users to check the privacy protection strength of FL, FederatedScope provides implementations of SOTA privacy attack algorithms. The users only need to add the configuration of the attack module into FL’s configuration, and provide the additional information required by the attack method.   The implemented privacy algorithms include: (1) Membership inference attack: Gradient Ascent (Active attack) [5] . (2) property inference attack: BPC (Passive attack) [6]. (3) Class representative attack: DCGAN (Passive attack) [2] . (4) training data/label inference attack: DLG [3], iDLG [1], InvertGradient [4].   In the next, we will use the example of attacking fedAvg to show the four kinds of attacks.   2.2 Example of Optimization based Training Data/Label Inference Attack.  In this attack, the server is set as the attacker performing the passive attack, i.e., when the server receives the model parameter updating information from the target client, it will perform the reconstruction procedure to find the data that generates the same parameter updating as received. Specifically, DLG method optimizes the Eqn. (4) in [1] and InvertingGradient (IG) method optimize the Eqn. (4) in [4]   The knowledge that the attacker needs to obtain includes the prior knowledge provided by the users, and the knowledge (e.g., model parameter updates/gradients) obtained from FL training procedure.   attacker’s prior knowledge:     The feature dimension of the dataset.   Attcker’s knowledge obtained from FL:     The parameter updates;   The number of samples corresponding the received parameter updates.   2.2.1 Running the attack on Femnist Example   Step 1: Set the configuration of fedavg;   Step 2:  Add the configurations of attack to the configuration in step 1;   configuration:   attack:   attack_method: DLG   max_ite: 500   reconstruct_lr: 0.1   The alternatives of attack_method are: “DLG”, “IG” which correspond to method DLG and InvertingGradient (IG) respectively. max_ite, reconstruct_lr denote the maximum iteration and the learning rate of the optimization for reconstruction, separately.   Step 3: Run the FL with the modified configuration;   The command to run the example attack:   python federatedscope/main.py --cfg federatedscope/attack/example_attack_config/reconstruct_fedavg_opt_on_femnist.yaml   Results on FedAvg on Femnist example: The recovered training images are plotted in the directory of cfg.outdir.   The reconstructed results at round 31 &amp; 32 is:    2.2.1  Running the attack on customized dataset   Step 1: The users should make sure the FL is set up correctly on the customized dataset.   Step 2: Add the prior knowledge about the dataset to function  get_data_info in federatedscope/attack/auxiliary/utils.py. An example of femnist dataset is:   def get_data_info(dataset_name):     if dataset_name.lower() == 'femnist':         return [1, 28, 28], 36, False     else:         ValueError(             'Please provide the data info of {}: data_feature_dim, num_class'.             format(dataset_name))  The function takes the dataset_name as the input, and it should return data_feature_dim, num_class, is_one_hot_label, denoting: feature dimension, number of total classes, whether the label is represented in one-hot version, separately.   Step 3: Run the FL with the modified configuration.   python federatedscope/main.py --cfg your_own_config.yaml    2.3 Example of Class Representative Attack   The attack module provides the class representative attack with GAN, which is the implementation of the method in [2].   2.3.1 Example attack on Femnist dataset   Configuration: The configuration that is added to the existing fedAve configurationn is:  attack:   attack_method: gan_attack   attacker_id: 5   target_label_ind: 3  attack_method is the method’s name, attacker_id is the id the client performs the class representative attack. target_label_ind is the index of the target class that the attacker wants to infer its representative samples.   The command to run the example attack:  python federatedscope/main.py --cfg federatedscope/attack/example_attack_config/CRA_fedavg_convnet2_on_femnist.yaml   Results: The results of the recovered class representatives are plotted in the directory of cfg.outdir.   Results on Femnist dataset with target label class “3”:  The representative samples is:     2.3.2  Running the attack on the customized dataset  To run the customized dataset, the users should define the dataset’s corresponding generator function, and add it to function ‘get_generator’ in ‘federatedscope/attack/auxiliary/utils.py’.   In the example of Femnist, the generator is defined in ‘federatedscope/attack/models/gan_based_model.py’, and the get_generator is:  def get_generator(dataset_name):     if dataset_name == 'femnist':         from federatedscope.attack.models.gan_based_model import GeneratorFemnist         return GeneratorFemnist     else:         ValueError(\"The generator to generate data like {} is not defined!\".format(dataset_name))  It takes the dataset_name as the input and returns the dataset’s corresponding generator.   2.4 Example of Membership Inference Attack  In the Membership Inference Attack, the attacker’s object is to infer whether the given target data exists in another client’s private datasets. FederatedScope provides the method GradAscent, which is an active attack proposed in [5]. Specifically, GradAscent runs the gradient ascent to the gradient corresponding to the target data:  \\(W  \\leftarrow W + \\gamma \\frac{\\partial L_x}{\\partial W},\\)  where $W$ is the model parameter, $L_x$ is the loss of the target data.   If the target data $x$ in another client’s training dataset, it would have a large update on the model updates, since its optimizer will abruptly reduce the gradient of $L_x$.   2.4.1 Example of MIA on Femnist dataset  The configuration of GradAscent is:  attack:   attack_method: GradAscent   attacker_id: 5   inject_round: 0  Where attack_method is the attack method name, attacker_id is the id of the client that performs the membership inference attack, inject_round is the round to run the gradient ascent on the target dataset.   The command to run the example attack:  python federatedscope/main.py --cfg federatedscope/attack/example_attack_config/gradient_ascent_MIA_on_femnist.yaml   The results of loss changes on the target data are plotted in the directory of cfg.outdir.   2.4.2 Running the attack on a customized dataset  The users should add the way to get the target data in function get_target_data in federatedscope/attack/auxiliary/MIA_get_target_data.py   2.5 Example of Property Inference Attack  In the property inference attack, the attacker aims to infer the property of the sample that is irrelevant to the classification task. In the server-client FL settings, the attacker is usually the server, and based on the received parameter updates, and it wants to infer whether the property exists in the batch based on which the batch owner client sends the parameter updates to the server.   2.5.1 Running the attack on the synthetic dataset  Since the Femnist dataset doesn’t have the additional property information, we use the synthetic dataset as the example dataset and implement the BPC, which is algorithm 3 in [6].   The synthetic dataset:     The feature x with 5 dimension is generation by $N(0,0.5)$;   Label related:  $w_x\\sim N(0, 1)$ and $b_x\\sim N(0, 1)$ ;   Property related:  $w_p\\sim N(0, 1)$ and $b_p\\sim N(0, 1)$;   Label: $y = w_xx + b_x$   Property: $\\text{prop} = \\text{sigmoid}(w_px + b_p)$   The configureation of PIA:  attack:   attack_method: PassivePIA   classifier_PIA: svm  where attack_method is the attack method name; classifier_PIA is the method name of the classifer that infer the property.   The command to run the example attack:  python federatedscope/main.py --cfg federatedscope/attack/example_attack_config/PIA_toy.yaml   3. Develop Your Own Attack  When developing your own attack method, it only needs to overload the class of server, client, or trainer according to the role of the attacker and the actions the attacker made in FL. Before developing your own attack, please make sure that your target FL algorithm is already set up.   3.1 Add New Configuration   If your own attack method requires new hyper-parameters, they should be registered in the configuration. The details of registering your own hyper-parameter can be found in this post.   3.2 Server as the attacker  When the attacker is the server, it requires an overload of the Server class of the target FL algorithm.     If the attack happens in the FL training process, the users should rewrite the callback_funcs_model_para, which is the function that called when the server receives the model parameter updates.   If the attack happens at the end of the FL training, the users should check whether it is the last round, and if yes, it will execute the attack actions.   After overloading the server class, you should add it the function get_server_cls in federatedscope/core/auxiliaries/worker_builder.py .   3.2.1 Example of developing property inference attack  The following is an example in a property inference attack where the attacker (server) performs attack actions both during and after the FL training. The attacker collects the received parameter updates during the FL training and generates the training data for PIA classifier based on the current model. After the FL training, the attacker trains the PIA classifier, and then infers the property based on the collected parameter updates.   In the following code, the original Server’s callback_funcs_model_para function is rewritten to perform the above attack actions.  class PassivePIAServer(Server):     '''     The implementation of the batch property classifier, the algorithm 3 in paper: Exploiting Unintended Feature Leakage in Collaborative Learning      References:     Melis, Luca, Congzheng Song, Emiliano De Cristofaro and Vitaly Shmatikov. “Exploiting Unintended Feature Leakage in Collaborative Learning.” 2019 IEEE Symposium on Security and Privacy (SP) (2019): 691-706     '''     def __init__(self,                  ID=-1,                  state=0,                  data=None,                  model=None,                  client_num=5,                  total_round_num=10,                  device='cpu',                  strategy=None,                  **kwargs):         super(PassivePIAServer, self).__init__(ID=ID,                                                state=state,                                                data=data,                                                model=model,                                                client_num=client_num,                                                total_round_num=total_round_num,                                                device=device,                                                strategy=strategy,                                                **kwargs)         self.atk_method = self._cfg.attack.attack_method         self.pia_attacker = PassivePropertyInference(             classier=self._cfg.attack.classifier_PIA,             fl_model_criterion=get_criterion(self._cfg.criterion.type,                                              device=self.device),             device=self.device,             grad_clip=self._cfg.optimizer.grad_clip,             dataset_name=self._cfg.data.type,             fl_local_update_num=self._cfg.federate.local_update_steps,             fl_type_optimizer=self._cfg.fedopt.type_optimizer,             fl_lr=self._cfg.optimizer.lr,             batch_size=100)      def callback_funcs_model_para(self, message: Message):         round, sender, content = message.state, message.sender, message.content         # For a new round         if round not in self.msg_buffer['train'].keys():             self.msg_buffer['train'][round] = dict()          self.msg_buffer['train'][round][sender] = content          # PIA: collect the parameter updates          self.pia_attacker.collect_updates(             previous_para=self.model.state_dict(),             updated_parameter=content[1],             round=round,             client_id=sender)         # PIA: generate the training data for inference classifier training based on the auxiliary dataset          self.pia_attacker.get_data_for_dataset_prop_classifier(             model=self.model)          if self._cfg.federate.online_aggr:             self.aggregator.inc(content)         self.check_and_move_on()          if self.state == self.total_round_num:             # PIA: Property inference classifier training             self.pia_attacker.train_property_classifier()             # PIA: Property inference classifier infer the property based on the colloected parameter updates              self.pia_results = self.pia_attacker.infer_collected()             print(self.pia_results)   The final step is to add the class PassivePIAServer into get_server_cls in federatedscope/core/auxiliaries/worker_builder.py.   def get_server_cls(cfg):     if cfg.attack.attack_method.lower() in ['dlg', 'ig']:         from federatedscope.attack.worker_as_attacker.server_attacker import PassiveServer         return PassiveServer     elif cfg.attack.attack_method.lower() in ['passivepia']:         from federatedscope.attack.worker_as_attacker.server_attacker import PassivePIAServer         return PassivePIAServer   3.3 Client as the attacker  When the attacker is one of the clients:     If the attack actions only happen in the local training procedure, users only need to define the wrapping function to wrap the trainer and add the attack actions.   If the attack actions also happen at the end of the FL training, users need to overload the client class and modify its callback_funcs_for_finish function.   After setting the trainer wrapping function, it should be added to the function wrap_attacker_trainer in federatedscope/attack/auxiliary/attack_trainer_builder.py. Similarly, after overloading the client class, it should be added to function get_client_cls in federatedscope/core/auxiliaries/worker_builder.py.   3.3.1 Example of developing class representative attack  The following is an example of wrapping the trainer in the implementation of the class representative attack in [2]. In this method, the attacker holds a local GAN, and at each FL training round, it will first update the GAN’s discriminator with the received paramters, and then local trains GAN’s generator so that its generated data can be classified as the target class. After that, the client labels the generated data as the class other than the target class and injects them into the training batch to perform the regular local training.   The following code shows the wrapping function that adds the above procedures to the trainer. The wrapping function takes the trainer instance as the input.  It first adds an instance of GANCRA class which is the GAN attack class defined in federatedscope/attack/models/gan_based_model.py to the context of the trainer, i.e., base_trainer.ctx . Then it registers different hooks into its corresponding phase:     Register the hook hood_on_fit_start_generator to the phase’on_fit_start`, so that the trainer will update the round number at the beginning of local training in each round.   Register the hook hook_on_gan_cra_train and hook_on_batch_injected_data_generation to the phase on_batch_start, so that the trainer will train the GAN and inject the generated data to the training batch when preparing the train batch for local training.   Register the hook hook_on_data_injection_sav_data to the phase on_fit_end , so that at the end of local training, it will save the data generated by GAN.   def wrap_GANTrainer(         base_trainer: Type[GeneralTrainer]) -&gt; Type[GeneralTrainer]:      # ---------------- attribute-level plug-in -----------------------      base_trainer.ctx.target_label_ind = base_trainer.cfg.attack.target_label_ind     base_trainer.ctx.gan_cra = GANCRA(base_trainer.cfg.attack.target_label_ind,                                       base_trainer.ctx.model,                                       dataset_name = base_trainer.cfg.data.type,                                       device=base_trainer.ctx.device,                                       sav_pth=base_trainer.cfg.outdir                                       )      # ---- action-level plug-in -------      base_trainer.register_hook_in_train(new_hook=hood_on_fit_start_generator,                                         trigger='on_fit_start',                                         insert_mode=-1)     base_trainer.register_hook_in_train(new_hook=hook_on_gan_cra_train,                                         trigger='on_batch_start',                                         insert_mode=-1)     base_trainer.register_hook_in_train(         new_hook=hook_on_batch_injected_data_generation,         trigger='on_batch_start',         insert_mode=-1)     base_trainer.register_hook_in_train(         new_hook=hook_on_batch_forward_injected_data,         trigger='on_batch_forward',         insert_mode=-1)      base_trainer.register_hook_in_train(         new_hook=hook_on_data_injection_sav_data,         trigger='on_fit_end',         insert_mode=-1)      return base_trainer   def hood_on_fit_start_generator(ctx):   # update the round number     ctx.gan_cra.round_num += 1     print('----- Round {}: GAN training ............'.format(         ctx.gan_cra.round_num))   def hook_on_batch_forward_injected_data(ctx):     # inject the generated data into training batch loss     x, label = [_.to(ctx.device) for _ in ctx.injected_data]     pred = ctx.model(x)     if len(label.size()) == 0:         label = label.unsqueeze(0)     ctx.loss_task += ctx.criterion(pred, label)     ctx.y_true_injected = label     ctx.y_prob_injected = pred   def hook_on_batch_injected_data_generation(ctx):     # generate the injected data     ctx.injected_data = ctx.gan_cra.generate_fake_data()   def hook_on_gan_cra_train(ctx):    # update the GAN's discriminator with the broadcasted parameter     ctx.gan_cra.update_discriminator(ctx.model)    # train the GAN's generator     ctx.gan_cra.generator_train()   def hook_on_data_injection_sav_data(ctx):     ctx.gan_cra.generate_and_save_images()   After defining the trainer wrapping function, the following code adds the wrap_GANTrainer into wrap_attacker_trainer in federatedscope/attack/auxiliary/attack_trainer_builder.py.   def wrap_attacker_trainer(base_trainer, config):     if config.attack.attack_method.lower() == 'gan_attack':         from federatedscope.attack.trainer.GAN_trainer import wrap_GANTrainer         return wrap_GANTrainer(base_trainer)     elif config.attack.attack_method.lower() == 'gradascent':         from federatedscope.attack.trainer.MIA_invert_gradient_trainer import wrap_GradientAscentTrainer         return wrap_GradientAscentTrainer(base_trainer)      else:         raise ValueError('Trainer {} is not provided'.format(             config.attack.attack_method))   4. Contribute Your Attacker to FederatedScope  Users are welcome to contribute their own attack methods to FederatedScope. Please refer Contributing to FederatedScope for more details.    Reference  [1]  Zhao B, Mopuri K R, Bilen H. idlg: Improved deep leakage from gradients[J]. arXiv preprint arXiv:2001.02610, 2020.   [2]  Hitaj, Briland, Giuseppe Ateniese, and Fernando Perez-Cruz. “Deep models under the GAN: information leakage from collaborative deep learning.” Proceedings of the 2017 ACM SIGSAC conference on computer and communications security. 2017.   [3] Zhu, Ligeng, Zhijian Liu, and Song Han. “Deep leakage from gradients.” Advances in Neural Information Processing Systems 32 (2019).   [4] Geiping, Jonas, et al. “Inverting gradients-how easy is it to break privacy in federated learning?.” Advances in Neural Information Processing Systems 33 (2020): 16937-16947.   [5] Nasr, Milad, R. Shokri and Amir Houmansadr. “Comprehensive Privacy Analysis of Deep Learning: Stand-alone and Federated Learning under Passive and Active White-box Inference Attacks.” ArXiv abs/1812.00910 (2018): n. pag.   [6] Melis, Luca, Congzheng Song, Emiliano De Cristofaro and Vitaly Shmatikov. “Exploiting Unintended Feature Leakage in Collaborative Learning.” 2019 IEEE Symposium on Security and Privacy (SP) (2019): 691-706.   [7] Lyu, Lingjuan, Han Yu and Qiang Yang. “Threats to Federated Learning: A Survey.” ArXiv abs/2003.02133 (2020): n. pag.  ","categories": [],
        "tags": [],
        "url": "/docs/privacy-attacks/",
        "teaser": null
      },{
        "title": "Graph",
        "excerpt":"Background   For privacy reasons, there are many graphs in scenarios that are split into different subgraphs in different clients, which leads to missing of the cross-client edges and data non.i.i.d., etc.   Not only in areas such as CV and NLP, but FederatedScope also provides support for graph learning researchers with a rich collection of datasets, the latest federated graph algorithms and benchmarks.   In this tutorial, you will learn:      How to start graph learning with FederatedScope [click]   How to reproduce the main experimental results in EasyFGL paper [click]   How to use build-in or create a new federated graph dataset [click]   How to run with built-in or new models [click]   How to develop new federated graph algorithms [click]   How to enable FedOptimizer, PersonalizedFL and FedHPO [click]   Benchmarkcketing Federated GNN [click]   Quick start   Let’s start with a two-layer GCN on (fed) Cora to familiarize you with FederatedScope.   Start with built-in functions   You can easily run through a yaml file:   # Whether to use GPU use_gpu: True  # Deciding which GPU to use device: 0  # Federate learning related options federate:   # `standalone` or `distributed`   mode: standalone   # Evaluate in Server or Client test set   make_global_eval: True   # Number of dataset being split   client_num: 5   # Number of communication round   total_round_num: 400  # Dataset related options data:   # Root directory where the data stored   root: data/   # Dataset name   type: cora   # Use Louvain algorithm to split `Cora`   splitter: 'louvain' dataloader:   # Type of sampler   type: pyg   # Use fullbatch training, batch_size should be `1`   batch_size: 1  # Model related options model:   # Model type   type: gcn   # Hidden dim   hidden: 64   # Dropout rate   dropout: 0.5   # Number of Class of `Cora`   out_channels: 7  # Criterion related options criterion:   # Criterion type   type: CrossEntropyLoss  # Trainer related options trainer:   # Trainer type   type: nodefullbatch_trainer  # Train related options train:   # Number of local update steps   local_update_steps: 4   # Optimizer related options   optimizer:     # Learning rate     lr: 0.25     # Weight decay     weight_decay: 0.0005     # Optimizer type     type: SGD  # Evaluation related options eval:   # Frequency of evaluation   freq: 1   # Evaluation metrics, accuracy and number of correct items   metrics: ['acc', 'correct']   If the yaml file is named as example.yaml, just run:   python federatedscope/main.py --cfg example.yaml   Then, the FedAVG performance is around 0.87.   Start with customized functions   FederatedScope also provides register function to set up the FL procedure. Here we only provide an example about two-layer GCN on (fed) Cora, please refer to Start with your own case for details.      Load Cora dataset and split into 5 subgraph   # federatedscope/contrib/data/my_cora.py  import copy import numpy as np  from torch_geometric.datasets import Planetoid from federatedscope.core.splitters.graph import LouvainSplitter from federatedscope.register import register_data from federatedscope.core.data import DummyDataTranslator   def my_cora(config=None):     path = config.data.root      num_split = [232, 542, np.iinfo(np.int64).max]     dataset = Planetoid(path,                         'cora',                         split='random',                         num_train_per_class=num_split[0],                         num_val=num_split[1],                         num_test=num_split[2])     global_data = copy.deepcopy(dataset)[0]     dataset = LouvainSplitter(config.federate.client_num)(dataset[0])      data_local_dict = dict()     for client_idx in range(len(dataset)):         local_data = dataset[client_idx]         data_local_dict[client_idx + 1] = {             'data': local_data,             'train': [local_data],             'val': [local_data],             'test': [local_data]         }      data_local_dict[0] = {         'data': global_data,         'train': [global_data],         'val': [global_data],         'test': [global_data]     }      translator = DummyDataTranslator(config)      return translator(data_local_dict), config   def call_my_data(config, client_cfgs):     if config.data.type == \"mycora\":         data, modified_config = my_cora(config)         return data, modified_config   register_data(\"mycora\", call_my_data)      Build a two-layer GCN   # federatedscope/contrib/model/my_gcn.py  import torch import torch.nn.functional as F  from torch.nn import ModuleList from torch_geometric.data import Data from torch_geometric.nn import GCNConv from federatedscope.register import register_model   class MyGCN(torch.nn.Module):     def __init__(self,                  in_channels,                  out_channels,                  hidden=64,                  max_depth=2,                  dropout=.0):         super(MyGCN, self).__init__()         self.convs = ModuleList()         for i in range(max_depth):             if i == 0:                 self.convs.append(GCNConv(in_channels, hidden))             elif (i + 1) == max_depth:                 self.convs.append(GCNConv(hidden, out_channels))             else:                 self.convs.append(GCNConv(hidden, hidden))         self.dropout = dropout      def forward(self, data):         if isinstance(data, Data):             x, edge_index = data.x, data.edge_index         elif isinstance(data, tuple):             x, edge_index = data         else:             raise TypeError('Unsupported data type!')          for i, conv in enumerate(self.convs):             x = conv(x, edge_index)             if (i + 1) == len(self.convs):                 break             x = F.relu(F.dropout(x, p=self.dropout, training=self.training))         return x   def gcnbuilder(model_config, input_shape):     x_shape, num_label, num_edge_features = input_shape     model = MyGCN(x_shape[-1],                   model_config.out_channels,                   hidden=model_config.hidden,                   max_depth=model_config.layer,                   dropout=model_config.dropout)     return model   def call_my_net(model_config, local_data):     # Please name your gnn model with prefix 'gnn_'     if model_config.type == \"gnn_mygcn\":         model = gcnbuilder(model_config, local_data)         return model   register_model(\"gnn_mygcn\", call_my_net)      Run with following command to start:   python federatedscope/main.py --cfg example.yaml data.type mycora model.type mygcn   Reproduce the main experimental results    We also provide configuration files to help you easily reproduce the results in our EasyFGL paper. All the yaml files are in federatedscope/gfl/baseline.      Train two-layer GCN with Node-level task dataset Cora   python federatedscope/main.py --cfg federatedscope/gfl/baseline/fedavg_gnn_node_fullbatch_citation.yaml   Then, the FedAVG performance is around 0.87.      Train two-layer GCN with Link-level task dataset WN18   python federatedscope/main.py --cfg federatedscope/gfl/baseline/fedavg_gcn_minibatch_on_kg.yaml   Then, the FedAVG performance is around hits@1: 0.30, hits@5: 0.79, hits@10: 0.96.      Train two-layer GCN with Graph-level task dataset HIV   python federatedscope/main.py --cfg federatedscope/gfl/baseline/fedavg_gcn_minibatch_on_hiv.yaml   Then, the FedAVG performance is around accuracy: 0.96 and roc_aucs: 0.62.     DataZoo   FederatedScope provides a rich collection of datasets for graph learning researchers, including real federation datasets as well as simulated federation datasets split by some sampling or clustering algorithms. The dataset statistics are shown in the table and more datasets are coming soon:            Task     Domain     Dataset     Splitter     # Graph     Avg. # Nodes     Avg. # Edges     # Class     Evaluation             Node-level     Citation network     Cora [1]     random&amp;community     1     2,708     5,429     7     ACC           Citation network     CiteSeer [2]     random&amp;community     1     4,230     5,358     6     ACC           Citation network     PubMed [3]     random&amp;community     1     19,717     44,338     3     ACC           Citation network     FedDBLP [4]     meta     1     52,202     271,054     4     ACC           Link-level     Recommendation System     Ciao [5]     meta     28     5,875.68     20,189.29     6     ACC           Recommendation System     Taobao     meta     3     443,365     2,015,558     2     ACC           Knowledge Graph     WN18 [6]     label_space     1     40,943     151,442     18     Hits@n           Knowledge Graph     FB15k-237 [6]     label_space     1     14,541     310,116     237     Hits@n           Graph-level     Molecule     HIV [7]     instance_space     41,127     25.51     54.93     2     ROC-AUC           Proteins     Proteins [8]     instance_space     1,113     39.05     145.63     2     ACC           Social network     IMDB [8]     label_space     1,000     19.77     193.06     2     ACC           Multi-task     Mol [8]     multi_task     18,661     55.62     1,466.83     -     ACC          Dataset format   Let’s start Dataset with torch_geometric.data. Our DataZoo contains three levels of tasks which are node-level, link-level and graph-level. Different levels of data have different attributes:      Node-level dataset Node-level dataset contains one torch_geometric.data with attributes: x represents the node attribute, y represents the node label, edge_index represents the edges of the graph, edge_attr  represents the edge attribute which is optional, and train_mask, val_mask,test_mask are the node mask of each splits.   # Cora Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])      Link-level dataset Link-level dataset contains one torch_geometric.data with attributes: x represents the node attribute, edge_index represents the edges of the graph, edge_type represents the link label, edge_attr  represents the edge attribute which is optional, train_edge_mask, valid_edge_mask,test_edge_mask are the link mask of each splits, and input_edge_index is optional if the input is edge_index.T[train_edge_mask].T.   # WN18 Data(x=[40943, 1], edge_index=[2, 151442], edge_type=[151442], num_nodes=40943, train_edge_mask=[151442], valid_edge_mask=[151442], test_edge_mask=[151442], input_edge_index=[2, 282884])      Graph-level dataset Graph-level dataset contains several torch_geometric.data, and the task is to predict the label of each graph.   # HIV[0] Data(x=[19, 9], edge_index=[2, 40], edge_attr=[40, 3], y=[1, 1], smiles='CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)=[O+]2') ... # HIV[41126] Data(x=[37, 9], edge_index=[2, 80], edge_attr=[80, 3], y=[1, 1], smiles='CCCCCC=C(c1cc(Cl)c(OC)c(-c2nc(C)no2)c1)c1cc(Cl)c(OC)c(-c2nc(C)no2)c1')   Dataloader   For node-level and link-level tasks, we use full-batch training by default. However, some large graphs can not be adopted to full-batch training due to the video memory limitation. Fortunately, we also provide some graph sampling algorithms, like GraphSAGE and GraphSAINT which are subclasses of torch_geometric.loader.      In node-level task, you should set:   cfg.data.loader = 'graphsaint-rw' # or `neighbor` cfg.model.type = 'sage' cfg.trainer.type = 'nodeminibatch_trainer'      In link-level task, you should set:   cfg.data.loader = 'graphsaint-rw' cfg.model.type = 'sage' cfg.trainer.type = 'linkminibatch_trainer'   Splitter   Existing graph datasets are a valuable source to meet the need for more FL datasets. Under the federated learning setting, the dataset is decentralized. To simulate federated graph datasets by existing standalone ones, our DataZoo integrates a rich collection of federatedscope.gfl.dataset.splitter.  Except for meta_splitter which comes from the meta information of datasets, we have the following splitters:      Node-level task            community_splitter: Split by cluster cfg.data.splitter = 'louvain' Community detection algorithms such as Louvain are at first applied to partition a graph into several clusters. Then these clusters are assigned to the clients, optionally with the objective of balancing the number of nodes in each client.       random_splitter: Split by random cfg.data.splitter = 'random' The node set of the original graph is randomly split into 𝑁 subsets with or without intersections. Then, the subgraph of each client is deduced from the nodes assigned to that client. Optionally, a specified fraction of edges is randomly selected to be removed.           Link-level task            label_space_splitter: Split by latent dirichlet allocation cfg.data.splitter = 'rel_type' It is designed to provide label distribution skew via latent dirichlet allocation (LDA).           Graph-level task            instance_space_splitter: Split by index cfg.data.splitter = 'scaffold' or 'rand_chunk' It is responsible for creating feature distribution skew (i.e., covariate shift). To realize this, we sort the graphs based on their values of a certain aspect.       multi_task_splitter: Split by dataset cfg.data.splitter = 'louvain' Different clients have different tasks.           ModelZoo   GNN   We implemented GCN [9], GraphSAGE [10], GAT [11], GIN [12], and GPR-GNN [13] on different levels of tasks in federatedscope.gfl.model, respectively. In order to run your FL procedure with these models, set cfg.model.task to node, link or graph, and all models can be instantiated automatically based on the data provided. More GNN models are coming soon!   Trainer   We provide several Trainers for different models and for different tasks.      NodeFullBatchTrainer            For node-level tasks.       For full batch training.           NodeMiniBatchTrainer            For node-level tasks.       For GraphSAGE, GraphSAINT and other graph sampling methods.           LinkFullBatchTrainer            For link-level tasks.       For full batch training.           LinkMiniBatchTrainer            For link-level tasks.       For GraphSAGE, GraphSAINT and other graph sampling methods.           GraphMiniBatchTrainer            For graph-level tasks.           Develop federated GNN algorithms   FederatedScope provides comprehensive support to help you develop federated GNN algorithms. Here we will go through FedSage+ [14] and GCFL+ [15] as examples.      FedSage+, Subgraph Federated Learning with Missing Neighbor Generation, in NeurIPS 2021 FedSage+ try to “restore” the missing graph structure by jointly training a Missing Neighbor Generator, each client sends Missing Neighbor Generator to other clients, and the other clients optimize it with their own local data and send the model gradient back in order to achieve joint training without privacy leakage. We implemented FedSage+ in federatedscope/gfl/fedsageplus with FedSagePlusServer and FedSagePlusClient. In FederatedScope, we need to define new message types and the corresponding handler functions.   # FedSagePlusServer self.register_handlers('clf_para', self.callback_funcs_model_para) self.register_handlers('gen_para', self.callback_funcs_model_para) self.register_handlers('gradient', self.callback_funcs_gradient)   Because FedSage+ has multiple stages, please carefully deal with the msg_buffer in check_and_move_on() in different states.      GCFL+, Federated Graph Classification over Non-IID Graphs, NeurIPS 2021 GCFL+ clusters clients according to the sequence of the gradients of each local model, and those with a similar sequence of the gradients share the same model parameters. We implemented GCFL+ in federatedscope/gfl/gcflplus with FedSagePlusServer and FedSagePlusClient. Since no more messages are involved, we can implement GCFL+ by simply defining how to clustering clients and adding gradients to message model_para.   Enable build-in Federated Algorithms   FederatedScope provides many built-in FedOptimize, PersonalizedFL and FedHPO algorithms. You can adapt them to graph learning by simply turning on the switch.   For more details, see:      FedOptimize   PersonalizedFL   FedHPO   Benchmarks   We’ve conducted extensive experiments to build the benchmarks of FedGraph, which simultaneously gains many valuable insights for the community.   Node-level task           Results on representative node classification datasets with random_splitter Mean accuracy (%) ± standard deviation.                     Cora     CiteSeer     PubMed                  Local     FedAVG     FedOpt     FedPeox     Global     Local     FedAVG     FedOpt     FedPeox     Global     Local     FedAVG     FedOpt     FedPeox     Global           GCN     80.95±1.49     86.63±1.35     86.11±1.29     86.60±1.59     86.89±1.82     74.29±1.35     76.48±1.52     77.43±0.90     77.29±1.20     77.42±1.15     85.25±0.73     85.29±0.95     84.39±1.53     85.21±1.17     85.38±0.33           GraphSAGE     75.12±1.54     85.42±1.80     84.73±1.58     84.83±1.66     86.86±2.15     73.30±1.30     76.86±1.38     75.99±1.96     78.05±0.81     77.48±1.27     84.58±0.41     86.45±0.43     85.67±0.45     86.51±0.37     86.23±0.58           GAT     78.86±2.25     85.35±2.29     84.40±2.70     84.50±2.74     85.78±2.43     73.85±1.00     76.37±1.11     76.96±1.75     77.15±1.54     76.91±1.02     83.81±0.69     84.66±0.74     83.78±1.11     83.79±0.87     84.89±0.34           GPR-GNN     84.90±1.13     89.00±0.66     87.62±1.20     88.44±0.75     88.54±1.58     74.81±1.43     79.67±1.41     77.99±1.25     79.35±1.11     79.67±1.42     86.85±0.39     85.88±1.24     84.57±0.68     86.92±1.25     85.15±0.76           FedSage+     -     85.07±1.23     -     -     -     -     78.04±0.91     -     -     -     -     88.19±0.32     -     -     -                Results on representative node classification datasets with community_splitter: Mean accuracy (%) ± standard deviation.                     Cora     CiteSeer     PubMed                  Local     FedAVG     FedOpt     FedProx     Global     Local     FedAVG     FedOpt     FedProx     Global     Local     FedAVG     FedOpt     FedProx     Global           GCN     65.08±2.39     87.32±1.49     87.29±1.65     87±16±1.51     86.89±1.82     67.53±1.87     77.56±1.45     77.80±0.99     77.62±1.42     77.42±1.15     77.01±3.37     85.24±0.69     84.11±0.87     85.14, 0.88     85.38±0.33           GraphSAGE     61.29±3.05     87.19±1.28     87.13±1.47     87.09±1.46     86.86±2.15     66.17±1.50     77.80±1.03     78.54±1.05     77.70±1.09     77.48±1.27     78.35±2.15     86.87±0.53     85.72±0.58     86.65±0.60     86.23±0.58           GAT     61.53±2.81     86.08±2.52     85.65±2.36     85.68±2.68     85.78±2.43     66.17±1.31     77.21±0.97     77.34±1.33     77.26±1.02     76.91±1.02     75.97±3.32     84.38±0.82     83.34±0.87     84.34±0.63     84.89±0.34           GPR-GNN     69.32±2.07     88.93±1.64     88.37±2.12     88.80±1.29     88.54±1.58     71.30±1.65     80.27±1.28     78.32±1.45     79.73±1.52     79.67±1.42     78.52±3.61     85.06±0.82     84.30±1.57     86.77±1.16     85.15±0.76           FedSage+     -     87.68±1.55     -     -     -     -     77.98±1.23     -     -     -     -     87.94, 0.27     -     -     -           Link-level task           Results on representative link prediction datasets with label_space_splitter: Hits@$n$.                     WN18     FB15k-237                  Local     FedAVG     FedOpt     FedProx     Global     Local     FedAVG     FedOpt     FedProx     Global                1     5     10     1     5     10     1     5     10     1     5     10     1     5     10     1     5     10     1     5     10     1     5     10     1     5     10     1     5     10           GCN     20.70     55.34     73.85     30.00     79.72     96.67     22.13     78.96     94.07     27.32     83.01     96.38     29.67     86.73     97.05     6.07     20.29     30.35     9.86     34.27     48.02     4.12     18.07     31.79     4.66     28.74     41.67     7.80     32.46     44.64           GraphSAGE     21.06     54.12     79.88     23.14     78.85     93.70     22.82     79.86     93.12     23.14     78.52     93.67     24.24     79.86     93.84     3.95     14.64     24.47     7.13     23.38     36.60     2.20     19.21     27.64     5.85     24.05     36.33     6.19     23.57     35.98           GAT     20.89     49.42     72.48     23.14     77.62     93.49     23.14     74.64     93.52     23.53     78.40     93.00     24.24     80.18     93.76     3.44     15.02     25.14     6.06     25.76     39.04     2.71     18.89     32.76     6.19     25.09     38.00     6.94     24.43     37.87           GPR-GNN     22.86     60.45     80.73     26.67     82.35     96.18     24.46     73.33     87.18     27.62     81.87     95.68     29.19     82.34     96.24     4.45     13.26     21.24     9.62     32.76     45.97     2.01     9.81     16.65     3.72     15.62     27.79     10.62     33.87     47.45           Graph-level task           Results on representative graph classification datasets: Mean accuracy (%) ± standard deviation.                     PROTEINS     IMDB     Multi-task                  Local     FedAVG     FedOpt     FedProx     Global     Local     FedAVG     FedOpt     FedProx     Global     Local     FedAVG     FedOpt     FedProx     Global           GCN     71.10±4.65     73.54±4.48     71.24±4.17     73.36±4.49     71.77±3.62     50.76±1.14     53.24±6.04     50.49±8.32     48.72±6.73     53.24±6.04     66.37±1.78     65.99±1.18     69.10±1.58     68.59±1.99     -           GIN     69.06±3.47     73.74±5.71     60.14±1.22     73.18±5.66     72.47±5.53     55.82±7.56     64.79±10.55     51.87±6.82     70.65±8.35     72.61±2.44     75.05±1.81     63.40±2.22     63.33±1.18     63.01±0.44     -           GAT     70.75±3.33     71.95±4.45     71.07±3.45     72.13±4.68     72.48±4.32     53.12±5.81     53.24±6.04     47.94±6.53     53.82±5.69     53.24±6.04     67.72±3.48     66.75±2.97     69.58±1.21     69.65±1.14     -           GCFL+     -     73.00±5.72     -     74.24±3.96     -     -     69.47±8.71     -     68.90±6.30     -     -     65.14±1.23     -     65.69±1.55     -                Results with PersonalizedFL on representative graph classification datasets: Mean accuracy (%) ± standard deviation. (GIN)                     Multi-task             FedBN [16]     72.90±1.33           ditto [17]     63.35±0.69           References   [1] McCallum, Andrew Kachites, et al. “Automating the construction of internet portals with machine learning.” Information Retrieval 2000   [2] Giles, C. Lee, Kurt D. Bollacker, and Steve Lawrence. “CiteSeer: An automatic citation indexing system.” Proceedings of the third ACM conference on Digital libraries. 1998.   [3] Sen, Prithviraj, et al. “Collective classification in network data.” AI magazine 2008.   [4] Tang, Jie, et al. “Arnetminer: extraction and mining of academic social networks.” SIGKDD 2008.   [5] Tang, Jiliang, Huiji Gao, and Huan Liu. “mTrust: Discerning multi-faceted trust in a connected world.” WSDM 2012.   [6] Bordes, Antoine, et al. “Translating embeddings for modeling multi-relational data.” NeurIPS 2013.   [7] Wu, Zhenqin, et al. “MoleculeNet: a benchmark for molecular machine learning.” Chemical science 2018.   [8] Ivanov, Sergei, Sergei Sviridov, and Evgeny Burnaev. “Understanding isomorphism bias in graph data sets.” arXiv 2019.   [9] Kipf, Thomas N., and Max Welling. “Semi-supervised classification with graph convolutional networks.” arXiv 2016.   [10] Veličković, Petar, et al. “Graph attention networks.” ICLR 2018.   [11] Hamilton, Will, Zhitao Ying, and Jure Leskovec. “Inductive representation learning on large graphs.” NeurIPS 2017.   [12] Xu, Keyulu, et al. “How powerful are graph neural networks?.”  ICLR 2019.   [13] Chien, Eli, et al. “Adaptive universal generalized pagerank graph neural network.” ICLR 2021.   [14] Zhang, Ke, et al. “Subgraph federated learning with missing neighbor generation.” NeurIPS 2021.   [15] Xie, Han, et al. “Federated graph classification over non-iid graphs.” NeurIPS 2021.   [16] Li, Xiaoxiao, et al. “Fedbn: Federated learning on non-iid features via local batch normalization.” ICLR 2021.   [17]Li, Tian, et al. “Ditto: Fair and robust federated learning through personalization.” PMLR 2021.  ","categories": [],
        "tags": [],
        "url": "/docs/graph/",
        "teaser": null
      },{
        "title": "NLP and Speech",
        "excerpt":"  Background   Recently, privacy-preserving methods gain increasing attentions in machine learning (ML) applications using linguistic data  including text and audio, due to the fact that linguistic data can involve a wealth of information relating to an identified or identifiable natural person, such as the physiological, psychological, economic, cultural or social identity.   Federated Learning (FL) methods show promising results for collaboratively training models from a large number of clients without sharing their private linguistic data. To facilitate FL research in linguistic data, FederatedScope provides several built-in linguistic datasets and supports various tasks such as language modeling and text classification with various FL algorithms.     Natural Language Processing (NLP)     Datasets   We provide three popular text datasets for next-character prediction, next-word prediction, and sentiment analysis.      Shakespeare: a federation text dataset of Shakespeare Dialogues from LEAF [1] for next-character prediction, which contains 422,615 sentences and about 1,100 clients.   subReddit: a federation text dataset and subsampled of reddit from LEAF for next-word prediction, which contains 216,858 sentences and about 800 clients.   Sentiment140: a federation text dataset of Twitter from LEAF for Sentiment Analysis, which contains 1,600,498 sentences, about 660,000  clients.     Models   We provide a LSTM model implementation in federatedscope/nlp/model      LSTM: a type of RNN that solves the vanishing gradient problem through additional cells, input and output gates. (cfg.model.type = 'lstm')   class LSTM(nn.Module):     def __init__(self,                  in_channels,                  hidden,                  out_channels,                  n_layers=2,                  embed_size=8):         pass      Currently, we are working on implement more interfaces to support more popular NLP Transformer models and more NLP tasks with HuggingFace Transformers [2].     Start an example   Next-character/word prediction is a classic NLP task as it can be applied in many consumer applications and appropriately be modeled by statistical language models, we show how to achieve next-character prediction in cross-device FL setting.      Here we implement a simple LSTM model for next-character prediction: taking an English characters sequence as input, the model learns to predict the next possible character. After registering the model, we can use it by specifying cfg.model.type=lstm and  hyper-parameters such as  cfg.model.in_channels=80, cfg.model.out_channels=80, cfg.model.emd_size=8.  Complete codes are in federatedscope/nlp/model/rnn.py and federatedscope/nlp/model/model_builder.py.   class LSTM(nn.Module):     def __init__(self,                  in_channels,                  hidden,                  out_channels,                  n_layers=2,                  embed_size=8):         super(LSTM, self).__init__()         self.in_channels = in_channels         self.hidden = hidden         self.embed_size = embed_size         self.out_channels = out_channels         self.n_layers = n_layers          self.encoder = nn.Embedding(in_channels, embed_size)          self.rnn =\\             nn.LSTM(                 input_size=embed_size,                 hidden_size=hidden,                 num_layers=n_layers,                 batch_first=True             )          self.decoder = nn.Linear(hidden, out_channels)      def forward(self, input_):         encoded = self.encoder(input_)         output, _ = self.rnn(encoded)         output = self.decoder(output)         output = output.permute(0, 2, 1)  # change dimension to (B, C, T)         final_word = output[:, :, -1]         return final_word      For the dataset, we use the Shakespeare dataset from LEAF, which is built from The Complete Works of William Shakespeare,  and partitioned to ~1100 clients (speaking roles) from 422615.  We can specify the cfg.dataset.type=shakespeare and adjust the fraction of data subsample (cfg.data.sub_sample=0.2), and train/val/test ratio (``cfg.data.splits=[0.6,0.2,0.2]). Complete NLP data codes are infederatedscope/nlp/dataset.   class LEAF_NLP(LEAF):     \"\"\"     LEAF NLP dataset from          leaf.cmu.edu          self:         root (str): root path.         name (str): name of dataset, ‘shakespeare’ or ‘xxx’.         s_frac (float): fraction of the dataset to be used; default=0.3.         tr_frac (float): train set proportion for each task; default=0.8.         val_frac (float): valid set proportion for each task; default=0.0.     \"\"\"     def __init__(             self,             root,             name,             s_frac=0.3,              tr_frac=0.8,             val_frac=0.0,             seed=123,             transform=None,             target_transform=None):         pass      To enable large-scale clients simulation, we provide online aggregator in standalone mode to save the memory, which  maintains only three model objects for the FL server aggregation. We can use this feature by specifying cfg.federate.online_aggr = True and federate.share_local_model=True , more details about this feature can be found in the post “Simulation and Deployment”.   To handle the non-i.i.d. challenge, FederatedScope supports several SOTA personalization algorithms and easy extension.   To enable partial clients participation in each FL round, we provide clients sampling feature with various configuration manners: 1) cfg.federate.sample_client_rate, which is in the range (0, 1] and indicates selecting partial clients using random sampling with replacement; 2) cfg.federate.sample_client_num , which is an integer to indicate sample client number at each round.   With these specification, we can run the experiment with    main.py --cfg federatedscope/nlp/baseline/fedavg_lstm_on_shakespeare.yaml   You will get the accuracy of FedAvg algorithm around 43.80%.   Other NLP related scripts to run the next-character prediction experiments can be found in federatedscope/nlp/baseline.     Customize your NLP task   FederatedScope enables users to easily implement and register more NLP datasets and models.      Implement and register your own NLP data ```python     federatedscope/contrib/data/my_nlp_data.py       import torch import copy import numpy as np   from federatedscope.register import register_data   def get_my_nlp_data(config):     r”””         This function returns a dictionary, where key is the client id and      value is the data dict of each client with ‘train’, ‘test’ or ‘val’.     \t\tNOTE: client_id 0 is SERVER!   Returns:       dict: {                 'client_id': {                     'train': DataLoader or Data,                     'test': DataLoader or Data,                     'val': DataLoader or Data,                 }             } \"\"\" import numpy as np from torch.utils.data import DataLoader  # Build data dataset = LEAF_NLP(root=path,                    name=\"twitter\",                    s_frac=config.data.subsample,                    tr_frac=splits[0],                    val_frac=splits[1],                    seed=1234,                    transform=transform)  client_num = min(len(dataset), config.federate.client_num                  ) if config.federate.client_num &gt; 0 else len(dataset) config.merge_from_list(['federate.client_num', client_num])  # get local dataset data_local_dict = dict() for client_idx in range(client_num):     dataloader = {         'train': DataLoader(dataset[client_idx]['train'],                             batch_size,                             shuffle=config.data.shuffle,                             num_workers=config.data.num_workers),         'test': DataLoader(dataset[client_idx]['test'],                            batch_size,                            shuffle=False,                            num_workers=config.data.num_workers)     }     if 'val' in dataset[client_idx]:         dataloader['val'] = DataLoader(dataset[client_idx]['val'],                                        batch_size,                                        shuffle=False,                                        num_workers=config.data.num_workers)      data_local_dict[client_idx + 1] = dataloader  return data_local_dict, confi   def call_my_data(config):     if config.data.type == “my_nlp_data”:         data, modified_config = get_my_nlp_data(config)         return data, modified_config   register_data(“my_nlp_data”, call_my_data)     -  Implement and register your own NLP model  ```python import torch from federatedscope.register import register_model   class  KIM_CNN(nn.Module):   \"\"\"   \t\tref to Kim's CNN text classification paper [3]   \t\thttps://github.com/Shawn1993/cnn-text-classification-pytorch   \"\"\"     def __init__(self, args):         super(CNN_Text, self).__init__()         self.args = args                  V = args.embed_num         D = args.embed_dim         C = args.class_num         Ci = 1         Co = args.kernel_num         Ks = args.kernel_sizes          self.embed = nn.Embedding(V, D)         self.convs = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])         self.dropout = nn.Dropout(args.dropout)         self.fc1 = nn.Linear(len(Ks) * Co, C)          if self.args.static:             self.embed.weight.requires_grad = False      def forward(self, x):         x = self.embed(x)  # (N, W, D)              x = x.unsqueeze(1)  # (N, Ci, W, D)          x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(N, Co, W), ...]*len(Ks)          x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co), ...]*len(Ks)          x = torch.cat(x, 1)          x = self.dropout(x)  # (N, len(Ks)*Co)         logit = self.fc1(x)  # (N, C)         return logit        def call_my_net(model_config, local_data):     if model_config.type == \"my_nlp_model\":         model = KIM_CNN(args=model_config)         return model  register_model(\"my_nlp_model\", call_my_net)      Then with fruitful  built-in FL experiments scripts , users can run own FL experiments by replacing the model type and dataset type in the provided  scripts.     Speech (Coming soon)   We are working on implement more interfaces to support more Conformer [4] models and more speech-related tasks with WeNet [5], which is designed for various end-2-end speech recognition tasks and provides full stack solutions for production and real-world applications.     Reference   [1] Caldas, Sebastian, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečný, H. Brendan McMahan, Virginia Smith, and Ameet Talwalkar. “Leaf: A benchmark for federated settings.” arXiv preprint arXiv:1812.01097 (2018).   [2] Wolf, Thomas, et al. “Huggingface’s transformers: State-of-the-art natural language processing.” arXiv preprint arXiv:1910.03771 (2019).   [3] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, Doha, Qatar.   [4] Gulati, Anmol, et al. “Conformer: Convolution-augmented transformer for speech recognition.” arXiv preprint arXiv:2005.08100 (2020).   [5] Zhang, Binbin, et al. “Wenet: Production first and production ready end-to-end speech recognition toolkit.” arXiv e-prints (2021): arXiv-2102.  ","categories": [],
        "tags": [],
        "url": "/docs/nlp-and-speech/",
        "teaser": null
      },{
        "title": "Join Our Community",
        "excerpt":"Users are encouraged to join our community to have more discussions about FederatedScope as well as the research and applications of federated learning.   GitHub  As an open-source software, the main channel to ask questions about FederatedScope is to create GitHub issues in our repository. Besides, we have provided a rich collection of federated learning-related materials at here, including paper lists for various research directions.   Slack  You can join our Slack channel via this link   DingDing  Please scan the following QR code to join our DingDing group.     ","categories": [],
        "tags": [],
        "url": "/docs/community/",
        "teaser": null
      },{
        "title": "Workers",
        "excerpt":"Worker class encapsulates the behaviors of FL participants (i.e., server and clients) and can be described via a collection of (message, handler) pairs. This tutorial focuses on helping you develop your own Worker subclasses. More details about the event-driven programming paradigm can be found in Event-driven Architecture.   Base Class of Workers   We show the class hierarchy for FS workers as below, and you can develop your own workers by inheriting the appropriate base class and making specialization.   Worker # Abstract class for both client and server ├── BaseClient # Abstract class for client │   ├── Client # An implemented client with BaseClient │   │   ├── XXXClient # Client for a specific algorithm with minor modifications │   │   ├── ... ... ├── BaseServer # Abstract class for server │   ├── Server # An implemented server with BaseServer │   │   ├── XXXServer # Server for a specific algorithm with minor modifications │   │   ├── ... ...   You can either develop your client (or server) from federatedscope.core.workers.base_client.BaseClient  (or federatedscope.core.workers.base_server.BaseServer) for a brand new client or  from federatedscope.core.workers.client.Client (or federatedscope.core.workers.server.Server) for minor modifications.   We have two mechanisms below to help you keep your Worker subclasses complete, which are all essential for the usage of Event-driven Architecture.   Completeness of implementation   In this section, we give a hands-on tutorial for developing a new client, which can be adapted to developing a new server. If you want to develop your own client with BaseClient as the base class, you must implement your own callback_funcs_for_xxx, which are abc.abstractmethod in BaseClient as shown below:   class BaseClient(Worker):     def __init__(self, ID, state, config, model, strategy):         super(BaseClient, self).__init__(ID, state, config, model, strategy)         self.msg_handlers = dict()         self.msg_handlers_str = dict()     def register_handlers(self, msg_type, callback_func, send_msg=[None]):         if msg_type in self.msg_handlers.keys():             logger.warning(f\"Overwriting msg_handlers {msg_type}.\")         self.msg_handlers[msg_type] = callback_func         self.msg_handlers_str[msg_type] = (callback_func.__name__, send_msg)      def _register_default_handlers(self):     \t\t...      @abc.abstractmethod     def run(self):         raise NotImplementedError      @abc.abstractmethod     def callback_funcs_for_model_para(self, message):         raise NotImplementedError      @abc.abstractmethod     def callback_funcs_for_assign_id(self, message):         raise NotImplementedError      @abc.abstractmethod     def callback_funcs_for_join_in_info(self, message):         raise NotImplementedError      @abc.abstractmethod     def callback_funcs_for_address(self, message):         raise NotImplementedError      @abc.abstractmethod     def callback_funcs_for_evaluate(self, message):         raise NotImplementedError      @abc.abstractmethod     def callback_funcs_for_finish(self, message):         raise NotImplementedError      @abc.abstractmethod     def callback_funcs_for_converged(self, message):         raise NotImplementedError   These messages and handlers are necessary for making up a complete FL course. With them properly implemented, you can run FS with your own client!   Completeness of messages and handlers   In this section, we will introduce the mechanism that would somewhat help you ensure the completeness of messages and handlers. This mechanism is to check the situation below:      If the messages and handlers are not set properly, the FL course might fail due to missing handlers for some specific message, hander never being used, etc. Thus, this mechanism will help you debug.   How to enable this check   Following our config rules, you can enable this feature by setting cfg.check_completeness=True, or running in the command line with check_completeness True:   python federatedscope/main.py --cfg scripts/example_configs/femnist.yaml check_completeness True   Status for the completeness check   There are three statuses for the check: Pass, WARNING, and Error.   Pass: Everything goes fine with workers.   WARNING: The FL course goes well, but some handers are never used.      Error: The FL course fails as there is no path from start (i.e., the initial state) to end (i.e., the state corresponding to the end of an FL course).   Debug information of the completeness check   With the help of the networkx, we can quickly visualize and find out whether there is a potential failure. A directed graph will be plotted and saved into cfg.exp_dir folder to remind you something might go wrong:     ","categories": [],
        "tags": [],
        "url": "/docs/workers/",
        "teaser": null
      },{
        "title": "FS Data",
        "excerpt":"FS Data Module   In the tutorial, you will learn how to use FS Data Module and the code structure is shown below:   federatedscope/core ├── auxiliaries │   ├── data_builder │   ├── dataloader_builder │   ├── ... ├── data │   ├── base_data │   |   ├── ClientData │   |   ├── StandaloneDataDict │   ├── base_translator │   ├── ...   We will discuss all the concepts of our FS data module from top to bottom.   The main entrance of FS data   federatedscope.core.auxiliaries.data_builder.get_data is the entrance functions to access built-in FS data. With the config appropriately set, you can easily get FS Data.   fs_data, modified_cfg = get_data(config=init_cfg.clone())   get_data consists of three steps:      Load Dataset            federatedscope.core.data.utils.load_dataset       Load local file to torch dataset           Translate data            federatedscope.core.data.BaseDataTranslator       Dataset -&gt; ML split -&gt; FL split -&gt;  FS Dataloader           Convert mode            federatedscope.core.data.utils.convert_data_mode       To adapt simulation mode and distributed mode           Data Translator   In FederatedScope, the input to Runner is ClientData (in distributed mode) or StandaloneDataDict (in standalone mode), which are both subclasses of python dict. So FederatedScope provides Data Translator to help you convert torch.utils.data.Dataset to our data format. Data Translator contains four steps, two of which are optional.   Dataset -&gt; (ML split) -&gt; (FL split) -&gt; FS Dataloader      ML split(split_train_val_test):            Build train/val/test data split           FL split(split_to_client) (please see splitter for details):            Split global data into local data for each client.           ClientData   In FederatedScope, each client will obtain a ClientData, which has the following attributes:           A subclass of dict with train, val and test.            Convert dataset/list/array to DataLoader.       Example:                       # Instantiate client_data for each Client client_data = ClientData(PyGDataLoader,                           cfg,                           train=train_data,                           val=None,                           test=test_data) # other_cfg with different batch size client_data.setup(other_cfg) print(client_data)      &gt;&gt; {'train': PyGDataLoader(train_data), 'test': PyGDataLoader(test_data)}                           StandaloneDataDict   In standalone mode,  the input to Runner is StandaloneDataDict,  which is the return value of calling Data Translator. StandaloneDataDict has the following attributes:      A subclass of dict with client_id as keys:            {1: ClientData, 2: ClientData, ...}           Responsible for some pre-process for FS data:            Global evaluation: merge test data       Global training: merge all data into one client       Injected data attacks       …           ","categories": [],
        "tags": [],
        "url": "/docs/fs-data/",
        "teaser": null
      },{
        "title": "LLM",
        "excerpt":"   Introduction   FederatedScope-LLM (FS-LLM) is a comprehensive package for federated fine-tuning large language models, which provide:      A complete end-to-end benchmarking pipeline, automizing the processes of dataset preprocessing, federated fine-tuning execution or simulation, and performance evaluation on federated LLM fine-tuning with different capability demonstration purposes;   Comprehensive and off-the-shelf federated fine-tuning algorithm implementations and versatile programming interfaces for future extension to enhance the capabilities of LLMs in FL scenarios with low communication and computation costs, even without accessing the full model (e.g., closed-source LLMs);   Several accelerating operators and resource-efficient operators for fine-tuning LLMs with limited resources and the flexible pluggable sub-routines for interdisciplinary study (e.g., LLMs in personalized FL).   For more details, please refer to our paper: FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning.   To facilitate further research and adoption, we release FS-LLM at this https URL.   In the tutorial, you will learn:      The structure of FS-LLM [click]   How to start fine-tuning LLMs with FederatedScope [click]   How to use build-in functions [click]   How to use built-in or create new fine-tuning datasets [click]   How to use built-in evaluation tools [click]   How to adopt different federated fine-tuning PEFT algorithms [click]   How to enable different resource-efficiency operations [click]   FAQ [click]   Citation [click]   Code Structure   LLM-related directory   FederatedScope ├── federatedscope │   ├── core                     # Federated learning backend modules │   ├── llm                      # Federated fine-tuning LLMs modules  │   │   ├── baseline             # Scripts for LLMs │   │   ├── dataloader           # Federated fine-tuning dataloader │   │   ├── dataset              # Federated fine-tuning dataset │   │   ├── eval                 # Evaluation for fine-tuned LLMs │   │   ├── misc                 # Miscellaneous │   │   ├── model                # LLMs and Adapter │   │   ├── trainer              # Fine-tuning with accerating operators │   │   ├── ... │   ├── main.py                  # Running interface │   ├── ... ...           ├── tests                        # Unittest modules for continuous integration ├── LICENSE └── setup.py    Quick Start   Let’s start with fine-tuning GPT-2 on Alpaca to familiarize you with FS-LLM.   Step 1. Installation   The installation of FS-LLM is similar to minimal FS (see here for details), except that it requires Pytorch&gt;=1.13.0 (we recommend version 2.0.X) because of the PEFT dependency:   # Create virtual environments with conda conda create -n fs-llm python=3.9 conda activate fs-llm  # Install Pytorch&gt;=1.13.0 (e.g., Pytorch==2.0.0) conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.7 -c pytorch -c nvidia  # Install FS-LLM with editable mode pip install -e .[llm]   Now, you have successfully installed the FS-LLM.   Step 2. Run with exmaple config   Before running, you should change your git branch to llm.   Now, you can fine-tune a GPT2 on Alpaca with FedAvg.   python federatedscope/main.py --cfg federatedscope/llm/baseline/testcase.yaml   Start with Built-in Functions    You can easily run through a customized yaml file. Here we only introduce the configuration related to FS-LLM, other configurations please refer to Configurations. For more examples, please refer to federatedscope/llm/baseline.   # For this configuration, you might need a GPU with at least 32GB of video memory to run.  # Whether to use GPU use_gpu: True  # Deciding which GPU to use device: 0  # Early stop steps, set `0` to disable early_stop:   patience: 0  # Federate learning related options federate:   # `standalone` or `distributed`   mode: standalone   # Number of communication round   total_round_num: 500   # Saving path for ckpt   save_to: \"llama_rosetta_9_fed.ckpt\"   # Number of dataset being split   client_num: 9   # Enable for saving memory, all workers share the same model instance   share_local_model: True  # Dataset related options data:   # Root directory where the data stored   root: data/   # Dataset name   type: 'rosetta_alpaca@llm'   # Train/val/test splits   splits: [0.89,0.1,0.01]   # Use meta inforamtion to split `rosetta_alpaca`   splitter: 'meta'  # LLM related options llm:   # Max token length for model input (training)   tok_len: 650   # ChatBot related options   chat:     # Max token length for model input (inference)     max_len: 1000     # Max number of history texts     max_history_len: 10   # Path for store model cache, default in `~/.cache/`   cache:     model: ''   # PEFT related options   adapter:     # Set ture to enable PEFT fine-tuning     use: True     # Args for PEFT fine-tuning     args: [ { 'adapter_package': 'peft', 'adapter_method': 'lora', 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1 } ]  # DataLoader related options dataloader:   # Batch size for iter loader   batch_size: 1  # Model related options model:   # Model type (format: {MODEL_REPO}@huggingface_llm)   type: 'decapoda-research/llama-7b-hf@huggingface_llm'  # Train related options train:   # Number of local update steps   local_update_steps: 30   # `batch` or `epoch` for local_update_steps   batch_or_epoch: batch   # Optimizer related options   optimizer:     # Learning rate     lr: 0.003     # Weight decay     weight_decay: 0.0   # Set ture to enable `model.half()`   is_enable_half: True  # Trainer related options trainer:   # Trainer type   type: llmtrainer  # Evaluation related options eval:   # Frequency of evaluation   freq: 50   # Evaluation metrics   metrics: ['loss']   # Set key to track best model   best_res_update_round_wise_key: val_loss    Fine-tuning Dataset    In general, we use instruction SFT following Alpaca team. And in standalone mode, all dataset can be split into several clients with spesific splitter (i.e., lda, meta, iid) and federate.num_client.   Built-in Data                  data.type       Source       Note                       alpaca@llm       Link       IIDSplitter                 alpaca_cleaned@llm       Link       IIDSplitter                 dolly-15k@llm       Link       LDASplitter or MetaSplitter split to 8 clients.                 gsm8k@llm       Link       IIDSplitter                 rosetta_alpaca@llm       Link       LDASplitter or MetaSplitter split to 9 clients.                 code_search_net@llm       Link       LDASplitter or MetaSplitter split to 6 clients.           Self-maintained Data   FS-LLM also supports using your own dataset in the following format.                  data.type       Note                       YOU_DATA_NAME.json@llm       Format: [{'instruction': ..., 'input': ..., 'output':...}], default key: instruction, input, output, category                 YOU_DATA_NAME.jsonl@llm       Format of each line: {'instruction': ..., 'input': ..., 'output':...}, default key: instruction, input, output, category            Evaluation Tools   You can evaluate model domain capability of fine-tuned models with easy-to-use evaluation tools.   FederatedScope ├── federatedscope │   ├── llm │   │   ├── eval │   │   │   ├── eval_for_code │   │   │   ├── eval_for_gsm8k │   │   │   ├── eval_for_helm │   │   │   ├── eval_for_mmlu ...   How to use:   For example, to evaluate the model fine-tuned with python federatedscope/main.py --cfg sft_gsm8k.yaml, you can run python federatedscope/llm/eval/eval_for_gsm8k/eval.py --cfg sft_gsm8k.yaml in the eval_for_gsm8k directory. For other usages, please refer to the README.md file in each subdirectory.    Algorithms   Parameter-Efficient Fine-Tuning   With the help of parameter-efficient fine-tuning methods, federated fine-tuning a large model requires passing only a very small percentage of model parameters (adapters), making it possible for the client to enable efficient adaptation of pre-trained language models to various downstream applications. We adopt PEFT for fine-tuning LLMs, and more methods are coming soon!                  Methods       Source       Example for llm.adapter.args                       LoRA       Link       [ { 'adapter_package': 'peft', 'adapter_method': 'lora', 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.1 } ]                 Prefix Tuning       Link, Link       [{'adapter_package': 'peft', 'adapter_method': 'prefix', 'prefix_projection': False, 'num_virtual_tokens': 20}]                 P-Tuning       Link       [{'adapter_package': 'peft', 'adapter_method': 'p-tuning', 'encoder_reparameterization_type': 'MLP', 'encoder_dropout': 0.1, 'num_virtual_tokens': 20}]                 Prompt Tuning       Link       [{'adapter_package': 'peft', 'adapter_method': 'prompt', 'prompt_tuning_init': 'RANDOM', 'num_virtual_tokens': 20}]           Federate fine-tune closed-source LLMs   We support federated fine-tuning not only for open-source LLMs, but also for closed-source LLMs. In this scenario, clients can fine-tune LLMs without fully accessing the model, where models and data are both considered as privacy.                  Methods       Source       How to enable       Note                       Offsite-Tuning       Link       llm.offsite_tuning.use=True       -           For example, the following methods are supported:                  Methods       Source       How to use       Note                       Drop layers       Link       llm.offsite_tuning.emu_l=2 llm.offsite_tuning.emu_r=30  llm.offsite_tuning.kwargs={\"drop_ratio\":0.2}}       The server fixes the first two layers and the layers after 30th layer as the adapter, and uniformly drops 20% of the remaining layers, denoted as the emulator                 Model distill       Link       llm.offsite_tuning.emu_align.use=True llm.offsite_tuning.emu_l=2 llm.offsite_tuning.emu_r=30        The server fixes the first two layers and the layers after 30th layer as the adapter, and regards the remaining as the teacher model, and distills a student model as the emulator           More methods will be supported ASAP.   Evaluation of fine-tuned closed-source LLMs   To evaluate fine-tuned closed-source LLMs, one should decide whether to evaluate the original model with fine-tuned adapters or the emulator with fine-tuned adapters.                  Methods       Source       How to use       note                       Evaluation of fine-tuned closed-source LLMs       Link       cfg.llm.offsite_tuning.eval_type='full' (or 'emu')       ‘full’ means evaluating the original model with fine-tuned adapters; ‘emu’ means evaluating the emulator with fine-tuned adapters            Federate Fine-tune with Efficiency    To make the federated fine-tuning efficient, we adopt a series of acceleration operators.                  Methods       Source       How to use       Note                       torch.nn.DataParallel       Link       cfg.train.data_para_dids=[0,1]       It splits the input across the specified devices by chunking in the batch dimension.                 DeepSpeed       Link       cfg.llm.accelation.use=True       Use nvcc - V to make sure CUDA installed.  When set it to True, we can full-parameter fine-tune a llama-7b on a machine with 4 V100-32G gpus.                 FP16       Link       train.is_enable_half=True       Converting float types to half-precision to save memory usage                 Share local model       -       federate.share_local_model=True       The clients will share the base model, which reduces a lot of cpu memory consumption.                 Move to cpu       -       llm.adapter.mv_to_cpu=True       Move adapter to cpu after training, which can save memory but cost more time.            FAQ       WARNING: Skip the batch due to the loss is NaN, it may be caused by exceeding the precision or invalid labels.            Possible reason 1: This is because llm.tok_len limits the input length, causing the label to be empty, which automatically skips that data. Setting a larger llm.tok_len can avoid this.       Possible reason 2: Due to the enabling of train.is_enable_half, numerical overflow may occur. This usually happens when setting the optimizer.type to Adam, since the default eps is 1e-8 but fp16 requires at least 1e-5.           ValueError: Tokenizer class LLaMATokenizer does not exist or is not currently imported.             This is a problem with transformers, you can fix it in your local file. Replace LLaMATokenizer with LlamaTokenizer in PATH_TO_DATA_ROOT/MODEL_REPO/snapshots/..../tokenizer_config.json           OutOfMemoryError: CUDA out of memory.            Torch’s garbage collection mechanism may not be timely resulting in OOM, please set cfg.eval.count_flops to False.           Citation   If you find FederatedScope-LLM useful for your research or development, please cite the following paper:  @article{kuang2023federatedscopellm,   title={FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning},   author={Weirui Kuang and Bingchen Qian and Zitao Li and Daoyuan Chen and Dawei Gao and Xuchen Pan and Yuexiang Xie and Yaliang Li and Bolin Ding and Jingren Zhou},   journal={arXiv preprint arXiv:2309.00363},   year={2023} }  ","categories": [],
        "tags": [],
        "url": "/docs/llm/",
        "teaser": null
      },{
        "title": "Lhasa Apso",
        "excerpt":"   The Lhasa Apso (/ˈlɑːsə ˈæpsoʊ/ lah-sə ap-soh) is a non-sporting dog breed originating in Tibet. It was bred as an interior sentinel in the Buddhist monasteries, to alert the monks to any intruders who entered. Lhasa is the capital city of Tibet, and apso is a word in the Tibetan language meaning “bearded”, so, Lhasa Apso simply means “long-haired Lhasa dog”. There are, however, some who claim that the word “apso” is a form of the Tibetan word “rapso”, meaning “goat-like”, which would make the equivalent translation “wooly Lhasa dog”.       From Wikipedia, the free encyclopedia   ","categories": [],
        "tags": [],
        "url": "/pets/lhasa-apso/",
        "teaser": null
      },{
        "title": "Tabby",
        "excerpt":"   A tabby is any domestic cat that has a coat featuring distinctive stripes, dots, lines or swirling patterns, usually together with a mark resembling an ‘M’ on its forehead. Tabbies are sometimes erroneously assumed to be a cat breed. In fact, the tabby pattern is found in many breeds, as well as among the general mixed-breed population. The tabby pattern is a naturally occurring feature that may be related to the coloration of the domestic cat’s direct ancestor, the African wildcat, which (along with the European wildcat and Asiatic wildcat) has a similar coloration.       From Wikipedia, the free encyclopedia   ","categories": [],
        "tags": [],
        "url": "/pets/tabby/",
        "teaser": null
      },{
        "title": "Baz Boom Identity",
        "excerpt":"Pictures of Goats section West Seattle Blog dingbat newspaper rubber cement Google+ newsroom cafe news.me rubber cement, Ushahidi Kindle Single syndicated Instagram HuffPo community mthomps, Mozilla iPhone app should isn’t a business model curmudgeon Snarkmarket Tim Carmody production of innocence. Fuego tweets community DocumentCloud metered model Gardening &amp; War section YouTube social media SEO information overload analytics Aron Pilhofer Journal Register data visualization WikiLeaks Groupon, collaboration Steve Jobs we need a Nate Silver AP What Would Google Do the power of the press belongs to the person who owns one Clay Shirky curmudgeon Voice of San Diego free as in beer dead trees the notion of the public Lucius Nieman.                                                                                                                                This is a sample gallery to go along with this case study.       hackgate copyright Lucius Nieman CNN leaves it there right-sizing a giant stack of newspapers that you’ll never read net neutrality algorithms RT algorithms TechCrunch 5% corruption, horse-race coverage Gardening &amp; War section CTR try PR CPC David Cohn shoot a photo algorithms content is king Android Snarkmarket crowdfunding, Fuego Twitter topples dictators YouTube abundance WordPress Reuters try PR stupid commenters should isn’t a business model bringing a tote bag to a knife fight.  ","categories": [],
        "tags": [],
        "url": "/portfolio/baz-boom-identity/",
        "teaser": "/assets/images/unsplash-gallery-image-1-th.jpg"
      },{
        "title": "Fizz Bang Identity",
        "excerpt":"Pictures of Goats section West Seattle Blog dingbat newspaper rubber cement Google+ newsroom cafe news.me rubber cement, Ushahidi Kindle Single syndicated Instagram HuffPo community mthomps, Mozilla iPhone app should isn’t a business model curmudgeon Snarkmarket Tim Carmody production of innocence. Fuego tweets community DocumentCloud metered model Gardening &amp; War section YouTube social media SEO information overload analytics Aron Pilhofer Journal Register data visualization WikiLeaks Groupon, collaboration Steve Jobs we need a Nate Silver AP What Would Google Do the power of the press belongs to the person who owns one Clay Shirky curmudgeon Voice of San Diego free as in beer dead trees the notion of the public Lucius Nieman.                                                                                                                                This is a sample gallery to go along with this case study.       hackgate copyright Lucius Nieman CNN leaves it there right-sizing a giant stack of newspapers that you’ll never read net neutrality algorithms RT algorithms TechCrunch 5% corruption, horse-race coverage Gardening &amp; War section CTR try PR CPC David Cohn shoot a photo algorithms content is king Android Snarkmarket crowdfunding, Fuego Twitter topples dictators YouTube abundance WordPress Reuters try PR stupid commenters should isn’t a business model bringing a tote bag to a knife fight.  ","categories": [],
        "tags": [],
        "url": "/portfolio/fizz-bang-identity/",
        "teaser": "/assets/images/unsplash-gallery-image-2-th.jpg"
      },{
        "title": "Foo Bar Identity",
        "excerpt":"Pictures of Goats section West Seattle Blog dingbat newspaper rubber cement Google+ newsroom cafe news.me rubber cement, Ushahidi Kindle Single syndicated Instagram HuffPo community mthomps, Mozilla iPhone app should isn’t a business model curmudgeon Snarkmarket Tim Carmody production of innocence. Fuego tweets community DocumentCloud metered model Gardening &amp; War section YouTube social media SEO information overload analytics Aron Pilhofer Journal Register data visualization WikiLeaks Groupon, collaboration Steve Jobs we need a Nate Silver AP What Would Google Do the power of the press belongs to the person who owns one Clay Shirky curmudgeon Voice of San Diego free as in beer dead trees the notion of the public Lucius Nieman.                                                                                                                                This is a sample gallery to go along with this case study.       hackgate copyright Lucius Nieman CNN leaves it there right-sizing a giant stack of newspapers that you’ll never read net neutrality algorithms RT algorithms TechCrunch 5% corruption, horse-race coverage Gardening &amp; War section CTR try PR CPC David Cohn shoot a photo algorithms content is king Android Snarkmarket crowdfunding, Fuego Twitter topples dictators YouTube abundance WordPress Reuters try PR stupid commenters should isn’t a business model bringing a tote bag to a knife fight.  ","categories": [],
        "tags": [],
        "url": "/portfolio/foo-bar-website/",
        "teaser": "/assets/images/foo-bar-identity-th.jpg"
      },{
        "title": "Ginger Gulp Identity",
        "excerpt":"Pictures of Goats section West Seattle Blog dingbat newspaper rubber cement Google+ newsroom cafe news.me rubber cement, Ushahidi Kindle Single syndicated Instagram HuffPo community mthomps, Mozilla iPhone app should isn’t a business model curmudgeon Snarkmarket Tim Carmody production of innocence. Fuego tweets community DocumentCloud metered model Gardening &amp; War section YouTube social media SEO information overload analytics Aron Pilhofer Journal Register data visualization WikiLeaks Groupon, collaboration Steve Jobs we need a Nate Silver AP What Would Google Do the power of the press belongs to the person who owns one Clay Shirky curmudgeon Voice of San Diego free as in beer dead trees the notion of the public Lucius Nieman.                                                                                                                                This is a sample gallery to go along with this case study.       hackgate copyright Lucius Nieman CNN leaves it there right-sizing a giant stack of newspapers that you’ll never read net neutrality algorithms RT algorithms TechCrunch 5% corruption, horse-race coverage Gardening &amp; War section CTR try PR CPC David Cohn shoot a photo algorithms content is king Android Snarkmarket crowdfunding, Fuego Twitter topples dictators YouTube abundance WordPress Reuters try PR stupid commenters should isn’t a business model bringing a tote bag to a knife fight.  ","categories": [],
        "tags": [],
        "url": "/portfolio/ginger-gulp-identity/",
        "teaser": "/assets/images/unsplash-gallery-image-3-th.jpg"
      },{
        "title": "Answering Multi-Dimensional Analytical Queries under Local Differential Privacy",
        "excerpt":"Multi-dimensional analytical (MDA) queries are often issued against a fact table with predicates on (categorical or ordinal) dimensions and aggregations on one or more measures. In this paper, we study the problem of answering MDA queries under local differential privacy (LDP). In the absence of a trusted agent, sensitive dimensions are encoded in a privacy-preserving (LDP) way locally before being sent to the data collector. The data collector estimates the answers to MDA queries, based on the encoded dimensions. We propose several LDP encoders and estimation algorithms, to handle a large class of MDA queries with different types of predicates and aggregation functions. Our techniques are able to answer these queries with tight error bounds and scale well in high-dimensional settings (i.e., error is polylogarithmic in dimension sizes). We conduct experiments on real and synthetic data to verify our theoretical results, and compare our solution with marginal-estimation based solutions.   Tianhao Wang, Bolin Ding, Jingren Zhou, Cheng Hong, Zhicong Huang, Ninghui Li, Somesh Jha: Answering Multi-Dimensional Analytical Queries under Local Differential Privacy. SIGMOD Conference 2019: 159-176 download  ","categories": [],
        "tags": ["multi-dimensional analytical (MDA) queries","local differential privacy","federated data analytics"],
        "url": "/mdaquery/",
        "teaser": null
      },{
        "title": "DPSAaS: Multi-Dimensional Data Sharing and Analytics as Services under Local Differential Privacy",
        "excerpt":"Differential privacy has emerged as the de facto standard for privacy definitions, and been used by, e.g., Apple, Google, Uber, and Microsoft, to collect sensitive information about users and to build privacy-preserving analytics engines. However, most of such advanced privacy-protection techniques are not accessible to mid-size companies and app developers in the cloud. We demonstrate a lightweight middleware DPSAaS, which provides differentially private data-sharing-and-analytics functionality as cloud services.   We focus on multi-dimensional analytical (MDA) queries under local differential privacy (LDP) in this demo. MDA queries against a fact table have predicates on (categorical or ordinal) dimensions and aggregate one or more measures. In the absence of a trusted agent, sensitive dimensions and measures are encoded in a privacy-preserving way locally using our LDP data sharing service, before being sent to the data collector. The data collector estimates the answers to MDA queries from the encoded data, using our data analytics service. We will highlight the design decisions of DPSAaS and twists made to LDA algorithms to fit the design, in order to smoothly connect DPSAaS to the data processing platform and analytics engines, and to facilitate efficient large-scale processing.   Min Xu, Tianhao Wang, Bolin Ding, Jingren Zhou, Cheng Hong, Zhicong Huang: DPSAaS: Multi-Dimensional Data Sharing and Analytics as Services under Local Differential Privacy. Proc. VLDB Endow. 12(12): 1862-1865 (2019) download  ","categories": [],
        "tags": ["multi-dimensional analytical (MDA) queries","local differential privacy","federated data analytics","system"],
        "url": "/dpsaas/",
        "teaser": null
      },{
        "title": "Collecting and Analyzing Data Jointly from Multiple Services under Local Differential Privacy",
        "excerpt":"Users’ sensitive data can be collected and analyzed under local differential privacy (LDP) without the need to trust the data collector. Most previous work on LDP can be applied when each user’s data is generated and collected from a single service or data source. In a more general and practical setting, sensitive data of each user needs to be collected under LDP from multiple services independently and can be joined on, e.g., user id. In this paper, we address two challenges in this setting: first, how to prevent the privacy guarantee from being weakened during the joint data collection; second, how to analyze perturbed data jointly from different services. We introduce the notation of user-level LDP to formalize and protect the privacy of a user when her joined data tuples are released. We propose mechanisms and estimation methods to process multidimensional analytical queries, each with sensitive attributes (in its aggregation and predicates) collected and perturbed independently in multiple services. We also introduce an online utility optimization technique for multi-dimensional range predicates, based on consistency in domain hierarchy. We conduct extensive evaluations to verify our theoretical results using synthetic and real datasets.   Min Xu, Bolin Ding, Tianhao Wang, Jingren Zhou: Collecting and Analyzing Data Jointly from Multiple Services under Local Differential Privacy. Proc. VLDB Endow. 13(11): 2760-2772 (2020) download  ","categories": [],
        "tags": ["federated data analytics","local differential privacy","frequency-based attack"],
        "url": "/jointldp/",
        "teaser": null
      },{
        "title": "Improving Utility and Security of the Shuffler-based Differential Privacy",
        "excerpt":"When collecting information, local differential privacy (LDP) alleviates privacy concerns of users because their private information is randomized before being sent it to the central aggregator. LDP imposes large amount of noise as each user executes the randomization independently. To address this issue, recent work introduced an intermediate server with the assumption that this intermediate server does not collude with the aggregator. Under this assumption, less noise can be added to achieve the same privacy guarantee as LDP, thus improving utility for the data collection task.   This paper investigates this multiple-party setting of LDP. We analyze the system model and identify potential adversaries. We then make two improvements: a new algorithm that achieves a better privacy-utility tradeoff; and a novel protocol that provides better protection against various attacks. Finally, we perform experiments to compare different methods and demonstrate the benefits of using our proposed method.   Tianhao Wang, Bolin Ding, Min Xu, Zhicong Huang, Cheng Hong, Jingren Zhou, Ninghui Li, Somesh Jha: Improving Utility and Security of the Shuffler-based Differential Privacy. Proc. VLDB Endow. 13(13): 3545-3558 (2020) download  ","categories": [],
        "tags": ["frequency queries","local differential privacy","privacy amplification","multiple-party computation"],
        "url": "/shufflerdp/",
        "teaser": null
      },{
        "title": "Federated Matrix Factorization with Privacy Guarantee",
        "excerpt":"Matrix factorization (MF) approximates unobserved ratings in a rating matrix, whose rows correspond to users and columns correspond to items to be rated, and has been serving as a fundamental building block in recommendation systems. This paper comprehensively studies the problem of matrix factorization in different federated learning (FL) settings, where a set of parties want to cooperate in training but refuse to share data directly. We first propose a generic algorithmic framework for various settings of federated matrix factorization (FMF) and provide a theoretical convergence guarantee. We then systematically characterize privacy-leakage risks in data collection, training, and publishing stages for three different settings and introduce privacy notions to provide end-to-end privacy protections. The first one is vertical federated learning (VFL), where multiple parties have the ratings from the same set of users but on disjoint sets of items. The second one is horizontal federated learning (HFL), where parties have ratings from different sets of users but on the same set of items. The third setting is local federated learning (LFL), where the ratings of the users are only stored on their local devices. We introduce adapted versions of FMF with the privacy notions guaranteed in the three settings. In particular, a new private learning technique called embedding clipping is introduced and used in all the three settings to ensure differential privacy. For the LFL setting, we combine differential privacy with secure aggregation to protect the communication between user devices and the server with a strength similar to the local differential privacy model, but much better accuracy. We perform experiments to demonstrate the effectiveness of our approaches.   Zitao Li, Bolin Ding, Ce Zhang, Ninghui Li, Jingren Zhou: Federated Matrix Factorization with Privacy Guarantee. Proc. VLDB Endow. 15(4): 900-913 (2021) download  ","categories": [],
        "tags": ["federated learning","recommender system"],
        "url": "/federated-matrix-factorization/",
        "teaser": null
      },{
        "title": "FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning",
        "excerpt":"The incredible development of federated learning (FL) has benefited various tasks in the domains of computer vision and natural language processing, and the existing frameworks such as TFF and FATE has made the deployment easy in real-world applications. However, federated graph learning (FGL), even though graph data are prevalent, has not been well supported due to its unique characteristics and requirements. The lack of FGL-related framework increases the efforts for accomplishing reproducible research and deploying in real-world applications. Motivated by such strong demand, in this paper, we first discuss the challenges in creating an easy-to-use FGL package and accordingly present our implemented package FederatedScope-GNN, which provides (1) a unified view for modularizing and expressing FGL algorithms; (2) comprehensive DataZoo and ModelZoo for out-of-the-box FGL capability; (3) an efficient model auto-tuning component; and (4) off-the-shelf privacy attack and defense abilities.  We validate the effectiveness of FederatedScope-GNN by conducting extensive experiments, which simultaneously gains many valuable insights about FGL for the community. Moreover, we employ FederatedScope-GNN to serve the FGL application in real-world E-commerce scenarios, where the attained improvements indicate great potential business benefits. We publicly release FederatedScope-GNN, as submodules of FederatedScope, at https://github.com/alibaba/FederatedScope to promote FGL’s research and enable broad applications that would otherwise be infeasible due to the lack of a dedicated package.   Zhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang Li, Bolin Ding, Jingren Zhou: FederatedScope-GNN: Towards a Unified, Comprehensive and Efficient Package for Federated Graph Learning download  ","categories": [],
        "tags": ["federated learning","graph"],
        "url": "/federated-graph-learning/",
        "teaser": null
      },{
        "title": "FederatedScope: A Flexible Federated Learning Platform for Heterogeneity",
        "excerpt":"Although remarkable progress has been made by the existing federated learning (FL) platforms to provide fundamental functionalities for development, these platforms cannot well tackle the challenges brought by the heterogeneity of FL scenarios from both academia and industry.  To fill this gap, in this paper, we propose a flexible federated learning platform, named FederatedScope, for handling various types of heterogeneity in FL. Considering both flexibility and extensibility, FederatedScope adopts an event-driven architecture to frame an FL course into event-handler pairs: the behaviors of participants are described in handlers, and triggered by events of message passing or meeting certain conditions in training. For a new FL application, developers only need to specify the adopted FL algorithm by defining new types of events and the corresponding handling functions based on participants’ behaviors, which would be automatically executed in an asynchronous way for balancing effectiveness and efficiency in FederatedScope. Meanwhile, towards an easy-to-use platform, FederatedScope provides rich built-in algorithms, including personalization, federated aggregation, privacy protection, and privacy attack, for users to conveniently customize participant-specific training, fusing, aggregating, and protecting. Besides, a federated hyperparameter optimization module is integrated into FederatedScope for users to automatically tune their FL systems for resolving the unstable issues brought by heterogeneity. We conduct a series of experiments on the provided easy-to-use and comprehensive FL benchmarks to validate the correctness and efficiency of FederatedScope.  We have released FederatedScope for users on https://github.com/alibaba/FederatedScope to promote research and industrial deployment of federated learning in a variety of real-world applications.   Yuexiang Xie, Zhen Wang, Daoyuan Chen, Dawei Gao, Liuyi Yao, Weirui Kuang, Yaliang Li, Bolin Ding, Jingren Zhou: FederatedScope: A Flexible Federated Learning Platform for Heterogeneity download  ","categories": [],
        "tags": ["federated learning","system"],
        "url": "/federatedscope-core/",
        "teaser": null
      },{
        "title": "Chocolate Chip Cookies",
        "excerpt":"A chocolate chip cookie is a drop cookie that originated in the United States and features chocolate chips as its distinguishing ingredient.   The traditional recipe combines a dough composed of butter and both brown and white sugar with semi-sweet chocolate chips. Variations include recipes with other types of chocolate as well as additional ingredients such as nuts or oatmeal.   This recipe makes 4 dozen cookies.   Ingredients      2 1/4 cups all-purpose flour   1 teaspoon baking soda   1/2 teaspoon salt   1 cup butter, softened and cut to pieces   1 cup sugar   1 cup light brown sugar, packed   2 teaspoons vanilla extract   2 large eggs   2 cups semi-sweet chocolate chips   1/2 teaspoon nutmeg (optional)   1 cup chopped pecans or walnuts (optional)   Directions      Preheat the oven to 350 F.   In a medium bowl, whisk flour with baking soda, nutmeg and salt.   In a large bowl, beat butter with sugar and brown sugar until creamy and light. Add vanilla and eggs, one at a time, and mix until incorporated.   Gradually add dry mixture into the butter-sugar wet blend, mixing with a spatula until combined. Add chocolate chips and nuts until just mixed.   Drop tablespoon-sized clumps onto un-greased cookie sheets. Bake for 8-12 minutes, or until pale brown. Allow to cool on the pan for a minute or three, then transfer cookies to a wire rack to finish cooling.  ","categories": [],
        "tags": [],
        "url": "/recipes/chocolate-chip-cookies/",
        "teaser": null
      },{
        "title": "Oatmeal Cookies",
        "excerpt":"Oatmeal cookies are a proverbial favorite with both kids and adults. This crisp and chewy cookie is loaded with oats, dried fruit, and chopped nuts.   Ingredients      1 cup butter, softened 1 cup white sugar   1 cup packed brown sugar   2 eggs   1 teaspoon vanilla extract   2 cups all-purpose flour   1 teaspoon baking soda   1 teaspoon salt   1 1/2 teaspoons ground cinnamon   3 cups quick cooking oats   Directions      In a medium bowl, cream together butter, white sugar, and brown sugar. Beat in eggs one at a time, then stir in vanilla. Combine flour, baking soda, salt, and cinnamon; stir into the creamed mixture. Mix in oats. Cover, and chill dough for at least one hour.   Preheat the oven to 375 degrees F (190 degrees C). Grease cookie sheets. Roll the dough into walnut sized balls, and place 2 inches apart on cookie sheets. Flatten each cookie with a large fork dipped in sugar.   Bake for 8 to 10 minutes in preheated oven. Allow cookies to cool on baking sheet for 5 minutes before transferring to a wire rack to cool completely.  ","categories": [],
        "tags": [],
        "url": "/recipes/oatmeal-cookies/",
        "teaser": null
      },{
        "title": "Peanut Butter Cookies",
        "excerpt":"A peanut butter cookie is a type of cookie that is distinguished for having peanut butter as a principal ingredient. The cookie generally originated in the United States, its development dating back to the 1910s.   Ingredients      1 cup unsalted butter   1 cup crunchy peanut butter   1 cup white sugar   1 cup packed brown sugar   2 eggs 2   1/2 cups all-purpose flour   1 teaspoon baking powder   1/2 teaspoon salt   1 1/2 teaspoons baking soda   Directions      Cream butter, peanut butter, and sugars together in a bowl; beat in eggs.   In a separate bowl, sift flour, baking powder, baking soda, and salt; stir into butter mixture. Put dough in refrigerator for 1 hour.   Roll dough into 1 inch balls and put on baking sheets. Flatten each ball with a fork, making a crisscross pattern. Bake in a preheated 375 degrees F oven for about 10 minutes or until cookies begin to brown.  ","categories": [],
        "tags": [],
        "url": "/recipes/peanut-butter-cookies/",
        "teaser": null
      },]
